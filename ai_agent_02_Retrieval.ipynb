{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOr/rsyGXIWo24VetopmtXk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/427paul/ai_agent/blob/main/ai_agent_02_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I23Wxapk3yPp"
      },
      "outputs": [],
      "source": [
        "!pip install -U \"langchain==0.3.*\" \"langchain-core==0.3.*\" \"langchain-community==0.3.*\" \"langgraph==0.3.*\" \"langchain-huggingface\" \"huggingface_hub\" \"sentence-transformers\" wikipedia -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mIFDJgbn4uF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def load_api_keys(filepath=\"api_key.txt\"):\n",
        "    with open(filepath, \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line and \"=\" in line:\n",
        "                key, value = line.split(\"=\", 1)\n",
        "                os.environ[key.strip()] = value.strip()\n",
        "\n",
        "path = '/content/drive/MyDrive/LangGraph/'\n",
        "\n",
        "# API 키 로드 및 환경변수 설정\n",
        "load_api_keys(path + 'api_key.txt')"
      ],
      "metadata": {
        "id": "dUp_VY3r4x7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Loader"
      ],
      "metadata": {
        "id": "PoYjIP0U4qL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## html loader"
      ],
      "metadata": {
        "id": "LPQXIYmh49Ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured"
      ],
      "metadata": {
        "id": "a4-G2PYN5SYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 lxml"
      ],
      "metadata": {
        "id": "6Mec_PzL5ilS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
        "\n",
        "# requires `pip install unstructured`\n",
        "loader = UnstructuredHTMLLoader(\"sample.html\")\n",
        "data = loader.load()\n",
        "print(data)\n",
        "\"\"\"\n",
        "[Document(page_content=\"Welcome to My Web Page\\n\\nThis is a simple HTML page. It's a great starting point for learning HTML.\\n\\nClick here to visit Example.com\", metadata={'source': 'sample.html'})]\n",
        "\"\"\"\n",
        "\n",
        "from langchain.document_loaders import BSHTMLLoader\n",
        "\n",
        "# requires `pip install beautifulsoup4 lxml`\n",
        "loader = BSHTMLLoader(\"sample.html\")\n",
        "data = loader.load()\n",
        "print(data)\n",
        "\"\"\"\n",
        "[Document(page_content=\"\\n\\nMy Simple Web Page\\n\\n\\nWelcome to My Web Page\\nThis is a simple HTML page. It's a great starting point for learning HTML.\\nClick here to visit Example.com\\n\\n\\n\", metadata={'source': 'sample.html', 'title': 'My Simple Web Page'})]\n",
        "\"\"\"\n",
        "\n",
        "print(data[0].page_content)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "My Simple Web Page\n",
        "\n",
        "\n",
        "Welcome to My Web Page\n",
        "This is a simple HTML page. It's a great starting point for learning HTML.\n",
        "Click here to visit Example.com\n",
        "\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DrKohp3I4CVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## json loader"
      ],
      "metadata": {
        "id": "ZHxPleEa54tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jq"
      ],
      "metadata": {
        "id": "RcL6llMM-gwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from langchain_community.document_loaders import JSONLoader\n",
        "import json\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "\n",
        "# required `pip install jq`\n",
        "file_path='./facebook_chat.json'\n",
        "data = json.loads(Path(file_path).read_text())\n",
        "pprint(data)\n",
        "\"\"\"\n",
        "{'image': {'creation_timestamp': 1675549016, 'uri': 'image_of_the_chat.jpg'},\n",
        " 'is_still_participant': True,\n",
        " 'joinable_mode': {'link': '', 'mode': 1},\n",
        " 'magic_words': [],\n",
        " 'messages': [{'content': 'Bye!',\n",
        "               'sender_name': 'User 2',\n",
        "               'timestamp_ms': 1675597571851},\n",
        "              {'content': 'Oh no worries! Bye',\n",
        "               'sender_name': 'User 1',\n",
        "               'timestamp_ms': 1675597435669},\n",
        "              {'content': 'No Im sorry it was my mistake, the blue one is not '\n",
        "                          'for sale',\n",
        "               'sender_name': 'User 2',\n",
        "               'timestamp_ms': 1675596277579},\n",
        "              {'content': 'I thought you were selling the blue one!',\n",
        "               'sender_name': 'User 1',\n",
        "               'timestamp_ms': 1675595140251},\n",
        "              {'content': 'Im not interested in this bag. Im interested in the '\n",
        "                          'blue one!',\n",
        "               'sender_name': 'User 1',\n",
        "               'timestamp_ms': 1675595109305},\n",
        "              {'content': 'Here is $129',\n",
        "               'sender_name': 'User 2',\n",
        "               'timestamp_ms': 1675595068468},\n",
        "              {'photos': [{'creation_timestamp': 1675595059,\n",
        "                           'uri': 'url_of_some_picture.jpg'}],\n",
        "               'sender_name': 'User 2',\n",
        "               'timestamp_ms': 1675595060730},\n",
        "              {'content': 'Online is at least $100',\n",
        "               'sender_name': 'User 2',\n",
        "               'timestamp_ms': 1675595045152},\n",
        "              {'content': 'How much do you want?',\n",
        "               'sender_name': 'User 1',\n",
        "               'timestamp_ms': 1675594799696},\n",
        "              {'content': 'Goodmorning! $50 is too low.',\n",
        "               'sender_name': 'User 2',\n",
        "               'timestamp_ms': 1675577876645},\n",
        "              {'content': 'Hi! Im interested in your bag. Im offering $50. Let '\n",
        "                          'me know if you are interested. Thanks!',\n",
        "               'sender_name': 'User 1',\n",
        "               'timestamp_ms': 1675549022673}],\n",
        " 'participants': [{'name': 'User 1'}, {'name': 'User 2'}],\n",
        " 'thread_path': 'inbox/User 1 and User 2 chat',\n",
        " 'title': 'User 1 and User 2 chat'}\n",
        "\"\"\"\n",
        "\n",
        "loader = JSONLoader(\n",
        "    file_path='./facebook_chat.json',\n",
        "    jq_schema='.messages[].content',\n",
        "    text_content=False)\n",
        "\n",
        "data = loader.load()\n",
        "pprint(data)\n",
        "\"\"\"\n",
        "[Document(page_content='Bye!', metadata={'source': '/Users/seungjoonlee/git/learn-langchain/document_loader/facebook_chat.json', 'seq_num': 1}),\n",
        " Document(page_content='Oh no worries! Bye', metadata={'source': '/Users/seungjoonlee/git/learn-langchain/document_loader/facebook_chat.json', 'seq_num': 2}),\n",
        " Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': '/Users/seungjoonlee/git/learn-langchain/document_loader/facebook_chat.json', 'seq_num': 3}),\n",
        " Document(page_content='I thought you were selling the blue one!', metadata={'source': '/Users/seungjoonlee/git/learn-langchain/document_loader/facebook_chat.json', 'seq_num': 4}),\n",
        " Document(page_content='Im not interested in this bag. Im interested in the blue one!', metadata={'source': '/Users/seungjoonlee/git/learn-langchain/document_loader/facebook_chat.json', 'seq_num': 5}),\n",
        " Document(page_content='Here is $129', metadata={'source': '/Users/seungjoonlee/git/learn-langchain/document_loader/facebook_chat.json', 'seq_num': 6}),\n",
        " Document(page_content='', metadata={'source': '/Users/seungjoonlee/git/learn-langchain/document_loader/facebook_chat.json', 'seq_num': 7}),\n",
        " Document(page_content='Online is at least $100', metadata={'source': '/Users/seungjoonlee/git/learn-langchain/document_loader/facebook_chat.json', 'seq_num': 8}),\n",
        " Document(page_content='How much do you want?', metadata={'source': '/Users/seungjoonlee/git/learn-langchain/document_loader/facebook_chat.json', 'seq_num': 9}),\n",
        " Document(page_content='Goodmorning! $50 is too low.', metadata={'source': '/Users/seungjoonlee/git/learn-langchain/document_loader/facebook_chat.json', 'seq_num': 10}),\n",
        " Document(page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!', metadata={'source': '/Users/seungjoonlee/git/learn-langchain/document_loader/facebook_chat.json', 'seq_num': 11})]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "A6qIrbwz5ALu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pdf loader"
      ],
      "metadata": {
        "id": "dc4vHwDb6IwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# required `pip install pypdf`\n",
        "loader = PyPDFLoader(\"csv_sample.pdf\")\n",
        "pages = loader.load_and_split()\n",
        "\n",
        "print(pages[0].page_content)\n",
        "\"\"\"\n",
        "csv_sample\n",
        "Page 1nameagecountry\n",
        "Neville Hardy 56Niue\n",
        "Dacia Cohen 74Falkland Islands (Malvinas)\n",
        "Kathey Daniel 10Slovenia\n",
        "Mallie Welch 12Equatorial Guinea\n",
        "Katia Bryant 14Ghana\n",
        "Laurice Robertson 53Saudi Arabia\n",
        "Minh Barrett 27French Southern Territories\n",
        "Latashia Perez 52Finland\n",
        "Elvina Ross 68New Zealand\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ERU6-ArP6Jyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Transformer"
      ],
      "metadata": {
        "id": "qEXlMaGZ6lFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## text splitter"
      ],
      "metadata": {
        "id": "NSde4wIL8MmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "with open('./state_of_the_union.txt') as f:\n",
        "    state_of_the_union = f.read()\n",
        "\n",
        "# default split on are [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap  = 20, # max overlap\n",
        "    length_function = len,\n",
        "    add_start_index = True,\n",
        ")\n",
        "\n",
        "texts = text_splitter.create_documents([state_of_the_union])\n",
        "print(texts[0])\n",
        "\"\"\"\n",
        "page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and' metadata={'start_index': 0}\n",
        "\"\"\"\n",
        "print(texts[1])\n",
        "\"\"\"\n",
        "page_content='of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.' metadata={'start_index': 82}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "M5VO5Ijq6ow9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## code splitter"
      ],
      "metadata": {
        "id": "dRsAnXR18RF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import (\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    Language,\n",
        ")\n",
        "\n",
        "print([e.value for e in Language])\n",
        "\"\"\"\n",
        "['cpp', 'go', 'java', 'kotlin', 'js', 'ts', 'php', 'proto', 'python', 'rst', 'ruby', 'rust', 'scala', 'swift', 'markdown', 'latex', 'html', 'sol', 'csharp', 'cobol']\n",
        "\"\"\"\n",
        "\n",
        "separators = RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)\n",
        "print(separators)\n",
        "\"\"\"\n",
        "['\\nclass ', '\\ndef ', '\\n\\tdef ', '\\n\\n', '\\n', ' ', '']\n",
        "\"\"\"\n",
        "\n",
        "PYTHON_CODE = \"\"\"\n",
        "def hello_world():\n",
        "    print(\"Hello, World!\")\n",
        "\n",
        "# Call the function\n",
        "hello_world()\n",
        "\"\"\"\n",
        "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
        ")\n",
        "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
        "print(python_docs)\n",
        "\"\"\"\n",
        "[Document(page_content='def hello_world():\\n    print(\"Hello, World!\")'), Document(page_content='# Call the function\\nhello_world()')]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kTC7qSmf8Uv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## split by token"
      ],
      "metadata": {
        "id": "7lkOTTXu8W90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "\n",
        "# required `pip install tiktoken`\n",
        "with open('./state_of_the_union.txt') as f:\n",
        "    state_of_the_union = f.read()\n",
        "\n",
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=100, chunk_overlap=0\n",
        ")\n",
        "texts = text_splitter.split_text(state_of_the_union)\n",
        "print(len(texts))\n",
        "print(texts[0])"
      ],
      "metadata": {
        "id": "HkE-uM678ZY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Embedding"
      ],
      "metadata": {
        "id": "sY7imaxA80fS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# requires `pip install openai`\n",
        "embeddings_model = OpenAIEmbeddings()\n",
        "\n",
        "# Embed list of texts\n",
        "embeddings = embeddings_model.embed_documents(\n",
        "    [\n",
        "        \"Hi there!\",\n",
        "        \"Oh, hello!\",\n",
        "        \"What's your name?\",\n",
        "        \"My friends call me World\",\n",
        "        \"Hello World!\"\n",
        "    ]\n",
        ")\n",
        "print(len(embeddings))\n",
        "\"\"\"\n",
        "5\n",
        "\"\"\"\n",
        "\n",
        "# length of 1536\n",
        "print(len(embeddings[0]))\n",
        "print(len(embeddings[1]))\n",
        "\"\"\"\n",
        "[... 0.022085255981691917, 0.025720015991018582, 0.008734743689027272, -0.006709843137048811, -0.022764415491192392, -0.00257671800269355, 0.010677894145694868, 0.0001446357869742665, -0.02568228625240111, -0.010438930752548039, -0.002831402818756228, -0.012992066737283132, 0.0015925658455746433, -0.021569597594135712, 0.011853846242120273, 0.015771589535625893, 0.006238204640524732, 0.02429881221167677, 0.014086268272736402, -0.024575506274763608, -0.021129402409104603, 0.007653119664435697, 0.006021250727232698, -0.02475158583889211, -0.012853719705739713, 0.018048030525951612, -0.0018441062839218978, -0.008445472115078739, -0.006885921304193508, 0.00240850043059146, 0.00827568270336489, -0.008030431020448483, -0.004181860777053302, 0.0010344603379206113, 0.007552503768493557, 0.01879007479579295, 0.008451761336170855, -0.014249769394680672, -0.03264995904888929, 0.004728961544779937, -0.0020343339179553677, -0.024927663540375542, -0.006565207350074544, -0.014765427782236877 ...]\n",
        "\"\"\"\n",
        "\n",
        "# Use Document Loader\n",
        "from langchain_community.document_loaders import CSVLoader\n",
        "loader = CSVLoader(file_path='./csv_sample.csv')\n",
        "data = loader.load()\n",
        "print(data)\n",
        "embeddings = embeddings_model.embed_documents(\n",
        "    [\n",
        "        text.page_content for text in data\n",
        "    ]\n",
        ")\n",
        "print(len(embeddings))\n",
        "\"\"\"\n",
        "9\n",
        "\"\"\"\n",
        "\n",
        "# Embed single query\n",
        "# Embed a single piece of text for the purpose of comparing to other embedded pieces of texts.\n",
        "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
        "print(embedded_query[:5])\n",
        "\"\"\"\n",
        "[0.005354681365594307, -0.0005715346531097274, 0.03887590993433691, -0.0029596003572924623, -0.00896628532870428]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NsHNg1rK826Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings # 최신 라이브러리 권장\n",
        "from langchain_community.document_loaders import CSVLoader\n",
        "\n",
        "# 1. 모델 설정 (무료 로컬 임베딩 모델)\n",
        "# sentence-transformers/all-MiniLM-L6-v2: 가볍고 빠르며 384차원의 벡터를 생성함\n",
        "# 한국어가 포함되어 있다면 \"jhgan/ko-sroberta-multitask\" 등을 추천합니다.\n",
        "embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': 'cpu'}, # GPU가 있다면 'cuda'로 변경 가능\n",
        "    encode_kwargs={'normalize_embeddings': True} # 유사도 검색을 위해 정규화 권장\n",
        ")\n",
        "\n",
        "# 2. 텍스트 리스트 임베딩\n",
        "texts = [\n",
        "    \"Hi there!\",\n",
        "    \"Oh, hello!\",\n",
        "    \"What's your name?\",\n",
        "    \"My friends call me World\",\n",
        "    \"Hello World!\"\n",
        "]\n",
        "embeddings = embeddings_model.embed_documents(texts)\n",
        "\n",
        "print(f\"임베딩된 문서 수: {len(embeddings)}\")\n",
        "print(f\"벡터의 차원 수: {len(embeddings[0])}\") # all-MiniLM-L6-v2 모델은 384 출력\n",
        "\n",
        "# 3. CSV 로더 사용 예시\n",
        "# (./csv_sample.csv 파일이 있어야 작동합니다)\n",
        "try:\n",
        "    loader = CSVLoader(file_path='./csv_sample.csv')\n",
        "    data = loader.load()\n",
        "\n",
        "    # 리스트 컴프리헨션으로 page_content만 추출하여 임베딩\n",
        "    csv_embeddings = embeddings_model.embed_documents([doc.page_content for doc in data])\n",
        "    print(f\"CSV 임베딩 완료: {len(csv_embeddings)}개 문서\")\n",
        "except FileNotFoundError:\n",
        "    print(\"CSV 파일이 없습니다. 경로를 확인해주세요.\")\n",
        "\n",
        "# 4. 단일 쿼리 임베딩\n",
        "# 검색이나 질문을 던질 때 비교용으로 사용합니다.\n",
        "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
        "print(f\"쿼리 임베딩 결과(앞 5개): {embedded_query[:5]}\")"
      ],
      "metadata": {
        "id": "ucW4psTH9sqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Store"
      ],
      "metadata": {
        "id": "3kZvegBy_jfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install chromadb"
      ],
      "metadata": {
        "id": "YNOfnpSTA4is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from langchain_community.document_loaders import CSVLoader\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.vectorstores.chroma import Chroma\n",
        "\n",
        "# requires `pip install chromadb`\n",
        "loader = CSVLoader(file_path='./fortune_500_2020.csv')\n",
        "raw_documents = loader.load()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=0)\n",
        "documents = text_splitter.split_documents(raw_documents)\n",
        "openai_embedding = OpenAIEmbeddings()\n",
        "db = Chroma.from_documents(documents, openai_embedding, persist_directory=\"./fortune_500_db\")\n",
        "\n",
        "# save to disk\n",
        "db.persist()\n",
        "\n",
        "db_conn = Chroma(persist_directory=\"./fortune_500_db\", embedding_function=openai_embedding)\n",
        "query = \"What is JPMorgan Revenue?\"\n",
        "docs = db_conn.similarity_search(query)\n",
        "print(docs)\n",
        "print(docs[0].page_content)\n",
        "\n",
        "# # retriever\n",
        "# db_conn = Chroma(persist_directory=\"./fortune_500_db\", embedding_function=openai_embedding)\n",
        "# retriever = db_conn.as_retriever()\n",
        "# result = retriever.get_relevant_documents('walmart')\n",
        "# print(result[0].page_content)\n",
        "# \"\"\"\n",
        "# rank: 1\n",
        "# company: Walmart\n",
        "# no_of_employees: 2,200,000.00\n",
        "# rank_change: None\n",
        "# revenues: 523,964.00\n",
        "# revenue_change: 0.02\n",
        "# profits: 14,881.00\n",
        "# profit_change: 1.23\n",
        "# assets: 236,495.00\n",
        "# market_value: 321,803.30\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "vcCuUlrU9-02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import CSVLoader\n",
        "# OpenAIEmbeddings 대신 HuggingFaceEmbeddings를 사용합니다.\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.vectorstores.chroma import Chroma\n",
        "import os\n",
        "\n",
        "# 1. 문서 로드 (CSV 파일이 로컬에 있어야 함)\n",
        "loader = CSVLoader(file_path='./fortune_500_2020.csv')\n",
        "raw_documents = loader.load()\n",
        "\n",
        "# 2. 텍스트 분할 (Chunking)\n",
        "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=0)\n",
        "documents = text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# 3. Hugging Face 임베딩 모델 설정\n",
        "# 한국어/영어 범용으로 뛰어난 성능을 보이는 모델을 추천합니다.\n",
        "# 영어 전용을 원하신다면 \"sentence-transformers/all-MiniLM-L6-v2\"를 사용하세요.\n",
        "model_name = \"jhgan/ko-sroberta-multitask\"\n",
        "hf_embedding = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={'device': 'cpu'}, # GPU가 있다면 'cuda'로 변경\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "# 4. 벡터 DB(Chroma) 생성 및 저장\n",
        "# persist_directory에 DB 데이터가 저장됩니다.\n",
        "db = Chroma.from_documents(\n",
        "    documents,\n",
        "    hf_embedding,\n",
        "    persist_directory=\"./fortune_500_db\"\n",
        ")\n",
        "\n",
        "# 최신 버전의 Chroma는 persist()를 명시적으로 호출하지 않아도 자동 저장되지만,\n",
        "# 코드의 명확성을 위해 유지합니다. (일부 구버전 대응)\n",
        "# db.persist()\n",
        "\n",
        "# 5. DB 연결 및 데이터 검색\n",
        "db_conn = Chroma(\n",
        "    persist_directory=\"./fortune_500_db\",\n",
        "    embedding_function=hf_embedding\n",
        ")\n",
        "\n",
        "query = \"What is JPMorgan Revenue?\"\n",
        "# 유사도 검색 수행\n",
        "docs = db_conn.similarity_search(query, k=3)\n",
        "\n",
        "print(\"--- 검색 결과 ---\")\n",
        "print(docs[0].page_content)\n",
        "\n",
        "# 6. 리트리버(Retriever) 활용 예시\n",
        "retriever = db_conn.as_retriever(search_kwargs={\"k\": 1})\n",
        "result = retriever.invoke('walmart') # get_relevant_documents 대신 invoke 권장\n",
        "print(\"\\n--- 리트리버 검색 결과 (Walmart) ---\")\n",
        "print(result[0].page_content)"
      ],
      "metadata": {
        "id": "4CYPjLd1APSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retriever Object"
      ],
      "metadata": {
        "id": "mE2tTuz4Dh68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "\n",
        "# Build a sample vectorDB\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores.chroma import Chroma\n",
        "\n",
        "# Load blog post\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "data = loader.load()\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "splits = text_splitter.split_documents(data)\n",
        "\n",
        "# VectorDB\n",
        "embedding = OpenAIEmbeddings()\n",
        "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "question = \"What are the approaches to Task Decomposition?\"\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
        "    retriever=vectordb.as_retriever(), llm=llm)\n",
        "\n",
        "# Set logging for the queries\n",
        "import logging\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
        "\n",
        "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
        "\"\"\"\n",
        "INFO:langchain.retrievers.multi_query:Generated queries: [\n",
        "    '1. How can Task Decomposition be achieved through different methods?',\n",
        "    '2. What strategies are commonly used for breaking down tasks in Task Decomposition?',\n",
        "    '3. What are the various techniques employed in Task Decomposition to simplify complex tasks?']\n",
        "\"\"\"\n",
        "print(len(unique_docs))\n",
        "\"\"\"\n",
        "6\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CoHzJY_zAziz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "\n",
        "# Build a sample vectorDB\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores.chroma import Chroma\n",
        "\n",
        "# --- 1. 문서 로드 및 전처리 ---\n",
        "# 특정 기술 블로그 포스트를 웹에서 불러옵니다.\n",
        "# Load blog post\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "data = loader.load()\n",
        "\n",
        "# 긴 문서를 500자 단위로 쪼갭니다. (의미 보존을 위해 Recursive 사용)\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "splits = text_splitter.split_documents(data)\n",
        "\n",
        "# --- 2. 벡터 DB 생성 (HuggingFace 임베딩 모델 사용) ---\n",
        "# 로컬 자원을 사용하는 임베딩 모델을 설정합니다.\n",
        "# VectorDB\n",
        "embedding = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={'device': 'cpu'}, # GPU가 있다면 'cuda'로 변경\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "# Chroma 벡터 DB에 문서 조각들을 저장합니다.\n",
        "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "# --- 3. MultiQueryRetriever 설정 ---\n",
        "# 질문을 확장할 LLM 모델을 설정합니다. (gpt-oss-20b 사용)\n",
        "llm_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"text-generation\")\n",
        "llm = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# [핵심] 사용자의 질문을 LLM이 3개 정도의 유사 질문으로 변형하도록 설정합니다.\n",
        "# 이는 검색 키워드가 살짝 달라도 관련 문서를 더 잘 찾기 위함입니다.\n",
        "# MultiQueryRetriever 프롬프으에 '3개의 질문을 생성하라'는 지시사항 포함되어 있음\n",
        "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
        "    retriever=vectordb.as_retriever(), llm=llm)\n",
        "\n",
        "# --- 4. 로그 설정 (LLM이 생성한 질문을 확인하기 위함) ---\n",
        "# Set logging for the queries\n",
        "import logging\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
        "\n",
        "# --- 5. 질문 실행 및 검색 결과 도출 ---\n",
        "question = \"What are the approaches to Task Decomposition?\"\n",
        "\n",
        "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
        "# unique_docs = retriever_from_llm.invoke(question)\n",
        "\"\"\"\n",
        "INFO:langchain.retrievers.multi_query:Generated queries: [\n",
        "    '1. How can Task Decomposition be achieved through different methods?',\n",
        "    '2. What strategies are commonly used for breaking down tasks in Task Decomposition?',\n",
        "    '3. What are the various techniques employed in Task Decomposition to simplify complex tasks?']\n",
        "\"\"\"\n",
        "print(len(unique_docs))\n",
        "\"\"\"\n",
        "6\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3bI7eUXeFNpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Context Compression"
      ],
      "metadata": {
        "id": "DK2RgS2fIt7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain_community.vectorstores.chroma import Chroma\n",
        "\n",
        "data = TextLoader('./state_of_the_union.txt').load()\n",
        "\n",
        "# Split\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "splits = text_splitter.split_documents(data)\n",
        "\n",
        "# VectorDB\n",
        "embedding = OpenAIEmbeddings()\n",
        "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
        "\n",
        "# Helper function for printing docs\n",
        "def pretty_print_docs(docs):\n",
        "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n",
        "\n",
        "retriever = vectordb.as_retriever()\n",
        "docs = retriever.get_relevant_documents(\"What did the president say about Ketanji Brown Jackson\")\n",
        "# pretty_print_docs(docs)\n",
        "\"\"\"\n",
        "Document 1:\n",
        "\n",
        "Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections.\n",
        "\n",
        "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\n",
        "\n",
        "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\n",
        "\n",
        "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n",
        "...\n",
        "\"\"\"\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "                            base_compressor=compressor,\n",
        "                            base_retriever=retriever)\n",
        "\n",
        "compressed_docs = compression_retriever.get_relevant_documents(\n",
        "    \"What did the president say about Ketanji Brown Jackson\")\n",
        "pretty_print_docs(compressed_docs)\n",
        "\"\"\"\n",
        "Document 1:\n",
        "\n",
        "\"One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\n",
        "\n",
        "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nY3wRAR4GcRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "# Hugging Face용 임베딩과 모델 임포트\n",
        "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain_community.vectorstores.chroma import Chroma\n",
        "import os\n",
        "\n",
        "# 1. 문서 로드 (state_of_the_union.txt 파일이 필요합니다)\n",
        "# 파일이 없다면 미리 생성하거나 경로를 확인하세요.\n",
        "loader = TextLoader('./state_of_the_union.txt')\n",
        "data = loader.load()\n",
        "\n",
        "# 2. 문서 분할\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "splits = text_splitter.split_documents(data)\n",
        "\n",
        "# 3. 로컬 임베딩 모델 설정\n",
        "embedding = HuggingFaceEmbeddings(\n",
        "    model_name=\"jhgan/ko-sroberta-multitask\", # 한국어/영어 범용 모델\n",
        "    model_kwargs={'device': 'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "# 4. Vector DB 생성 (Chroma)\n",
        "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
        "\n",
        "# 5. Hugging Face LLM 설정 (압축 작업을 수행할 뇌 역할)\n",
        "llm_ep = HuggingFaceEndpoint(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\", # 요약 및 추출 성능이 좋은 모델 추천\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512\n",
        ")\n",
        "llm = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# 6. 문맥 압축 리트리버 설정\n",
        "# LLMChainExtractor는 검색된 문서에서 질문과 관련된 부분만 추출합니다.\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "\n",
        "base_retriever = vectordb.as_retriever()\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=base_retriever\n",
        ")\n",
        "\n",
        "# 7. 실행 및 결과 출력\n",
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "\n",
        "# 일반 검색 결과\n",
        "print(\"\\n[일반 검색 결과 개수]:\", len(base_retriever.invoke(query)))\n",
        "\n",
        "# 압축 검색 결과 (핵심 내용만 추출됨)\n",
        "compressed_docs = compression_retriever.invoke(query)\n",
        "\n",
        "def pretty_print_docs(docs):\n",
        "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n",
        "\n",
        "pretty_print_docs(compressed_docs)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TDc_7B8VI0bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z1MpqTNHI_6_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}