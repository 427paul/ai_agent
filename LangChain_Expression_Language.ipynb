{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "11c2c332db6a430ebe527b2290d8fd89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c30e0dc805444bc9bc159dcaedfbe11",
              "IPY_MODEL_2f793add08304d30a9b567d8566bd3af",
              "IPY_MODEL_514bd397fdda49e99fd1fbe6e219672e"
            ],
            "layout": "IPY_MODEL_d0553f7e43074b22a3a670677e288589"
          }
        },
        "5c30e0dc805444bc9bc159dcaedfbe11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec633c956cb94655ae66c352ba1af4b9",
            "placeholder": "​",
            "style": "IPY_MODEL_05dea3f94b5e45e2bd3f71e149474c87",
            "value": "Loading weights: 100%"
          }
        },
        "2f793add08304d30a9b567d8566bd3af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4862634035064ede9a381e6168bfcec4",
            "max": 103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a34b651b9ee14ca789251f84f881e39d",
            "value": 103
          }
        },
        "514bd397fdda49e99fd1fbe6e219672e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2923b889a95640ada7afa549cc3e60f9",
            "placeholder": "​",
            "style": "IPY_MODEL_485ac650d8d14d63b40a11d87ae2b3f2",
            "value": " 103/103 [00:00&lt;00:00, 464.74it/s, Materializing param=pooler.dense.weight]"
          }
        },
        "d0553f7e43074b22a3a670677e288589": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec633c956cb94655ae66c352ba1af4b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05dea3f94b5e45e2bd3f71e149474c87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4862634035064ede9a381e6168bfcec4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a34b651b9ee14ca789251f84f881e39d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2923b889a95640ada7afa549cc3e60f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "485ac650d8d14d63b40a11d87ae2b3f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyFfU5k10RDm",
        "outputId": "06b44d24-da84-4ae2-adba-a97c35276726"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-experimental 0.4.1 requires langchain-community<1.0.0,>=0.4.0, but you have langchain-community 0.3.31 which is incompatible.\n",
            "langchain-experimental 0.4.1 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.3.83 which is incompatible.\n",
            "langchain-classic 1.0.1 requires langchain-core<2.0.0,>=1.2.5, but you have langchain-core 0.3.83 which is incompatible.\n",
            "langchain-classic 1.0.1 requires langchain-text-splitters<2.0.0,>=1.1.0, but you have langchain-text-splitters 0.3.11 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U \"langchain==0.3.*\" \"langchain-core==0.3.*\" \"langchain-community==0.3.*\" \"langgraph==0.3.*\" \"langchain-huggingface\" \"huggingface_hub\" \"sentence-transformers\" wikipedia -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z68J7Xf0a3B",
        "outputId": "986720ab-0b50-4adf-fc14-ee1d9f3f6360"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def load_api_keys(filepath=\"api_key.txt\"):\n",
        "    with open(filepath, \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line and \"=\" in line:\n",
        "                key, value = line.split(\"=\", 1)\n",
        "                os.environ[key.strip()] = value.strip()\n",
        "\n",
        "path = '/content/drive/MyDrive/LangGraph/'\n",
        "\n",
        "# API 키 로드 및 환경변수 설정\n",
        "load_api_keys(path + 'api_key.txt')"
      ],
      "metadata": {
        "id": "15yLmyOq0cAx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prompt llm output parser"
      ],
      "metadata": {
        "id": "DMFPg6yA002E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
        "prompt_value = prompt.invoke({\"topic\": \"ice cream\"})\n",
        "\"\"\"\n",
        "messages=[HumanMessage(content='tell me a short joke about ice cream')]\n",
        "\"\"\"\n",
        "print(prompt_value.to_string())\n",
        "\"\"\"\n",
        "Human: tell me a short joke about ice cream\n",
        "\"\"\"\n",
        "\n",
        "model = ChatOpenAI()\n",
        "message = model.invoke(prompt_value)\n",
        "\"\"\"\n",
        "content='Why did the ice cream go to therapy? \\n\\nBecause it had too many scoops of emotions!'\n",
        "\"\"\"\n",
        "\n",
        "# from langchain.llms import OpenAI\n",
        "# llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
        "# llm_message = llm.invoke(prompt_value)\n",
        "\"\"\"\n",
        "Why did the ice cream go to therapy?\n",
        "\n",
        "Because it was having a meltdown!\n",
        "\"\"\"\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "print(output_parser.invoke(message))\n",
        "\"\"\"\n",
        "Why did the ice cream go to therapy?\n",
        "\n",
        "Because it had a meltdown!\n",
        "\"\"\"\n",
        "\n",
        "# similar to unix pipe operator\n",
        "chain = prompt | model | output_parser\n",
        "\n",
        "result = chain.invoke({\"topic\": \"ice cream\"})\n",
        "print(result)\n",
        "\"\"\"\n",
        "Why did the ice cream go to therapy?\n",
        "\n",
        "Because it had too many sprinkles of anxiety!\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "rM6ogqEM0ij_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# 1. 프롬프트 템플릿 정의\n",
        "# {topic} 자리에 사용자가 원하는 주제를 넣을 수 있는 대화형 프롬프트입니다.\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
        "\n",
        "# 프롬프트 단독 실행 테스트 (입력값을 넣어 메시지 객체로 변환)\n",
        "prompt_value = prompt.invoke({\"topic\": \"ice cream\"})\n",
        "\"\"\"\n",
        "messages=[HumanMessage(content='tell me a short joke about ice cream')]\n",
        "\"\"\"\n",
        "print(prompt_value.to_string())\n",
        "\"\"\"\n",
        "Human: tell me a short joke about ice cream\n",
        "\"\"\"\n",
        "\n",
        "# 2. 모델 설정 (HuggingFace 엔드포인트 사용)\n",
        "# gpt-oss-20b 모델을 호출하는 엔드포인트를 생성합니다.\n",
        "llm_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"text-generation\")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# 모델 단독 실행 테스트 (프롬프트 결과값을 모델에 전달)\n",
        "# 결과는 AIMessage 객체 형태로 반환됩니다. (내용은 content 속성에 담김)\n",
        "message = model.invoke(prompt_value)\n",
        "\"\"\"\n",
        "content='Why did the ice cream go to therapy? \\n\\nBecause it had too many scoops of emotions!'\n",
        "\"\"\"\n",
        "\n",
        "# from langchain.llms import OpenAI\n",
        "# llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
        "# llm_message = llm.invoke(prompt_value)\n",
        "\"\"\"\n",
        "Why did the ice cream go to therapy?\n",
        "\n",
        "Because it was having a meltdown!\n",
        "\"\"\"\n",
        "\n",
        "# 3. 출력 파서 설정\n",
        "# 모델의 응답(객체)에서 텍스트(string)만 깔끔하게 추출해주는 파서입니다.\n",
        "output_parser = StrOutputParser()\n",
        "# 파서 단독 실행 테스트\n",
        "print(output_parser.invoke(message))\n",
        "\"\"\"\n",
        "Why did the ice cream go to therapy?\n",
        "\n",
        "Because it had a meltdown!\n",
        "\"\"\"\n",
        "\n",
        "# --- 4. LCEL(LangChain Expression Language) 체인 생성 ---\n",
        "# | 연산자를 사용하여 각 단계를 하나로 묶습니다. (Unix 파이프와 동일한 원리)\n",
        "# 데이터 흐름: 입력(dict) -> 프롬프트 -> 모델 -> 출력 파서 -> 최종 문자열\n",
        "# similar to unix pipe operator\n",
        "chain = prompt | model | output_parser\n",
        "\n",
        "# 5. 체인 실행 (전체 과정을 한 번에 수행)\n",
        "result = chain.invoke({\"topic\": \"ice cream\"})\n",
        "print(result)\n",
        "\"\"\"\n",
        "Why did the ice cream go to therapy?\n",
        "\n",
        "Because it had too many sprinkles of anxiety!\n",
        "# \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "-c5ogi2i2BPe",
        "outputId": "49ad395a-5ecb-4f45-d0cc-5965d0028a65"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: tell me a short joke about ice cream\n",
            "Why did the ice cream start a band? Because it wanted to be a “scoop” of music!\n",
            "Why did the ice cream keep a diary?  \n",
            "Because it was always “melting” under pressure!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWhy did the ice cream go to therapy?\\n\\nBecause it had too many sprinkles of anxiety!\\n# '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prompt llm function call"
      ],
      "metadata": {
        "id": "CMbGuJY7-tqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.output_parsers.openai_functions import (\n",
        "    JsonOutputFunctionsParser,\n",
        "    JsonKeyOutputFunctionsParser)\n",
        "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
        "model = ChatOpenAI()\n",
        "\n",
        "# function call\n",
        "functions = [\n",
        "    {\n",
        "        \"name\": \"joke\",\n",
        "        \"description\": \"A joke\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"setup\": {\"type\": \"string\", \"description\": \"The setup for the joke\"},\n",
        "                \"punchline\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The punchline for the joke\",\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"setup\", \"punchline\"],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "chain = prompt | model.bind(function_call={\"name\": \"joke\"}, functions=functions)\n",
        "# print(chain.invoke({\"topic\": \"ice cream\"}, config={}))\n",
        "\"\"\"\n",
        "content='' additional_kwargs={'function_call': {'arguments': '{\\n  \"setup\": \"Why did the ice cream go to therapy?\",\\n  \"punchline\": \"Because it was feeling a little melty!\"\\n}', 'name': 'joke'}}\n",
        "\"\"\"\n",
        "\n",
        "# chain = prompt | model.bind(function_call={\"name\": \"joke\"}, functions=functions) | JsonOutputFunctionsParser()\n",
        "# print(chain.invoke({\"topic\": \"ice cream\"}))\n",
        "\"\"\"\n",
        "{'setup': 'Why did the ice cream go to therapy?', 'punchline': 'Because it had too many sprinkles of anxiety!'}\n",
        "\"\"\"\n",
        "\n",
        "# chain = prompt | model.bind(function_call={\"name\": \"joke\"}, functions=functions) | JsonKeyOutputFunctionsParser(key_name=\"setup\")\n",
        "# print(chain.invoke({\"topic\": \"ice cream\"}))\n",
        "\"\"\"\n",
        "Why did the ice cream go to therapy?\n",
        "\"\"\"\n",
        "\n",
        "map_ = RunnableParallel(topic=RunnablePassthrough())\n",
        "chain = (\n",
        "    map_\n",
        "    | prompt\n",
        "    | model.bind(function_call={\"name\": \"joke\"}, functions=functions)\n",
        "    | JsonKeyOutputFunctionsParser(key_name=\"setup\")\n",
        ")\n",
        "print(chain.invoke(\"ice cream\"))\n",
        "\"\"\"\n",
        "Why did the ice cream go to therapy?\n",
        "\"\"\"\n",
        "\n",
        "chain = (\n",
        "    {\"topic\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model.bind(function_call={\"name\": \"joke\"}, functions=functions)\n",
        "    | JsonKeyOutputFunctionsParser(key_name=\"setup\")\n",
        ")\n",
        "print(chain.invoke(\"ice cream\"))\n",
        "\"\"\"\n",
        "Why did the ice cream break up with the cone?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "iFeLt5Ku2by7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8_yisuuADZa",
        "outputId": "59cc2b26-43d0-474c-a547-499a98beeff1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (0.3.83)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.6.6)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (2.12.3)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# 1. HF_TOKEN 설정 (선택 사항이지만 권장)\n",
        "# Colab 왼쪽 열쇠 아이콘(Secrets)에서 HF_TOKEN을 추가하거나 직접 입력하세요.\n",
        "# os.environ[\"HF_TOKEN\"] = \"your_huggingface_token_here\"\n",
        "\n",
        "# 2. 모델 설정 (대화형 태스크에 최적화된 Llama-3.2 사용)\n",
        "repo_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    task=\"text-generation\", # 내부 엔진용\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "# Chat 모델로 래핑 (conversational 태스크 대응)\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "# 3. JSON 형식을 강력하게 요구하는 프롬프트\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are a joke generator.\n",
        "    Respond ONLY with a JSON object containing 'setup' and 'punchline' keys.\n",
        "    Do not include any other text or explanation.\n",
        "\n",
        "    Topic: {topic}\"\"\"\n",
        ")\n",
        "\n",
        "# 4. 체인 구성\n",
        "chain = (\n",
        "    {\"topic\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | JsonOutputParser()\n",
        ")\n",
        "\n",
        "# 5. 실행\n",
        "try:\n",
        "    print(\"--- 결과 확인 ---\")\n",
        "    result = chain.invoke(\"ice cream\")\n",
        "    print(f\"성공! 결과: {result}\")\n",
        "    print(f\"Setup: {result.get('setup')}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"에러 발생: {e}\")\n",
        "    # 실패 시 모델이 보낸 생문자열 확인 (디버깅용)\n",
        "    debug_chain = {\"topic\": RunnablePassthrough()} | prompt | model\n",
        "    debug_res = debug_chain.invoke(\"ice cream\")\n",
        "    print(f\"모델 원본 응답: {debug_res.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIOgD3aH-zv1",
        "outputId": "5dd44424-a843-48e4-c250-5afc9386f3a1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 결과 확인 ---\n",
            "성공! 결과: {'setup': 'Why did the ice cream go to therapy?', 'punchline': 'Because it had a meltdown!'}\n",
            "Setup: Why did the ice cream go to therapy?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval-augmented generation(RAG)"
      ],
      "metadata": {
        "id": "EaDmzBx3EJh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from operator import itemgetter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
        "from langchain_community.vectorstores.faiss import FAISS\n",
        "\n",
        "# It requires `pip install langchain openai faiss-cpu tiktoken`\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_texts(\n",
        "    [\"harrison worked at kensho\"], embedding=embedding_model\n",
        ")\n",
        "# save\n",
        "vectorstore.save_local(\"faiss_index\")\n",
        "\n",
        "vectorstore_new = FAISS.load_local(\"faiss_index\", embedding_model)\n",
        "\n",
        "retriever = vectorstore_new.as_retriever()\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(chain.invoke(\"where did harrison work?\"))\n",
        "\"\"\"\n",
        "Harrison worked at Kensho.\n",
        "\"\"\"\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer in the following language: {language}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "        \"language\": itemgetter(\"language\"),\n",
        "    }\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(chain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"}))\n",
        "\"\"\"\n",
        "Harrison ha lavorato a Kensho.\n",
        "\"\"\"\n",
        "\n",
        "# # itemgetter example\n",
        "# from operator import itemgetter\n",
        "# # Suppose we have a dictionary\n",
        "# person = {'name': 'Alice', 'age': 30, 'job': 'Engineer'}\n",
        "# # We can use itemgetter to create a function that fetches the 'name' from a dictionary\n",
        "# get_name = itemgetter('name')\n",
        "# # Now, when we use this function with our dictionary\n",
        "# name = get_name(person)\n",
        "# print(name)  # Output: Alice"
      ],
      "metadata": {
        "id": "MPiwOSX9-8lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyBUszOVElNg",
        "outputId": "d827b1cd-0138-4589-f9e5-7b0b9cfd1482"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from operator import itemgetter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# 1. 임베딩 모델 설정 (로컬 HuggingFace 모델)\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# 2. 벡터스토어 생성 및 로드 (FAISS)\n",
        "# 주의: allow_dangerous_deserialization=True는 로컬에서 직접 만든 인덱스를 로드할 때 필요합니다.\n",
        "vectorstore = FAISS.from_texts(\n",
        "    [\"harrison worked at kensho\"], embedding=embeddings\n",
        ")\n",
        "vectorstore.save_local(\"faiss_index\")\n",
        "\n",
        "vectorstore_new = FAISS.load_local(\n",
        "    \"faiss_index\", embeddings, allow_dangerous_deserialization=True\n",
        ")\n",
        "retriever = vectorstore_new.as_retriever()\n",
        "\n",
        "# 3. LLM 설정 (Hugging Face 오픈소스 모델)\n",
        "# llm_ep = HuggingFaceEndpoint(\n",
        "#     repo_id=\"meta-llama/Llama-3.2-3B-Instruct\", # 또는 \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "#     task=\"text-generation\",\n",
        "#     max_new_tokens=512,\n",
        "#     temperature=0.1\n",
        "# )\n",
        "llm_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"text-generation\")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# --- 시나리오 1: 단순 질문 전달 ---\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"--- Scenario 1 Result ---\")\n",
        "print(chain.invoke(\"where did harrison work?\"))\n",
        "\n",
        "\n",
        "# --- 시나리오 2: itemgetter를 이용한 다중 파라미터 전달 ---\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer in the following language: {language}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# [핵심] 입력 데이터 딕셔너리에서 원하는 키값만 뽑아서 각 컴포넌트에 전달합니다.\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | retriever, # 질문만 뽑아서 리트리버에 전달\n",
        "        \"question\": itemgetter(\"question\"),           # 질문만 뽑아서 프롬프트에 전달\n",
        "        \"language\": itemgetter(\"language\"),           # 언어 설정만 뽑아서 프롬프트에 전달\n",
        "    }\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"\\n--- Scenario 2 Result (Italian) ---\")\n",
        "print(chain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289,
          "referenced_widgets": [
            "11c2c332db6a430ebe527b2290d8fd89",
            "5c30e0dc805444bc9bc159dcaedfbe11",
            "2f793add08304d30a9b567d8566bd3af",
            "514bd397fdda49e99fd1fbe6e219672e",
            "d0553f7e43074b22a3a670677e288589",
            "ec633c956cb94655ae66c352ba1af4b9",
            "05dea3f94b5e45e2bd3f71e149474c87",
            "4862634035064ede9a381e6168bfcec4",
            "a34b651b9ee14ca789251f84f881e39d",
            "2923b889a95640ada7afa549cc3e60f9",
            "485ac650d8d14d63b40a11d87ae2b3f2"
          ]
        },
        "id": "BholklytEN89",
        "outputId": "bf19d045-3f45-435f-e0ed-feaaac399b05"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11c2c332db6a430ebe527b2290d8fd89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Scenario 1 Result ---\n",
            "Harrison worked at **Kensho**.\n",
            "\n",
            "--- Scenario 2 Result (Italian) ---\n",
            "Harrison ha lavorato a Kensho.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runnable Protocol"
      ],
      "metadata": {
        "id": "vK1kWVpJFvrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Runnable Lambda"
      ],
      "metadata": {
        "id": "anUPJhMeKSc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from operator import itemgetter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "\n",
        "def length_function(text):\n",
        "    return len(text)\n",
        "\n",
        "def _multiple_length_function(text1, text2):\n",
        "    return len(text1) * len(text2)\n",
        "\n",
        "def multiple_length_function(_dict):\n",
        "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
        "model = ChatOpenAI()\n",
        "chain = (\n",
        "    {\n",
        "        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),\n",
        "        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}\n",
        "        | RunnableLambda(multiple_length_function),\n",
        "    }\n",
        "    | prompt\n",
        "    | model\n",
        ")\n",
        "print(chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"}))\n",
        "\"\"\"\n",
        "content='3 + 9 = 12'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cYOjZ47UEUui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from operator import itemgetter\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 1. 사용자 정의 함수 정의\n",
        "# 각 함수는 입력값을 받아 특정 연산을 수행합니다.\n",
        "def length_function(text):\n",
        "    return len(text)\n",
        "\n",
        "def _multiple_length_function(text1, text2):\n",
        "    return len(text1) * len(text2)\n",
        "\n",
        "def multiple_length_function(_dict):\n",
        "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
        "\n",
        "# 2. 모델 설정 (Hugging Face 오픈소스 모델)\n",
        "# Llama-3 또는 Qwen2.5 같은 고성능 모델을 사용하면 계산 결과를 더 잘 설명합니다.\n",
        "llm_ep = HuggingFaceEndpoint(\n",
        "    repo_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.1\n",
        ")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# 3. 프롬프트 정의\n",
        "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
        "\n",
        "# 4. 체인 구성 (LCEL)\n",
        "# RunnableLambda를 사용해 일반 파이썬 함수를 체인의 구성 요소로 변환합니다.\n",
        "chain = (\n",
        "    {\n",
        "        # \"foo\"의 길이를 계산하여 'a'에 할당\n",
        "        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),\n",
        "\n",
        "        # \"foo\"와 \"bar\"의 길이를 곱하여 'b'에 할당\n",
        "        \"b\": (\n",
        "            {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}\n",
        "            | RunnableLambda(multiple_length_function)\n",
        "        ),\n",
        "    }\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser() # 깔끔한 텍스트 출력을 위해 추가\n",
        ")\n",
        "\n",
        "# 5. 실행\n",
        "# foo(3글자), bar(3글자) -> a=3, b=3*3=9 -> \"what is 3 + 9\"\n",
        "print(\"--- 연산 결과 ---\")\n",
        "print(chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sc7tkMUvKfZ-",
        "outputId": "9079945f-7191-4471-da56-934eefcf7390"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 연산 결과 ---\n",
            "3 + 9 = 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Runnable Branch"
      ],
      "metadata": {
        "id": "tTPyKTc8KVhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"Given the user question below, classify it as either being about `Strawberry`, `Banana`, or `Other`.\n",
        "\n",
        "Do not respond with more than one word.\n",
        "\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\n",
        "Classification:\"\"\"\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(chain.invoke({\"question\": \"What is the fruit that has red color?\"}))\n",
        "\"\"\"\n",
        "Strawberry\n",
        "\"\"\"\n",
        "\n",
        "strawberry_chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"You are an expert about strawberry. \\\n",
        "Always answer questions starting with \"As a Strawberry expert ... \". \\\n",
        "Respond to the following question:\n",
        "\n",
        "# Question: {question}\n",
        "# Answer:\"\"\"\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        ")\n",
        "\n",
        "banana_chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"You are an expert about banana. \\\n",
        "Always answer questions starting with \"As a Banana expert ... \". \\\n",
        "Respond to the following question:\n",
        "\n",
        "# Question: {question}\n",
        "# Answer:\"\"\"\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        ")\n",
        "\n",
        "general_chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"Respond to the following question:\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        ")\n",
        "\n",
        "\n",
        "from langchain.schema.runnable import RunnableBranch\n",
        "\n",
        "# the first element is a condition (a lambda function) and\n",
        "# the second element is the chain to execute if the condition is true\n",
        "branch = RunnableBranch(\n",
        "    (lambda x: \"strawberry\" in x[\"topic\"].lower(), strawberry_chain), # type: ignore\n",
        "    (lambda x: \"banana\" in x[\"topic\"].lower(), banana_chain), # type: ignore\n",
        "    general_chain,\n",
        ")\n",
        "\n",
        "# chain is invoked to classify the question, and its output is stored under the key topic.\n",
        "# The original question is passed through unchanged under the key question.\n",
        "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | branch\n",
        "\n",
        "print(full_chain.invoke({\"question\": \"What is the fruit that has red color?\"}))\n",
        "print(full_chain.invoke({\"question\": \"What is the fruit that has yellow color?\"}))\n",
        "print(full_chain.invoke({\"question\": \"What is the fruit that has green color?\"}))\n",
        "\"\"\"\n",
        "content='As a Strawberry expert, I can tell you that strawberries are the fruit that has a vibrant red color.'\n",
        "content='As a Banana expert, the fruit that has a yellow color is the banana.'\n",
        "content='The fruit that has a green color is typically an apple or a lime.'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UK6NVQORKZPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_core.runnables import RunnableBranch\n",
        "\n",
        "# 1. LLM 설정 (Hugging Face 오픈소스 모델)\n",
        "# 분류 및 전문 답변 능력이 좋은 모델을 사용합니다.\n",
        "llm_ep = HuggingFaceEndpoint(\n",
        "    repo_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.1\n",
        ")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# 2. 질문 분류 체인 (Classifier)\n",
        "# 입력된 질문을 Strawberry, Banana, Other 중 하나로 분류합니다.\n",
        "classification_chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"Given the user question below, classify it as either being about `Strawberry`, `Banana`, or `Other`.\n",
        "Do not respond with more than one word.\n",
        "\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\n",
        "Classification:\"\"\"\n",
        "    )\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 3. 각 주제별 전문 체인 (Sub-chains)\n",
        "strawberry_chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"You are an expert about strawberry. Always answer questions starting with \"As a Strawberry expert ... \".\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "    )\n",
        "    | model\n",
        ")\n",
        "\n",
        "banana_chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"You are an expert about banana. Always answer questions starting with \"As a Banana expert ... \".\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "    )\n",
        "    | model\n",
        ")\n",
        "\n",
        "general_chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"Respond to the following question:\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "    )\n",
        "    | model\n",
        ")\n",
        "\n",
        "# 4. RunnableBranch 설정 (분기 로직)\n",
        "# \"topic\" 결과값에 따라 어느 체인으로 갈지 결정합니다.\n",
        "branch = RunnableBranch(\n",
        "    (lambda x: \"strawberry\" in x[\"topic\"].lower(), strawberry_chain),\n",
        "    (lambda x: \"banana\" in x[\"topic\"].lower(), banana_chain),\n",
        "    general_chain,\n",
        ")\n",
        "\n",
        "# 5. 전체 체인 구성 (Full Chain)\n",
        "# 먼저 분류(topic)를 수행하고, 원본 질문(question)과 함께 branch로 넘깁니다.\n",
        "full_chain = {\"topic\": classification_chain, \"question\": lambda x: x[\"question\"]} | branch\n",
        "\n",
        "# 6. 실행 및 테스트\n",
        "print(\"--- 결과 확인 ---\")\n",
        "res1 = full_chain.invoke({\"question\": \"What is the fruit that has red color?\"})\n",
        "print(f\"Red: {res1.content}\")\n",
        "\n",
        "res2 = full_chain.invoke({\"question\": \"What is the fruit that has yellow color?\"})\n",
        "print(f\"Yellow: {res2.content}\")\n",
        "\n",
        "res3 = full_chain.invoke({\"question\": \"What is the fruit that has green color?\"})\n",
        "print(f\"Green: {res3.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAr3CMXpLByU",
        "outputId": "89113503-6bd9-4465-d67f-b2d89e98390f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 결과 확인 ---\n",
            "Red: As a Strawberry expert, I can tell you that strawberries are indeed one of the fruits that have a red color. However, it's worth noting that strawberries are actually aggregate fruits, meaning that they're formed from multiple ovaries of a single flower. Each \"seed\" on the surface of a strawberry is actually an individual fruit, and they're all fused together to form the single fruit. But if you're thinking of a single fruit with a red color, strawberries are definitely one of the top contenders!\n",
            "Yellow: I can't provide information on a specific fruit that has a yellow color, as the question doesn't specify a type of fruit. If you'd like to ask about strawberries, I can answer.\n",
            "\n",
            "As a Strawberry expert, I can tell you that strawberries are typically red in color, but they can vary in color depending on the variety and ripeness. Some strawberries may have a yellow or white tint to their skin, but this is not a characteristic of the fruit itself.\n",
            "Green: Unfortunately, there are many fruits that have a green color, so it's difficult to give a specific answer. However, some common fruits that are typically green include:\n",
            "\n",
            "* Honeydew melon\n",
            "* Green apples\n",
            "* Green grapes\n",
            "* Pears\n",
            "* Limes\n",
            "* Kiwi\n",
            "\n",
            "If you could provide more information or clarify which specific fruit you are thinking of, I'd be happy to give a more precise answer!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Runnable Parallel"
      ],
      "metadata": {
        "id": "5I0qMT7dKZwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from operator import itemgetter\n",
        "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "runnable = RunnableParallel(\n",
        "    passed=RunnablePassthrough()\n",
        ")\n",
        "\n",
        "print(runnable.invoke({\"num\": 1}))\n",
        "\"\"\"\n",
        "{'passed': {'num': 1}}\n",
        "\"\"\"\n",
        "\n",
        "runnable = RunnableParallel(\n",
        "    passed=RunnablePassthrough(),\n",
        "    extra=RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),\n",
        "    modified=lambda x: x[\"num\"] + 1,\n",
        ")\n",
        "\n",
        "print(runnable.invoke({\"num\": 1}))\n",
        "\"\"\"\n",
        "{'passed': {'num': 1}, 'extra': {'num': 1, 'mult': 3}, 'modified': 2}\n",
        "\"\"\"\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnableParallel\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n",
        "poem_chain = (\n",
        "    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model\n",
        ")\n",
        "\n",
        "# easy to execute multiple Runnables in parallel\n",
        "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
        "\n",
        "print(map_chain.invoke({\"topic\": \"bear\"}))\n",
        "\"\"\"\n",
        "{\n",
        "    'joke': AIMessage(content=\"Why did the bear break up with his girlfriend? \\n\\nBecause he couldn't bear the relationship any longer!\"),\n",
        "    'poem': AIMessage(content='In the dark woods, a bear roams free,\\nA majestic creature, wild and full of mystery.')\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LEcp_RWoKeTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# --- 1. 데이터 가공 기본 (RunnablePassthrough & assign) ---\n",
        "# 이 부분은 모델과 상관없이 LangChain의 핵심 로직이므로 동일하게 작동합니다.\n",
        "\n",
        "# 입력받은 값을 그대로 'passed' 키에 담습니다.\n",
        "runnable_basic = RunnableParallel(\n",
        "    passed=RunnablePassthrough()\n",
        ")\n",
        "print(\"--- Basic Invoke ---\")\n",
        "print(runnable_basic.invoke({\"num\": 1})) # {'passed': {'num': 1}}\n",
        "\n",
        "# assign을 사용해 기존 데이터를 유지하면서 새로운 값을 추가합니다.\n",
        "runnable_complex = RunnableParallel(\n",
        "    passed=RunnablePassthrough(),\n",
        "    extra=RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),\n",
        "    modified=lambda x: x[\"num\"] + 1,\n",
        ")\n",
        "print(\"\\n--- Complex Invoke ---\")\n",
        "print(runnable_complex.invoke({\"num\": 1}))\n",
        "# {'passed': {'num': 1}, 'extra': {'num': 1, 'mult': 3}, 'modified': 2}\n",
        "\n",
        "\n",
        "# --- 2. 병렬 체인 실행 (Hugging Face 모델 연동) ---\n",
        "\n",
        "# 모델 설정 (Llama-3.2 사용)\n",
        "llm_ep = HuggingFaceEndpoint(\n",
        "    repo_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.7\n",
        ")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# 농담 체인과 시(Poem) 체인을 각각 정의합니다.\n",
        "joke_chain = (\n",
        "    ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
        "    | model\n",
        "    | StrOutputParser() # 텍스트만 깔끔하게 추출\n",
        ")\n",
        "poem_chain = (\n",
        "    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\")\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# [핵심] RunnableParallel을 사용하여 두 체인을 묶습니다.\n",
        "# invoke 하나로 joke와 poem이 동시에 생성됩니다.\n",
        "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
        "\n",
        "print(\"\\n--- Map Chain Invoke (Parallel) ---\")\n",
        "result = map_chain.invoke({\"topic\": \"bear\"})\n",
        "print(f\"Joke: {result['joke']}\")\n",
        "print(f\"Poem: {result['poem']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OCWSTxqLI_e",
        "outputId": "9fca2ff8-9bd7-46ac-a657-d4560ab44baf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Basic Invoke ---\n",
            "{'passed': {'num': 1}}\n",
            "\n",
            "--- Complex Invoke ---\n",
            "{'passed': {'num': 1}, 'extra': {'num': 1, 'mult': 3}, 'modified': 2}\n",
            "\n",
            "--- Map Chain Invoke (Parallel) ---\n",
            "Joke: Why did the bear go to the doctor?\n",
            "\n",
            "Because it had a grizzly cough!\n",
            "Poem: Here's a 2-line poem about a bear:\n",
            "\n",
            "A gentle bear in forest deep,\n",
            "Roams freely, in a peaceful sleep.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple Chains"
      ],
      "metadata": {
        "id": "tbeyoOcmMTro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from operator import itemgetter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "# Example1\n",
        "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
        "prompt2 = ChatPromptTemplate.from_template(\n",
        "    \"what country is the city {city} in? respond in {language}\"\n",
        ")\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "chain1 = prompt1 | model | StrOutputParser()\n",
        "\n",
        "chain2 = (\n",
        "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
        "    | prompt2\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(chain2.invoke({\"person\": \"obama\", \"language\": \"english\"}))\n",
        "\"\"\"\n",
        "Barack Obama, the 44th President of the United States, was born in Honolulu, Hawaii, which is located in the United States of America.\n",
        "\"\"\"\n",
        "\n",
        "# Example2\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# Generates a prompt asking for a color based on a given attribute.\n",
        "prompt1 = ChatPromptTemplate.from_template(\n",
        "    \"generate a {attribute} color. Return the name of the color and nothing else:\"\n",
        ")\n",
        "\n",
        "# Asks for a fruit of a specified color.\n",
        "prompt2 = ChatPromptTemplate.from_template(\n",
        "    \"what is a fruit of color: {color}. Return the name of the fruit and nothing else:\"\n",
        ")\n",
        "\n",
        "# Requests the name of a country with a flag containing a certain color.\n",
        "prompt3 = ChatPromptTemplate.from_template(\n",
        "    \"what is a country with a flag that has the color: {color}. Return the name of the country and nothing else:\"\n",
        ")\n",
        "\n",
        "# Forms a prompt asking for the color of a specific fruit and the flag of a specific country\n",
        "prompt4 = ChatPromptTemplate.from_template(\n",
        "    \"What is the color of {fruit} and the flag of {country}?\"\n",
        ")\n",
        "\n",
        "# Extract model message\n",
        "model_parser = model | StrOutputParser()\n",
        "\n",
        "# Generating Color\n",
        "color_generator = (\n",
        "    {\"attribute\": RunnablePassthrough()} | prompt1 | {\"color\": model_parser}\n",
        ")\n",
        "\n",
        "# Takes a color and uses prompt2 to ask for a corresponding fruit\n",
        "color_to_fruit = prompt2 | model_parser\n",
        "\n",
        "# uses prompt3 to find a country with a flag containing that color\n",
        "color_to_country = prompt3 | model_parser\n",
        "\n",
        "\n",
        "question_generator = (\n",
        "    color_generator | {\"fruit\": color_to_fruit, \"country\": color_to_country} | prompt4\n",
        ")\n",
        "\n",
        "prompt = question_generator.invoke(\"warm\")\n",
        "print(prompt)\n",
        "\"\"\"\n",
        "messages=[HumanMessage(content='What is the color of Coral. and the flag of Comoros?')]\n",
        "\"\"\"\n",
        "\n",
        "print(model.invoke(prompt))\n",
        "\"\"\"\n",
        "content='The color of a pomegranate is typically a deep red or maroon. The flag of Armenia consists of three horizontal bands of equal width - the top band is red, the middle band is blue, and the bottom band is orange.'\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Example3\n",
        "# Branching and Merging\n",
        "\"\"\"\n",
        "     Input\n",
        "      / \\\n",
        "     /   \\\n",
        " Branch1 Branch2\n",
        "     \\   /\n",
        "      \\ /\n",
        "      Combine\n",
        "\"\"\"\n",
        "planner = (\n",
        "    ChatPromptTemplate.from_template(\"Generate an argument about: {input}\")\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        "    | {\"base_response\": RunnablePassthrough()}\n",
        ")\n",
        "\n",
        "arguments_for = (\n",
        "    ChatPromptTemplate.from_template(\n",
        "        \"List the pros or positive aspects of {base_response}\"\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        ")\n",
        "arguments_against = (\n",
        "    ChatPromptTemplate.from_template(\n",
        "        \"List the cons or negative aspects of {base_response}\"\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_responder = (\n",
        "    ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"ai\", \"{original_response}\"),\n",
        "            (\"human\", \"Pros:\\n{results_1}\\n\\nCons:\\n{results_2}\"),\n",
        "            (\"system\", \"Generate a final response given the critique\"),\n",
        "        ]\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain = (\n",
        "    planner\n",
        "    | {\n",
        "        \"results_1\": arguments_for,\n",
        "        \"results_2\": arguments_against,\n",
        "        \"original_response\": itemgetter(\"base_response\"),\n",
        "    }\n",
        "    | final_responder\n",
        ")\n",
        "\n",
        "print(chain.invoke({\"input\": \"scrum\"}))\n",
        "\"\"\"\n",
        "While Scrum has its pros and cons, it is important to recognize that no project management framework is a one-size-fits-all solution. The cons mentioned should be considered in the context of the specific project and organization.\n",
        "\n",
        "For example, while Scrum may have a lack of predictability, this can be mitigated by implementing effective estimation techniques and regularly reassessing and adjusting plans. Additionally, while Scrum relies on team communication, organizations can invest in improving communication practices and tools to address any gaps or issues.\n",
        "\n",
        "Similarly, while Scrum may have limitations for large projects, organizations can adapt Scrum by implementing scaled agile frameworks like SAFe or LeSS to address complexities and dependencies.\n",
        "\n",
        "Furthermore, while Scrum may prioritize working software over comprehensive documentation, it does not mean that documentation is disregarded entirely. Organizations can establish guidelines and processes to ensure that essential documentation is maintained alongside the iterative development.\n",
        "\n",
        "Regarding role clarity, organizations can establish clear role definitions and ensure that team members understand their responsibilities and accountabilities. This can be achieved through effective communication and regular feedback.\n",
        "\n",
        "Lastly, while Scrum relies on experienced Scrum Masters, organizations can invest in training and development programs to enhance the skills and knowledge of Scrum Masters, ensuring effective facilitation of the Scrum process.\n",
        "\n",
        "In conclusion, while Scrum has its limitations, many of these can be addressed and mitigated through proper implementation, adaptation, and organizational support. It is important to carefully consider the specific project and organizational context to determine if Scrum is the right fit and to make necessary adjustments to maximize its benefits and overcome any potential drawbacks.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "uU9ujTsaMQLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from operator import itemgetter\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# 1. 공통 모델 설정 (Llama-3.2 또는 Qwen 사용 권장)\n",
        "llm_ep = HuggingFaceEndpoint(\n",
        "    repo_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=1024,\n",
        "    temperature=0.7\n",
        ")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "model_parser = model | StrOutputParser()\n",
        "\n",
        "# --- Example 1: Sequential Chaining (순차 체인) ---\n",
        "#\n",
        "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
        "prompt2 = ChatPromptTemplate.from_template(\"what country is the city {city} in? respond in {language}\")\n",
        "\n",
        "chain1 = prompt1 | model_parser\n",
        "chain2 = (\n",
        "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
        "    | prompt2\n",
        "    | model_parser\n",
        ")\n",
        "\n",
        "print(\"--- Example 1 Result ---\")\n",
        "print(chain2.invoke({\"person\": \"obama\", \"language\": \"english\"}))\n",
        "\n",
        "\n",
        "# --- Example 2: Parallel Input (병렬 입력 생성) ---\n",
        "#\n",
        "prompt1 = ChatPromptTemplate.from_template(\"generate a {attribute} color. Return the name of the color and nothing else:\")\n",
        "prompt2 = ChatPromptTemplate.from_template(\"what is a fruit of color: {color}. Return the name of the fruit and nothing else:\")\n",
        "prompt3 = ChatPromptTemplate.from_template(\"what is a country with a flag that has the color: {color}. Return the name of the country and nothing else:\")\n",
        "prompt4 = ChatPromptTemplate.from_template(\"What is the color of {fruit} and the flag of {country}?\")\n",
        "\n",
        "color_generator = ({\"attribute\": RunnablePassthrough()} | prompt1 | {\"color\": model_parser})\n",
        "color_to_fruit = prompt2 | model_parser\n",
        "color_to_country = prompt3 | model_parser\n",
        "\n",
        "question_generator = (\n",
        "    color_generator\n",
        "    | {\"fruit\": color_to_fruit, \"country\": color_to_country}\n",
        "    | prompt4\n",
        ")\n",
        "\n",
        "print(\"\\n--- Example 2 Result ---\")\n",
        "generated_prompt = question_generator.invoke(\"warm\")\n",
        "print(f\"Generated Question: {generated_prompt.messages[0].content}\")\n",
        "print(model_parser.invoke(generated_prompt))\n",
        "\n",
        "\n",
        "# --- Example 3: Branching and Merging (분기 및 병합 - Critique Loop) ---\n",
        "#\n",
        "planner = (\n",
        "    ChatPromptTemplate.from_template(\"Generate a concise argument about: {input}\")\n",
        "    | model_parser\n",
        "    | {\"base_response\": RunnablePassthrough()}\n",
        ")\n",
        "\n",
        "arguments_for = (\n",
        "    ChatPromptTemplate.from_template(\"List the pros or positive aspects of {base_response}\")\n",
        "    | model_parser\n",
        ")\n",
        "arguments_against = (\n",
        "    ChatPromptTemplate.from_template(\"List the cons or negative aspects of {base_response}\")\n",
        "    | model_parser\n",
        ")\n",
        "\n",
        "final_responder = (\n",
        "    ChatPromptTemplate.from_messages([\n",
        "        (\"ai\", \"{original_response}\"),\n",
        "        (\"human\", \"Pros:\\n{results_1}\\n\\nCons:\\n{results_2}\"),\n",
        "        (\"system\", \"Generate a final balanced response given the critique above.\"),\n",
        "    ])\n",
        "    | model_parser\n",
        ")\n",
        "\n",
        "full_critique_chain = (\n",
        "    planner\n",
        "    | {\n",
        "        \"results_1\": arguments_for,\n",
        "        \"results_2\": arguments_against,\n",
        "        \"original_response\": itemgetter(\"base_response\"),\n",
        "    }\n",
        "    | final_responder\n",
        ")\n",
        "\n",
        "print(\"\\n--- Example 3 Result (Scrum Analysis) ---\")\n",
        "print(full_critique_chain.invoke({\"input\": \"scrum\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3yVVwz_NMqB",
        "outputId": "04f9a09b-224a-42fc-fa9e-7123eba17491"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Example 1 Result ---\n",
            "The city where Barack Obama, the 44th President of the United States, was born is located in Hawaii, specifically in the state of Hawaii, in the United States.\n",
            "\n",
            "--- Example 2 Result ---\n",
            "Generated Question: What is the color of Papaya and the flag of Sri Lanka?\n",
            "I cannot verify the color of Papaya but the flag of Sri Lanka is predominantly orange, red and green.\n",
            "\n",
            "--- Example 3 Result (Scrum Analysis) ---\n",
            "**Scrum: A Balanced Agile Framework**\n",
            "\n",
            "Scrum is a widely adopted Agile framework that has proven its effectiveness in software development, project management, and other complex endeavors. While it offers numerous benefits, it's essential to acknowledge its potential drawbacks and consider the nuances of its implementation.\n",
            "\n",
            "**Key Benefits:**\n",
            "\n",
            "1. **Increased Efficiency**: Scrum's iterative approach allows teams to deliver working software in short cycles, reducing overall project duration and improving response times.\n",
            "2. **Improved Collaboration**: Scrum fosters a culture of trust, respect, and open communication, leading to better decision-making and problem-solving.\n",
            "3. **Adaptability**: Scrum's flexibility allows teams to respond quickly to changing requirements, ensuring that projects stay on track and meet customer needs.\n",
            "4. **Reduced Risk**: Scrum's iterative approach and continuous improvement cycle reduce the risk of project failure by identifying and addressing issues early on.\n",
            "5. **Proven Track Record**: Scrum has been successfully implemented in numerous industries, including technology, finance, and healthcare, with notable examples including Microsoft, Dell, and IBM.\n",
            "\n",
            "**Potential Drawbacks:**\n",
            "\n",
            "1. **Steep Learning Curve**: Scrum can be complex, requiring a significant amount of training and investment in process and tooling.\n",
            "2. **Over-Reliance on Team Dynamics**: Scrum's emphasis on team collaboration and communication can create pressure on team members to be highly social and interacting with each other, which may not be suitable for all team compositions.\n",
            "3. **High Maintenance**: Scrum requires regular meetings, ceremonies, and retrospectives, which can be time-consuming and may require significant administrative effort.\n",
            "4. **Not Suitable for All Project Types**: While Scrum is effective for complex, iterative projects, it may not be the best fit for projects with well-defined requirements or those that do not require frequent changes.\n",
            "5. **Scrum Masters May Overstep**: In some cases, Scrum Masters may take on too much authority or become overly involved in the team's decision-making process, which can lead to an imbalance in team dynamics.\n",
            "6. **Data-Driven Decision Making**: Scrum's focus on iterative and adaptive development may lead to resistance to data-driven decision making, as teams may prioritize intuition over facts and figures.\n",
            "7. **Meeting Overload**: Excessive meetings can become a burden on teams, especially if they are poorly scheduled, poorly facilitated, or lack clear agendas and outcomes.\n",
            "\n",
            "**Mitigating the Drawbacks:**\n",
            "\n",
            "To maximize the benefits of Scrum while minimizing its drawbacks, consider the following strategies:\n",
            "\n",
            "1. **Provide thorough training**: Ensure that team members understand the Scrum framework, its principles, and its processes.\n",
            "2. **Develop a tailored approach**: Adapt Scrum to the specific needs and goals of your project and team.\n",
            "3. **Establish clear roles and responsibilities**: Define the roles and expectations of Scrum team members, including the Scrum Master, Product Owner, and Development Team.\n",
            "4. **Monitor and adjust**: Regularly assess the effectiveness of your Scrum implementation and make adjustments as needed.\n",
            "5. **Emphasize data-driven decision making**: Encourage teams to use data and metrics to inform their decisions and ensure that intuition is balanced with facts and figures.\n",
            "\n",
            "By acknowledging the potential drawbacks of Scrum and implementing strategies to mitigate them, teams can maximize the benefits of this Agile framework and achieve successful project outcomes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Querying a SQL DB"
      ],
      "metadata": {
        "id": "he01S3jBQW0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_community.utilities.sql_database import SQLDatabase\n",
        "\n",
        "\n",
        "template = \"\"\"Based on the table schema below, write a SQL query that would answer the user's question:\n",
        "{schema}\n",
        "\n",
        "Question: {question}\n",
        "SQL Query:\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# sqlite3 Chinook.db\n",
        "# .read Chinook_Sqlite.sql\n",
        "# download the sql file from the link below\n",
        "# https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql\n",
        "\n",
        "db = SQLDatabase.from_uri(\"sqlite:///./Chinook.db\")\n",
        "\n",
        "def get_schema(_):\n",
        "    return db.get_table_info()\n",
        "\n",
        "def run_query(query):\n",
        "    return db.run(query)\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "sql_response = (\n",
        "    RunnablePassthrough.assign(schema=get_schema)\n",
        "    | prompt\n",
        "    | model.bind(stop=[\"\\nSQLResult:\"])\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(sql_response.invoke({\"question\": \"How many employees are there?\"}))\n",
        "\"\"\"\n",
        "SELECT COUNT(*) FROM Employee\n",
        "\"\"\"\n",
        "\n",
        "template = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n",
        "{schema}\n",
        "\n",
        "Question: {question}\n",
        "SQL Query: {query}\n",
        "SQL Response: {response}\"\"\"\n",
        "prompt_response = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "full_chain = (\n",
        "    RunnablePassthrough.assign(query=sql_response)\n",
        "    | RunnablePassthrough.assign(\n",
        "        schema=get_schema,\n",
        "        response=lambda x: run_query(x[\"query\"]),\n",
        "    )\n",
        "    | prompt_response\n",
        "    | model\n",
        ")\n",
        "\n",
        "print(full_chain.invoke({\"question\": \"How many employees are there?\"}))\n",
        "\"\"\"\n",
        "content='There are 8 employees.'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9U8SMVDKNV-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_community.utilities.sql_database import SQLDatabase\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# 1. 데이터베이스 연결 (sqlite:///./Chinook.db 파일이 있어야 함)\n",
        "db = SQLDatabase.from_uri(\"sqlite:///./Chinook.db\")\n",
        "\n",
        "def get_schema(_):\n",
        "    return db.get_table_info()\n",
        "\n",
        "def run_query(query):\n",
        "    # 모델이 생성한 쿼리에 간혹 붙는 백틱(```sql) 등을 제거하는 전처리\n",
        "    clean_query = query.replace(\"```sql\", \"\").replace(\"```\", \"\").strip()\n",
        "    return db.run(clean_query)\n",
        "\n",
        "# 2. 모델 설정 (Qwen2.5 또는 Llama-3.2 추천 - 코딩 능력이 좋음)\n",
        "llm_ep = HuggingFaceEndpoint(\n",
        "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.1 # 쿼리 생성은 결정적이어야 하므로 낮은 온도 설정\n",
        ")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# 3. SQL 생성 체인 (첫 번째 단계)\n",
        "# [Image of Text-to-SQL architecture showing natural language query being converted to SQL via LLM and executed on a database]\n",
        "sql_template = \"\"\"Based on the table schema below, write ONLY the SQL query that answers the user's question.\n",
        "Do not write any explanation.\n",
        "\n",
        "{schema}\n",
        "\n",
        "Question: {question}\n",
        "SQL Query:\"\"\"\n",
        "sql_prompt = ChatPromptTemplate.from_template(sql_template)\n",
        "\n",
        "sql_response = (\n",
        "    RunnablePassthrough.assign(schema=get_schema)\n",
        "    | sql_prompt\n",
        "    | model.bind(stop=[\"\\nSQLResult:\", \" ;\", \";\"])\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 4. 자연어 응답 체인 (두 번째 단계)\n",
        "# [Image of SQL-to-Text process showing database results being formatted into a natural language response by an AI model]\n",
        "response_template = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n",
        "{schema}\n",
        "\n",
        "Question: {question}\n",
        "SQL Query: {query}\n",
        "SQL Response: {response}\n",
        "Natural Language Response:\"\"\"\n",
        "response_prompt = ChatPromptTemplate.from_template(response_template)\n",
        "\n",
        "full_chain = (\n",
        "    RunnablePassthrough.assign(query=sql_response)\n",
        "    | RunnablePassthrough.assign(\n",
        "        schema=get_schema,\n",
        "        response=lambda x: run_query(x[\"query\"]),\n",
        "    )\n",
        "    | response_prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 5. 실행 및 확인\n",
        "try:\n",
        "    print(\"--- 1. 생성된 SQL 쿼리 ---\")\n",
        "    query_text = sql_response.invoke({\"question\": \"How many employees are there?\"})\n",
        "    print(query_text)\n",
        "\n",
        "    print(\"\\n--- 2. 최종 자연어 답변 ---\")\n",
        "    final_answer = full_chain.invoke({\"question\": \"How many employees are there?\"})\n",
        "    print(final_answer)\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFWavYHaQdT5",
        "outputId": "2b32abb4-35dd-4866-8de2-9e808417b8fe"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. 생성된 SQL 쿼리 ---\n",
            "SELECT COUNT(*) FROM Employee\n",
            "\n",
            "--- 2. 최종 자연어 답변 ---\n",
            "There are 8 employees in the database.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Generator"
      ],
      "metadata": {
        "id": "4Pe20D7qRXz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        ")\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain_community.utilities import PythonREPL\n",
        "\n",
        "# Introduce PythonREPL\n",
        "python_repl = PythonREPL()\n",
        "print(python_repl.run(\"print(1+1)\"))\n",
        "\"\"\"\n",
        "Python REPL can execute arbitrary code. Use with caution.\n",
        "2\n",
        "\"\"\"\n",
        "\n",
        "template = \"\"\"Write some python code to solve the user's problem.\n",
        "\n",
        "Return only python code in Markdown format, e.g.:\n",
        "\n",
        "```python\n",
        "....\n",
        "```\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", template), (\"human\", \"{input}\")])\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "def _sanitize_output(text: str):\n",
        "    _, after = text.split(\"```python\")\n",
        "    return after.split(\"```\")[0]\n",
        "\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "print(chain.invoke({\"input\": \"Write the function to sort the list. Then call the function by pasing [1,4,2]\"}))\n",
        "\"\"\"\n",
        "```python\n",
        "def sort_list(lst):\n",
        "    return sorted(lst)\n",
        "\n",
        "my_list = [1, 4, 2]\n",
        "sorted_list = sort_list(my_list)\n",
        "print(sorted_list)\n",
        "```\n",
        "\"\"\"\n",
        "repl_chain = chain | _sanitize_output | PythonREPL().run\n",
        "\n",
        "print(repl_chain.invoke({\"input\": \"Write the function to sort the list. Then call the function by pasing [1,4,2]\"}))\n",
        "\"\"\"\n",
        "Python REPL can execute arbitrary code. Use with caution.\n",
        "[1, 2, 4]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Lqt3WbQmQi_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-experimental"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uo-nk1uSSYhn",
        "outputId": "680a29a9-330b-48e9-fb6b-13e9a43c4a94"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-experimental in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Collecting langchain-core<2.0.0,>=1.0.0 (from langchain-experimental)\n",
            "  Using cached langchain_core-1.2.8-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting langchain-community<1.0.0,>=0.4.0 (from langchain-experimental)\n",
            "  Using cached langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.0.46)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.6.6)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-experimental) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-experimental) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-experimental) (2.12.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-experimental) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-experimental) (0.14.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-experimental) (3.0.0)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.1.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community<1.0.0,>=0.4.0->langchain-experimental)\n",
            "  Using cached langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-experimental) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-experimental) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-experimental) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.1.0)\n",
            "Using cached langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "Using cached langchain_core-1.2.8-py3-none-any.whl (495 kB)\n",
            "Using cached langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Installing collected packages: langchain-core, langchain-text-splitters, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.83\n",
            "    Uninstalling langchain-core-0.3.83:\n",
            "      Successfully uninstalled langchain-core-0.3.83\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.11\n",
            "    Uninstalling langchain-text-splitters-0.3.11:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.11\n",
            "  Attempting uninstall: langchain-community\n",
            "    Found existing installation: langchain-community 0.3.31\n",
            "    Uninstalling langchain-community-0.3.31:\n",
            "      Successfully uninstalled langchain-community-0.3.31\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langgraph 0.3.34 requires langchain-core<0.4,>=0.1, but you have langchain-core 1.2.8 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.2.8 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.1.0 which is incompatible.\n",
            "langchain-huggingface 0.3.1 requires langchain-core<1.0.0,>=0.3.70, but you have langchain-core 1.2.8 which is incompatible.\n",
            "langgraph-prebuilt 0.1.8 requires langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43, but you have langchain-core 1.2.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-community-0.4.1 langchain-core-1.2.8 langchain-text-splitters-1.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "langchain_core"
                ]
              },
              "id": "ff6aa7f7188b40f394dfc030c5d19de6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "# [중요] 임포트 경로를 experimental로 변경합니다.\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "\n",
        "# 1. 모델 설정 (Qwen2.5-7B 추천)\n",
        "llm_ep = HuggingFaceEndpoint(\n",
        "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=1024,\n",
        "    temperature=0.1\n",
        ")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# 2. 프롬프트 설정\n",
        "template = \"\"\"Write some python code to solve the user's problem.\n",
        "\n",
        "Return only python code in Markdown format, e.g.:\n",
        "\n",
        "```python\n",
        "....\n",
        "```\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", template), (\"human\", \"{input}\")])\n",
        "\n",
        "# 3. 출력 정제 함수\n",
        "def _sanitize_output(text: str):\n",
        "    if \"```python\" in text:\n",
        "        _, after = text.split(\"```python\")\n",
        "        return after.split(\"```\")[0].strip()\n",
        "    return text.strip()\n",
        "\n",
        "# 4. 체인 구성\n",
        "code_gen_chain = prompt | model | StrOutputParser()\n",
        "\n",
        "# PythonREPL 객체 생성 (실행 시 주의 문구가 출력됩니다)\n",
        "repl = PythonREPL()\n",
        "repl_chain = code_gen_chain | _sanitize_output | repl.run\n",
        "\n",
        "# 5. 실행 테스트\n",
        "input_query = \"Write a function to sort a list. Then call the function by passing [1, 4, 2] and print the result.\"\n",
        "\n",
        "print(\"--- 1. 생성된 코드 확인 ---\")\n",
        "generated_code = code_gen_chain.invoke({\"input\": input_query})\n",
        "print(generated_code)\n",
        "\n",
        "print(\"\\n--- 2. 실행 결과 확인 ---\")\n",
        "#\n",
        "execution_result = repl_chain.invoke({\"input\": input_query})\n",
        "print(execution_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys10eaeaSJwS",
        "outputId": "06a7a887-d9cf-41da-a826-142bf1cd377f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. 생성된 코드 확인 ---\n",
            "```python\n",
            "def sort_list(lst):\n",
            "    return sorted(lst)\n",
            "\n",
            "result = sort_list([1, 4, 2])\n",
            "print(result)\n",
            "```\n",
            "\n",
            "--- 2. 실행 결과 확인 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 4]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t_JNDfmESKCm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}