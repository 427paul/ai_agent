{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMK5gmGJH7y4wDCGKY2JzDO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/427paul/ai_agent/blob/main/ai_agent_07_LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYz36gUyvNzx",
        "outputId": "15ab5dd9-ef34-4f1d-db9b-bcf38d9e5884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m458.9/458.9 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m553.3/553.3 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U \"langchain==0.3.*\" \"langchain-core==0.3.*\" \"langchain-community==0.3.*\" \"langgraph==0.3.*\" \"langchain-huggingface\" \"huggingface_hub\" \"sentence-transformers\" wikipedia -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtWG4t9MxJOi",
        "outputId": "61d9e112-a68f-41b7-bc70-8f59a51d7035"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def load_api_keys(filepath=\"api_key.txt\"):\n",
        "    with open(filepath, \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line and \"=\" in line:\n",
        "                key, value = line.split(\"=\", 1)\n",
        "                os.environ[key.strip()] = value.strip()\n",
        "\n",
        "path = '/content/drive/MyDrive/LangGraph/'\n",
        "\n",
        "# API í‚¤ ë¡œë“œ ë° í™˜ê²½ë³€ìˆ˜ ì„¤ì •\n",
        "load_api_keys(path + 'api_key.txt')"
      ],
      "metadata": {
        "id": "AO7QlRiwxL-c"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Chatbot"
      ],
      "metadata": {
        "id": "QTTc6VUSzEaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "class State(TypedDict):\n",
        "    # Messages have the type \"list\". The `add_messages` function\n",
        "    # in the annotation defines how this state key should be updated\n",
        "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\") # type: ignore\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
        "\n",
        "\n",
        "# The first argument is the unique node name\n",
        "# The second argument is the function or object that will be called whenever\n",
        "# the node is used.\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "graph = graph_builder.compile()\n",
        "# print(graph.get_graph().draw_mermaid())\n",
        "# https://mermaid.live/\n",
        "\n",
        "def stream_graph_updates(user_input: str):\n",
        "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
        "        for value in event.values():\n",
        "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
        "\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        stream_graph_updates(user_input)\n",
        "    except:\n",
        "        # fallback if input() is not available\n",
        "        user_input = \"What do you know about LangGraph?\"\n",
        "        print(\"User: \" + user_input)\n",
        "        stream_graph_updates(user_input)\n",
        "        break"
      ],
      "metadata": {
        "id": "nFg1nEFAxOXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "class State(TypedDict):\n",
        "    # Messages have the type \"list\". The `add_messages` function\n",
        "    # in the annotation defines how this state key should be updated\n",
        "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "llm_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"text-generation\")\n",
        "llm = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
        "\n",
        "\n",
        "# The first argument is the unique node name\n",
        "# The second argument is the function or object that will be called whenever\n",
        "# the node is used.\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "graph = graph_builder.compile()\n",
        "# print(graph.get_graph().draw_mermaid())\n",
        "# https://mermaid.live/\n",
        "\n",
        "def stream_graph_updates(user_input: str):\n",
        "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
        "        for value in event.values():\n",
        "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
        "\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        stream_graph_updates(user_input)\n",
        "    except:\n",
        "        # fallback if input() is not available\n",
        "        user_input = \"What do you know about LangGraph?\"\n",
        "        print(\"User: \" + user_input)\n",
        "        stream_graph_updates(user_input)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veUJMgZMzHm5",
        "outputId": "4d6402c2-e40c-4615-fc0a-4e14dfdcf7e3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: hello\n",
            "Assistant: Hello! ğŸ‘‹ How can I help you today?\n",
            "User: what do you know about LangGraph?\n",
            "Assistant: ## LangGraph â€“ A Quickâ€‘Start Reference\n",
            "\n",
            "| Topic | What you need to know |\n",
            "|-------|-----------------------|\n",
            "| **What is it?** | An openâ€‘source, Pythonâ€‘first framework that lets you *graphically* design, run, and debug LLMâ€‘powered workflows.  Think of it as a â€œworkflow engineâ€ built on top of the LangChain stack. |\n",
            "| **Why use it?** | Linear `Chain`s work great for simple pipelines, but many realâ€‘world LLM apps need branching, looping, external tool calls, and rich state sharing.  LangGraph gives you a declarative, typeâ€‘safe way to stitch all of that together. |\n",
            "| **Core idea** | **Nodes** â†’ discrete units (LLM call, function, tool, etc.)  **Edges** â†’ directed arrows that may be conditional on node output.  The graph is executed by a single `GraphExecutor` that walks the graph, passes a shared state object, and optionally streams results. |\n",
            "| **Installation** | ```bash\\npip install langgraph\\n``` |\n",
            "| **Where to learn** | â€¢ Official docs: https://langgraph.dev  <br>â€¢ GitHub repo: https://github.com/langchain-ai/langgraph  <br>â€¢ Tutorials & examples in the repoâ€™s `examples/` folder |\n",
            "\n",
            "---\n",
            "\n",
            "## Core Concepts\n",
            "\n",
            "| Concept | What it is | Typical useâ€‘case |\n",
            "|---------|------------|-----------------|\n",
            "| **Graph** | The entire workflow, defined as a directed graph of nodes. | A multiâ€‘step questionâ€‘answer pipeline that can fetch, summarize, and synthesize data. |\n",
            "| **Node** | A selfâ€‘contained unit that receives part of the state, produces output, and may push the next node ID. | A function that calls `OpenAI`, a database query, or a custom Python function. |\n",
            "| **Edge** | The directed link from one node to the next; can be *conditional* based on the nodeâ€™s output. | â€œIf the answer confidence > 0.8, go to nodeâ€¯5; otherwise, go to nodeâ€¯3.â€ |\n",
            "| **State** | A dictionary that is passed around.  It holds the conversation, memory, context, and any arbitrary data. | Storing the current user question, past LLM replies, or tool results. |\n",
            "| **Executor** | The runtime that traverses the graph, serialises calls, and streams partial outputs. | `GraphExecutor(graph).invoke(state)` |\n",
            "| **Tool** | A reusable component (e.g., an API call) that can be attached to a node or invoked directly. | Calling a weather API, querying a knowledge base, etc. |\n",
            "| **Memory** | Optional persistent storage for the state.  LangGraph integrates with LangChainâ€™s memory backâ€‘ends. | Maintaining longâ€‘term context across user sessions. |\n",
            "| **GraphQLâ€‘like DSL** | A succinct way to describe nodes and edges in a dictionary. | Defining a 5â€‘node graph in 10â€‘lines of code. |\n",
            "| **Visualization** | Optional rendering of the graph structure (via `graphviz` or `dagre`). | Debugging or explaining the workflow to stakeholders. |\n",
            "\n",
            "---\n",
            "\n",
            "## Quick Example: A 3â€‘Step Questionâ€‘Answer Pipeline\n",
            "\n",
            "```python\n",
            "from langgraph.graph import StateGraph, CompiledGraph, GraphBuilder\n",
            "from langchain_openai import ChatOpenAI\n",
            "from langchain_core.messages import HumanMessage, AIMessage\n",
            "\n",
            "# 1ï¸âƒ£ Define the state type (helps with type checking)\n",
            "class QAState:\n",
            "    question: str\n",
            "    answer: str | None = None\n",
            "\n",
            "# 2ï¸âƒ£ Create a simple LLM node\n",
            "llm = ChatOpenAI(temperature=0)\n",
            "\n",
            "def ask_llm(state: QAState) -> QAState:\n",
            "    resp = llm.invoke([HumanMessage(content=state.question)])\n",
            "    state.answer = resp.content\n",
            "    return state\n",
            "\n",
            "# 3ï¸âƒ£ Build the graph\n",
            "builder = StateGraph(QAState)\n",
            "\n",
            "builder.add_node(\"ask_llm\", ask_llm)\n",
            "builder.set_entry_point(\"ask_llm\")\n",
            "builder.set_finish_point(\"ask_llm\")\n",
            "\n",
            "# 4ï¸âƒ£ Compile and run\n",
            "graph = builder.compile()\n",
            "result = graph.invoke(QAState(question=\"What is LangGraph?\"))\n",
            "print(result.answer)  # => \"LangGraph is ...\"\n",
            "```\n",
            "\n",
            "**What the example does**  \n",
            "* Defines a state with a `question` and optional `answer`.  \n",
            "* Wraps an LLM call in a node.  \n",
            "* Builds a graph with a single node (no branching).  \n",
            "* Runs the graph and prints the answer.\n",
            "\n",
            "---\n",
            "\n",
            "## Adding Branching and Tools\n",
            "\n",
            "```python\n",
            "from langgraph.graph import StateGraph\n",
            "from langgraph.graph.message import add_messages\n",
            "from langgraph.graph import CompiledGraph\n",
            "from langchain_community.tools import DuckDuckGoSearchRun\n",
            "\n",
            "# A node that runs a web search tool\n",
            "def web_search(state: QAState) -> QAState:\n",
            "    tool = DuckDuckGoSearchRun()\n",
            "    result = tool.run(state.question)\n",
            "    # Store the search result in state\n",
            "    state.answer = f\"Search results: {result}\"\n",
            "    return state\n",
            "\n",
            "builder.add_node(\"search\", web_search)\n",
            "builder.add_edge(\"ask_llm\", \"search\")  # always go to search after asking LLM\n",
            "```\n",
            "\n",
            "You can make the edge *conditional*:\n",
            "\n",
            "```python\n",
            "from typing import Any\n",
            "\n",
            "def should_search(state: QAState) -> str:\n",
            "    # If the LLM answer is empty or short, search the web\n",
            "    return \"search\" if not state.answer or len(state.answer) < 20 else \"finish\"\n",
            "\n",
            "builder.add_conditional_edges(\"ask_llm\", should_search, [\"search\", \"finish\"])\n",
            "```\n",
            "\n",
            "Now the graph will decide at runtime whether to fetch more data.\n",
            "\n",
            "---\n",
            "\n",
            "## Advanced Features\n",
            "\n",
            "| Feature | What it gives you | Quickâ€‘look |\n",
            "|---------|------------------|------------|\n",
            "| **State persistence** | Save state to a database or vector store so you can resume sessions. | `builder.add_state_persistence(...)` |\n",
            "| **Parallel execution** | Run multiple nodes concurrently (e.g., two search engines). | `builder.add_parallel(\"search_a\", \"search_b\")` |\n",
            "| **Streaming** | Stream partial LLM outputs to the user as they arrive. | `graph.stream(state, stream=True)` |\n",
            "| **Custom Tool Integration** | Wrap any function or external API call as a node. | `builder.add_node(\"my_tool\", my_func)` |\n",
            "| **Observability** | Builtâ€‘in hooks for logging, tracing, and metrics. | `builder.set_on_step(...)` |\n",
            "| **Testing** | Graphs are pure Python objects â€“ you can unitâ€‘test them easily. | `graph.test(state)` |\n",
            "\n",
            "---\n",
            "\n",
            "## Typical Useâ€‘Cases\n",
            "\n",
            "| Domain | Sample Workflow |\n",
            "|--------|-----------------|\n",
            "| **Customer Support** | â€œAsk user, fetch knowledgeâ€‘base, generate response, optionally call external API for ticket creation.â€ |\n",
            "| **Data Extraction** | â€œParse PDF â†’ summarise â†’ validate â†’ store.â€ |\n",
            "| **Conversational Agents** | Multiâ€‘turn dialogue with memory, external tool usage, and dynamic routing. |\n",
            "| **Decisionâ€‘Making** | â€œLLM proposes options â†’ evaluate with cost calculator â†’ choose best.â€ |\n",
            "| **Education** | â€œExplain a concept â†’ quiz user â†’ adapt based on answers.â€ |\n",
            "\n",
            "---\n",
            "\n",
            "## Community & Ecosystem\n",
            "\n",
            "- **GitHub** â€“ active PRs, issues, and discussion.\n",
            "- **Slack / Discord** â€“ LangChain community channels have LangGraph support.\n",
            "- **Docs** â€“ Code samples for every major integration (OpenAI, Anthropic, Cohere, Vertex AI, etc.).\n",
            "- **Contributions** â€“ If you build a new tool or graph pattern, you can contribute back easily.\n",
            "\n",
            "---\n",
            "\n",
            "## Quick Checklist to Get Started\n",
            "\n",
            "1. **Install**: `pip install langgraph`\n",
            "2. **Define your state** (preferably a dataclass for type safety)\n",
            "3. **Create nodes** (LLM calls, functions, tools)\n",
            "4. **Build the graph** (add nodes, set entry/finish points, add edges)\n",
            "5. **Compile** the graph (`builder.compile()`)\n",
            "6. **Run** it with `graph.invoke(state)` or `graph.stream(state)`\n",
            "7. **Iterate** â€“ add conditional edges, parallel nodes, memory, etc.\n",
            "\n",
            "---\n",
            "\n",
            "## Final Thoughts\n",
            "\n",
            "LangGraph is essentially *the LangChain way to build complex, branching LLM workflows*.  If youâ€™re already comfortable with LangChainâ€™s `Chain`, `Agent`, and `Tool` patterns, youâ€™ll feel at home with LangGraphâ€™s nodeâ€‘edge model.  The biggest advantage?  You get a single, unified framework that:\n",
            "\n",
            "* Keeps the **state** explicit and typeâ€‘safe.\n",
            "* Allows **dynamic routing** (including loops and parallelism) that would be cumbersome with plain chains.\n",
            "* Provides **streaming** and **observability** out of the box.\n",
            "* Is **openâ€‘source** and tightly coupled to the LangChain ecosystem.\n",
            "\n",
            "So, if youâ€™re building anything beyond a straightâ€‘line prompt chainâ€”think multiâ€‘step reasoning, toolâ€‘augmented answers, or userâ€‘driven dialogueâ€”LangGraph is the goâ€‘to library. Happy graphing!\n",
            "User: q\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot with Tool"
      ],
      "metadata": {
        "id": "QaU-9mUu-zND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool]\n",
        "tool.invoke(\"What's a 'node' in LangGraph?\")\n",
        "\n",
        "\"\"\"\n",
        "[\n",
        "  {\n",
        "\t\"title\": \"LangGraph Glossary - GitHub Pages\",\n",
        "\t\"url\": \"https://langchain-ai.github.io/langgraph/concepts/low_level/\",\n",
        "\t\"content\": \"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a \\\"config\\\", containing optional configurable parameters (such as a thread_id).\\nSimilar to NetworkX, you add these nodes to a graph using the add_node method:\\nfrom langchain_core.runnables import RunnableConfig [...] LangGraph GlossaryÂ¶\\nGraphsÂ¶\\nAt its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:\\n\\n\\nState: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.\\n\\n\\nNodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State. [...] By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code.\\nIn short: nodes do the work. edges tell what to do next.\",\n",
        "\t\"score\": 0.8995547\n",
        "  },\n",
        "  {\n",
        "\t\"title\": \"A Comprehensive Guide About Langgraph: Code Included - Ionio\",\n",
        "\t\"url\": \"https://www.ionio.ai/blog/a-comprehensive-guide-about-langgraph-code-included\",\n",
        "\t\"content\": \"Now letâ€™s take a look at each component of langgraph in detail ğŸš€\\nNodes\\nA node can be any function or tool your agent uses in langgraph and these nodes are connected with other nodes using edges. Every workflow ends with a â€œENDâ€ node in langgraph which shows the end of workflow. You also need to define a starting node which will be the starting point of your workflow. Optionally, you can also define an ending node if you know the ending point of your workflow.\",\n",
        "\t\"score\": 0.8991304\n",
        "  }\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "class State(TypedDict):\n",
        "    # Messages have the type \"list\". The `add_messages` function\n",
        "    # in the annotation defines how this state key should be updated\n",
        "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\") # type: ignore\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "def stream_graph_updates(user_input: str):\n",
        "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
        "        for value in event.values():\n",
        "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
        "\n",
        "# The first argument is the unique node name\n",
        "# The second argument is the function or object that will be called whenever\n",
        "# the node is used.\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "\n",
        "class BasicToolNode:\n",
        "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
        "\n",
        "    def __init__(self, tools: list) -> None:\n",
        "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
        "\n",
        "    def __call__(self, inputs: dict):\n",
        "        if messages := inputs.get(\"messages\", []):\n",
        "            message = messages[-1]\n",
        "        else:\n",
        "            raise ValueError(\"No message found in input\")\n",
        "        outputs = []\n",
        "        for tool_call in message.tool_calls:\n",
        "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
        "                tool_call[\"args\"]\n",
        "            )\n",
        "            outputs.append(\n",
        "                ToolMessage(\n",
        "                    content=json.dumps(tool_result),\n",
        "                    name=tool_call[\"name\"],\n",
        "                    tool_call_id=tool_call[\"id\"],\n",
        "                )\n",
        "            )\n",
        "        return {\"messages\": outputs}\n",
        "\n",
        "\n",
        "tool_node = BasicToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "\n",
        "def route_tools(\n",
        "    state: State,\n",
        "):\n",
        "    \"\"\"\n",
        "    Use in the conditional_edge to route to the ToolNode if the last message\n",
        "    has tool calls. Otherwise, route to the end.\n",
        "    \"\"\"\n",
        "    if isinstance(state, list):\n",
        "        ai_message = state[-1] # type: ignore\n",
        "    elif messages := state.get(\"messages\", []):\n",
        "        ai_message = messages[-1]\n",
        "    else:\n",
        "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
        "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0: # type: ignore\n",
        "        return \"tools\"\n",
        "    return END\n",
        "\n",
        "\n",
        "# The `tools_condition` function returns \"tools\" if the chatbot asks to use a tool, and \"END\" if\n",
        "# it is fine directly responding. This conditional routing defines the main agent loop.\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    route_tools,\n",
        "    # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node\n",
        "    # It defaults to the identity function, but if you\n",
        "    # want to use a node named something else apart from \"tools\",\n",
        "    # You can update the value of the dictionary to something else\n",
        "    # e.g., \"tools\": \"my_tools\"\n",
        "    {\"tools\": \"tools\", END: END},\n",
        ")\n",
        "# Any time a tool is called, we return to the chatbot to decide the next step\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph = graph_builder.compile()\n",
        "# print(graph.get_graph().draw_mermaid())\n",
        "# https://mermaid.live/\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        stream_graph_updates(user_input)\n",
        "    except:\n",
        "        # fallback if input() is not available\n",
        "        user_input = \"What do you know about LangGraph?\"\n",
        "        print(\"User: \" + user_input)\n",
        "        stream_graph_updates(user_input)\n",
        "        break"
      ],
      "metadata": {
        "id": "mE8KbHYLzQIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "# os.environ['TAVILY_API_KEY'] = ''\n",
        "\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool]\n",
        "tool.invoke(\"What's a 'node' in LangGraph?\")\n",
        "\n",
        "\"\"\"\n",
        "[\n",
        "  {\n",
        "\t\"title\": \"LangGraph Glossary - GitHub Pages\",\n",
        "\t\"url\": \"https://langchain-ai.github.io/langgraph/concepts/low_level/\",\n",
        "\t\"content\": \"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a \\\"config\\\", containing optional configurable parameters (such as a thread_id).\\nSimilar to NetworkX, you add these nodes to a graph using the add_node method:\\nfrom langchain_core.runnables import RunnableConfig [...] LangGraph GlossaryÂ¶\\nGraphsÂ¶\\nAt its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:\\n\\n\\nState: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.\\n\\n\\nNodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State. [...] By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code.\\nIn short: nodes do the work. edges tell what to do next.\",\n",
        "\t\"score\": 0.8995547\n",
        "  },\n",
        "  {\n",
        "\t\"title\": \"A Comprehensive Guide About Langgraph: Code Included - Ionio\",\n",
        "\t\"url\": \"https://www.ionio.ai/blog/a-comprehensive-guide-about-langgraph-code-included\",\n",
        "\t\"content\": \"Now letâ€™s take a look at each component of langgraph in detail ğŸš€\\nNodes\\nA node can be any function or tool your agent uses in langgraph and these nodes are connected with other nodes using edges. Every workflow ends with a â€œENDâ€ node in langgraph which shows the end of workflow. You also need to define a starting node which will be the starting point of your workflow. Optionally, you can also define an ending node if you know the ending point of your workflow.\",\n",
        "\t\"score\": 0.8991304\n",
        "  }\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "class State(TypedDict):\n",
        "    # Messages have the type \"list\". The `add_messages` function\n",
        "    # in the annotation defines how this state key should be updated\n",
        "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "llm_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"text-generation\")\n",
        "llm = ChatHuggingFace(llm=llm_ep)\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "def stream_graph_updates(user_input: str):\n",
        "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
        "        for value in event.values():\n",
        "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
        "\n",
        "# The first argument is the unique node name\n",
        "# The second argument is the function or object that will be called whenever\n",
        "# the node is used.\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "\n",
        "class BasicToolNode:\n",
        "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
        "\n",
        "    def __init__(self, tools: list) -> None:\n",
        "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
        "\n",
        "    def __call__(self, inputs: dict):\n",
        "        if messages := inputs.get(\"messages\", []):\n",
        "            message = messages[-1]\n",
        "        else:\n",
        "            raise ValueError(\"No message found in input\")\n",
        "        outputs = []\n",
        "        for tool_call in message.tool_calls:\n",
        "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
        "                tool_call[\"args\"]\n",
        "            )\n",
        "            outputs.append(\n",
        "                ToolMessage(\n",
        "                    content=json.dumps(tool_result),\n",
        "                    name=tool_call[\"name\"],\n",
        "                    tool_call_id=tool_call[\"id\"],\n",
        "                )\n",
        "            )\n",
        "        return {\"messages\": outputs}\n",
        "\n",
        "\n",
        "tool_node = BasicToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "\n",
        "def route_tools(\n",
        "    state: State,\n",
        "):\n",
        "    \"\"\"\n",
        "    Use in the conditional_edge to route to the ToolNode if the last message\n",
        "    has tool calls. Otherwise, route to the end.\n",
        "    \"\"\"\n",
        "    if isinstance(state, list):\n",
        "        ai_message = state[-1] # type: ignore\n",
        "    elif messages := state.get(\"messages\", []):\n",
        "        ai_message = messages[-1]\n",
        "    else:\n",
        "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
        "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0: # type: ignore\n",
        "        return \"tools\"\n",
        "    return END\n",
        "\n",
        "\n",
        "# The `tools_condition` function returns \"tools\" if the chatbot asks to use a tool, and \"END\" if\n",
        "# it is fine directly responding. This conditional routing defines the main agent loop.\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    route_tools,\n",
        "    # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node\n",
        "    # It defaults to the identity function, but if you\n",
        "    # want to use a node named something else apart from \"tools\",\n",
        "    # You can update the value of the dictionary to something else\n",
        "    # e.g., \"tools\": \"my_tools\"\n",
        "    {\"tools\": \"tools\", END: END},\n",
        ")\n",
        "# Any time a tool is called, we return to the chatbot to decide the next step\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph = graph_builder.compile()\n",
        "# print(graph.get_graph().draw_mermaid())\n",
        "# https://mermaid.live/\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        stream_graph_updates(user_input)\n",
        "    except:\n",
        "        # fallback if input() is not available\n",
        "        user_input = \"What do you know about LangGraph?\"\n",
        "        print(\"User: \" + user_input)\n",
        "        stream_graph_updates(user_input)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpDRJhRw-4HC",
        "outputId": "c36de0e2-ff47-4b1a-9ffe-1551621b4fa3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: What do you know about LangGraph\n",
            "Assistant: \n",
            "Assistant: [{\"title\": \"A Developer's Guide to LangGraph for LLM Applications | MetaCTO\", \"url\": \"https://www.metacto.com/blogs/a-developer-s-guide-to-langgraph-building-stateful-controllable-llm-applications\", \"content\": \"We manage the entire process, allowing you to focus on your core product while we build the advanced AI capabilities that will set it apart.\\n\\n## Conclusion\\n\\nLangGraph represents a significant step forward in the development of intelligent applications. By providing a flexible, stateful, and controllable framework, it empowers developers to move beyond simple LLM integrations and build truly sophisticated agents capable of handling complex, real-world tasks. From its graph-based architecture that enables cyclical workflows to its deep support for human-in-the-loop collaboration, LangGraph is a powerful tool for anyone serious about production-grade AI. [...] AI\\n\\n# A Developer's Guide to LangGraph - Building Stateful, Controllable LLM Applications\\n\\nThis comprehensive guide explores LangGraph, a framework for building robust and stateful LLM applications with complex, controllable workflows. Let our experts at MetaCTO help you leverage LangGraph to create powerful, production-ready AI agents for your product.\\n\\n\\u2022 5 min read\\n\\nKeep Reading   Talk to an Expert\\n\\nBy Jamie Schiesel \\u2022 Fractional CTO, Head of Engineering [...] 1. LangGraph (Open Source): A free-to-use, MIT-licensed open-source library with SDKs in Python and JavaScript. It is a stateful orchestration framework that you can self-host and manage, giving you complete control over your environment.\\n2. LangGraph Platform: A proprietary, scalable infrastructure service for deploying, monitoring, and scaling LangGraph applications. It includes a host of managed services, an integrated developer studio, and multiple deployment options, allowing developers to focus on application logic rather than infrastructure management.\\n\\nTo better understand the distinction, let\\u2019s compare the features of the open-source library and the managed platform.\", \"score\": 0.99999905}, {\"title\": \"LangGraph: Modular LLM Agent Orchestration - Emergent Mind\", \"url\": \"https://www.emergentmind.com/topics/langgraph\", \"content\": \"2000 character limit reached\\n\\n# LangGraph: Modular LLM Agent Orchestration\\n\\nUpdated 11 September 2025\\n\\n LangGraph is a graph-based, agentic programming framework that orchestrates modular, stateful workflows among LLM agents.\\n It supports explicit control flow with conditional transitions, unified state propagation, and modular integration for both single-agent and multi-agent systems.\\n Applications include advanced RAG pipelines, security benchmarking, industrial process optimization, and multi-agent coordination for complex tasks. [...] ## 1. Graph-Based Workflow Modeling and State Management\\n\\nLangGraph structures agent workflows as explicit computational graphs in which each node encapsulates a discrete task (e.g., document retrieval, grading, query rewriting) and each edge defines a transition\\u2014often conditional\\u2014based on runtime state. The state is typically a strongly typed object (for instance, a Python TypedDict or custom state class) that persists information such as user input, intermediate results, document collections, tool outputs, and execution history as it progresses through the graph.\\n\\nThis modeling enables: [...] LangGraph is a graph-based, agentic programming framework for orchestrating modular, stateful workflows among large language model (LLM) agents. Designed to support robust, composable, and auditable execution in both single-agent and multi-agent systems, LangGraph emphasizes clarity of control flow, state propagation, and fine-grained control over task execution. It has been adopted for complex Retrieval-Augmented Generation (RAG) pipelines, multi-agent coordination, security benchmarking, planning, and industrial reasoning systems, reflecting its flexibility and impact across diverse domains.\\n\\n## 1. Graph-Based Workflow Modeling and State Management\", \"score\": 0.9999975}]\n",
            "Assistant: **LangGraph** is a frameworkâ€”originally built by Meta and now openâ€‘sourceâ€”that lets you write *stateful, graphâ€‘based* AI agents (or chains of agents) instead of the more common linear â€œprompt â†’ LLM â†’ outputâ€ pattern.  Think of it as a small, lightweight workflow engine that is built specifically for largeâ€‘languageâ€‘model (LLM) applications.\n",
            "\n",
            "---\n",
            "\n",
            "## 1. Core Idea\n",
            "\n",
            "| Concept | What it means | Why it matters |\n",
            "|---------|---------------|----------------|\n",
            "| **Graphâ€‘based workflow** | You model your application as a directed graph of *nodes* (tasks) and *edges* (transitions). | Clear, auditable control flow; supports loops, branching, and conditional logic. |\n",
            "| **Stateful execution** | A typed â€œstateâ€ object travels through the graph, carrying user input, intermediate results, tool calls, and history. | Allows the agent to remember context over multiple turns without having to embed it all in a single prompt. |\n",
            "| **Modularity** | Each node is an independent, reusable piece of logic (e.g., `Retrieve`, `Summarize`, `AskClarification`). | Encourages reuse and easier testing. |\n",
            "| **LLMâ€‘agnostic** | Works with any LLM provider that exposes a `call` API (OpenAI, Anthropic, Llamaâ€‘Index, etc.). | You can swap models without touching the graph. |\n",
            "\n",
            "---\n",
            "\n",
            "## 2. Openâ€‘Source Library (MITâ€‘licensed)\n",
            "\n",
            "- **Language bindings**: Python (official), JavaScript/TypeScript (communityâ€‘maintained).\n",
            "- **Typical workflow**:  \n",
            "  1. Define a `State` class (often a `TypedDict`).  \n",
            "  2. Create `Node` functions that take the state and return an updated state (or an `Edge` hint).  \n",
            "  3. Assemble nodes in a `Graph` (via a DSL or a builder).  \n",
            "  4. Run the graph with `graph.run(start_state)`.\n",
            "\n",
            "```python\n",
            "from langgraph import StateGraph\n",
            "from typing import TypedDict\n",
            "\n",
            "class MyState(TypedDict):\n",
            "    question: str\n",
            "    answer: str | None\n",
            "    step: str\n",
            "\n",
            "def ask_llm(state: MyState):\n",
            "    # call an LLM with state[\"question\"]\n",
            "    state[\"answer\"] = \"...\"\n",
            "    state[\"step\"] = \"done\"\n",
            "    return state\n",
            "\n",
            "graph = StateGraph(MyState)\n",
            "graph.add_node(\"ask\", ask_llm)\n",
            "graph.set_entry_point(\"ask\")\n",
            "graph.set_finish_node(\"done\")\n",
            "app = graph.compile()\n",
            "result = app.invoke({\"question\": \"What is LangGraph?\"})\n",
            "print(result[\"answer\"])\n",
            "```\n",
            "\n",
            "- **Tools / integrations**: can embed tool calls (Python function execution, vectorâ€‘store retrieval, HTTP requests, etc.) via the `ToolNode` pattern.  \n",
            "- **Debugging & introspection**: builtâ€‘in visualizer in Jupyter or VS Code to see node execution order and state diffs.  \n",
            "\n",
            "---\n",
            "\n",
            "## 3. LangGraph Platform (Managed Service)\n",
            "\n",
            "- **Deployment & scaling**: containerized, Kubernetesâ€‘friendly, or cloudâ€‘hosted (AWS, GCP, Azure, or private).  \n",
            "- **Observability**: dashboards for latency, usage, failure rates, and state traces.  \n",
            "- **Versioning & CI/CD**: store graph definitions as code artifacts, automatically roll out new versions.  \n",
            "- **Enterprise features**: fineâ€‘grained permissions, audit logs, SSO, and SLOâ€‘based autoscaling.\n",
            "\n",
            "> *Note:* the platform is proprietary, but the underlying openâ€‘source library remains free. Many companies use the platform when they need productionâ€‘grade monitoring, so you donâ€™t have to build it yourself.\n",
            "\n",
            "---\n",
            "\n",
            "## 4. Typical Useâ€‘Cases\n",
            "\n",
            "| Domain | Example Graph |\n",
            "|--------|---------------|\n",
            "| **RAG pipelines** | `Retrieval` â†’ `Rephrase` â†’ `LLM` â†’ `Postâ€‘process` |\n",
            "| **Multiâ€‘agent coordination** | `Planner` â†’ `Researcher` â†’ `Writer` â†’ `Verifier` |\n",
            "| **Security & testing** | `Pentester` â†’ `Exploit` â†’ `Report` |\n",
            "| **Business workflows** | `CustomerService` â†’ `TicketRouting` â†’ `Resolution` |\n",
            "| **Education** | `QuizGenerator` â†’ `QuestionAnswering` â†’ `FeedbackLoop` |\n",
            "\n",
            "---\n",
            "\n",
            "## 5. Why Use LangGraph?\n",
            "\n",
            "| Pain Point | LangGraph Solution |\n",
            "|------------|---------------------|\n",
            "| Linear promptâ€‘loop limits context | State flows through nodes; you can store large histories or structured data. |\n",
            "| Hard to reason about agent behavior | Graph visualization + typeâ€‘checked state keeps the flow explicit. |\n",
            "| Repeating patterns across projects | Reusable node templates (retriever, summarizer, etc.). |\n",
            "| Scaling out multiâ€‘agent systems | Nodes can be distributed; the platform manages load balancing. |\n",
            "\n",
            "---\n",
            "\n",
            "## 6. Getting Started\n",
            "\n",
            "1. **Install**  \n",
            "   ```bash\n",
            "   pip install langgraph\n",
            "   # or npm i @langgraph/core\n",
            "   ```\n",
            "2. **Pick a state model**  \n",
            "   ```python\n",
            "   from typing import TypedDict\n",
            "   class State(TypedDict):\n",
            "       input: str\n",
            "       output: str | None\n",
            "   ```\n",
            "3. **Define nodes**  \n",
            "   ```python\n",
            "   def summarize(state: State):\n",
            "       # LLM call\n",
            "       state[\"output\"] = \"summary\"\n",
            "       return state\n",
            "   ```\n",
            "4. **Build graph**  \n",
            "   ```python\n",
            "   from langgraph import StateGraph\n",
            "   graph = StateGraph(State)\n",
            "   graph.add_node(\"summarize\", summarize)\n",
            "   graph.set_entry_point(\"summarize\")\n",
            "   graph.set_finish_node(\"done\")\n",
            "   app = graph.compile()\n",
            "   ```\n",
            "5. **Run**  \n",
            "   ```python\n",
            "   result = app.invoke({\"input\": \"Long text...\"} )\n",
            "   print(result[\"output\"])\n",
            "   ```\n",
            "\n",
            "For a deeper dive, the official documentation (https://docs.langgraph.org/) is very thorough, and there are plenty of example notebooks on GitHub.\n",
            "\n",
            "---\n",
            "\n",
            "## 7. Community & Ecosystem\n",
            "\n",
            "- **GitHub repo**: https://github.com/langgraph/langgraph â€“ active issue tracker, PR reviews.  \n",
            "- **Tutorials**: Meta CTO blog, Emergent Mind blog, and communityâ€‘built YouTube series.  \n",
            "- **Integrations**: Works seamlessly with LangChain, LlamaIndex, Haystack, and the LangSmith traceâ€‘visualizer.\n",
            "\n",
            "---\n",
            "\n",
            "## 8. Quick Comparison With Other Frameworks\n",
            "\n",
            "| Feature | LangGraph | LangChain | LlamaIndex |\n",
            "|---------|-----------|-----------|------------|\n",
            "| **Explicit graph** | âœ… | âŒ (sequential chain) | âŒ |\n",
            "| **Stateful node** | âœ… (typed state) | âœ… (context object) | âœ… |\n",
            "| **Multiâ€‘agent orchestration** | âœ… | Partial (via callbacks) | âŒ |\n",
            "| **Platform/managed service** | âœ” (LangGraph Platform) | âŒ | âŒ |\n",
            "| **Typeâ€‘checked state** | âœ… | âŒ | âŒ |\n",
            "\n",
            "---\n",
            "\n",
            "### TL;DR\n",
            "\n",
            "- **LangGraph** is a lightweight, graphâ€‘based framework that turns LLMâ€‘powered tasks into *stateful workflows*.\n",
            "- You can write it in Python or JavaScript, run it locally, or deploy it via the managed LangGraph Platform for production.\n",
            "- It shines when you need explicit control flow, reusable modular nodes, and the ability to scale multiâ€‘agent systems.\n",
            "\n",
            "Feel free to dive deeper into any of the sectionsâ€”whether you want code samples, architectural patterns, or deployment tips!\n",
            "User: q\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot with Memory"
      ],
      "metadata": {
        "id": "MBQsa4WWJVnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "memory = MemorySaver()\n",
        "\n",
        "from typing import Annotated\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool]\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\") # type: ignore\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "# Add a tool node to the graph\n",
        "# This node will be called when the tool is invoked\n",
        "tool_node = ToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "\n",
        "# Any time a tool is called, we return to the chatbot to decide the next step\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "# Compile with memory\n",
        "graph = graph_builder.compile(checkpointer=memory)\n",
        "# print(graph.get_graph().draw_mermaid())\n",
        "# https://mermaid.live/\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "user_input = \"Hi there! My name is Will.\"\n",
        "\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "    config, # type: ignore\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "user_input = \"Remember my name?\"\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "    config, # type: ignore\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "# events = graph.stream(\n",
        "#     {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "#     {\"configurable\": {\"thread_id\": \"2\"}},\n",
        "#     stream_mode=\"values\",\n",
        "# )\n",
        "# for event in events:\n",
        "#     event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "# config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "\n",
        "snapshot = graph.get_state(config) # type: ignore\n",
        "print(snapshot)\n",
        "# refer to chatbot_with_memory.json\n",
        "# print(snapshot)"
      ],
      "metadata": {
        "id": "9NbgegeGJ69_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "memory = MemorySaver()\n",
        "\n",
        "from typing import Annotated\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from typing_extensions import TypedDict\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# os.environ['TAVILY_API_KEY'] = ''\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool]\n",
        "\n",
        "llm_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"text-generation\")\n",
        "llm = ChatHuggingFace(llm=llm_ep)\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "# Add a tool node to the graph\n",
        "# This node will be called when the tool is invoked\n",
        "tool_node = ToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "\n",
        "# Any time a tool is called, we return to the chatbot to decide the next step\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "# Compile with memory\n",
        "graph = graph_builder.compile(checkpointer=memory)\n",
        "# print(graph.get_graph().draw_mermaid())\n",
        "# https://mermaid.live/\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "user_input = \"Hi there! My name is Will.\"\n",
        "\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "    config, # type: ignore\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "user_input = \"Remember my name?\"\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "    config, # type: ignore\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "# events = graph.stream(\n",
        "#     {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "#     {\"configurable\": {\"thread_id\": \"2\"}},\n",
        "#     stream_mode=\"values\",\n",
        "# )\n",
        "# for event in events:\n",
        "#     event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "# config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "\n",
        "snapshot = graph.get_state(config) # type: ignore\n",
        "print(snapshot)\n",
        "# refer to chatbot_with_memory.json\n",
        "# print(snapshot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p38VDYRWJX0j",
        "outputId": "44f2551b-5d4d-4574-8281-2368065899ae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hi there! My name is Will.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hi Will! How can I help you today?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Remember my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I donâ€™t have that information. Could you tell me your name?\n",
            "StateSnapshot(values={'messages': [HumanMessage(content='Remember my name?', additional_kwargs={}, response_metadata={}, id='76ffa6cb-335a-4429-af59-1fa26ad5b151'), AIMessage(content='I donâ€™t have that information. Could you tell me your name?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 101, 'prompt_tokens': 158, 'total_tokens': 259}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_7d448090ba', 'finish_reason': 'stop', 'logprobs': None}, id='run--019c5643-f8a6-73b2-9b45-ff95ff1c8de3-0', usage_metadata={'input_tokens': 158, 'output_tokens': 101, 'total_tokens': 259})]}, next=(), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f108bbd-2c0b-6dd4-8001-8b534fc308b6'}}, metadata={'source': 'loop', 'writes': {'chatbot': {'messages': [AIMessage(content='I donâ€™t have that information. Could you tell me your name?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 101, 'prompt_tokens': 158, 'total_tokens': 259}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_7d448090ba', 'finish_reason': 'stop', 'logprobs': None}, id='run--019c5643-f8a6-73b2-9b45-ff95ff1c8de3-0', usage_metadata={'input_tokens': 158, 'output_tokens': 101, 'total_tokens': 259})]}}, 'step': 1, 'parents': {}}, created_at='2026-02-13T09:10:21.294907+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f108bbd-2a1e-6b91-8000-68bf12b1cd2c'}}, tasks=())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot with Memory(SQLite)"
      ],
      "metadata": {
        "id": "X9S0e8c_EsGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "\n",
        "# find the current directory\n",
        "import os\n",
        "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "conn = sqlite3.connect(f\"{current_dir}/checkpoints.sqlite\", check_same_thread=False)\n",
        "memory = SqliteSaver(conn)\n",
        "\n",
        "from typing import Annotated\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool]\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\") # type: ignore\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "# Add a tool node to the graph\n",
        "# This node will be called when the tool is invoked\n",
        "tool_node = ToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "\n",
        "# Any time a tool is called, we return to the chatbot to decide the next step\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "# Compile with memory\n",
        "graph = graph_builder.compile(checkpointer=memory)\n",
        "# print(graph.get_graph().draw_mermaid())\n",
        "# https://mermaid.live/\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "user_input = \"Hi there! My name is Joon.\"\n",
        "\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "    config, # type: ignore\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "user_input = \"Remember my name?\"\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "    config, # type: ignore\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "# # events = graph.stream(\n",
        "# #     {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "# #     {\"configurable\": {\"thread_id\": \"2\"}},\n",
        "# #     stream_mode=\"values\",\n",
        "# # )\n",
        "# # for event in events:\n",
        "# #     event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "# # config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "\n",
        "# snapshot = graph.get_state(config) # type: ignore\n",
        "# print(snapshot)\n",
        "# # refer to chatbot_with_memory.json\n",
        "# # print(snapshot)\n",
        "\n",
        "print(list(graph.get_state_history(config)))"
      ],
      "metadata": {
        "id": "iTBF0Wa-JEZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Human-in-the-Loop"
      ],
      "metadata": {
        "id": "nFw70GVoLD79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.tools import tool\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "from langgraph.types import Command, interrupt\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "\n",
        "@tool\n",
        "def human_assistance(query: str) -> str:\n",
        "    \"\"\"Request assistance from a human.\"\"\"\n",
        "    human_response = interrupt({\"query\": query})\n",
        "    return human_response[\"data\"]\n",
        "\n",
        "\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool, human_assistance]\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\") # type: ignore\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    message = llm_with_tools.invoke(state[\"messages\"])\n",
        "    # Because we will be interrupting during tool execution,\n",
        "    # we disable parallel tool calling to avoid repeating any\n",
        "    # tool invocations when we resume.\n",
        "    assert len(message.tool_calls) <= 1 # type: ignore\n",
        "    return {\"messages\": [message]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "tool_node = ToolNode(tools=tools)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "memory = MemorySaver()\n",
        "\n",
        "graph = graph_builder.compile(checkpointer=memory)\n",
        "\n",
        "user_input = \"I need some expert guidance for building an AI agent. Could you request assistance for me?\"\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "    config, # type: ignore\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "snapshot = graph.get_state(config) # type: ignore\n",
        "print(snapshot.next)\n",
        "\n",
        "# human_response = (\n",
        "#     \"We, the experts are here to help! We'd recommend you check out LangGraph to build your agent.\"\n",
        "#     \" It's much more reliable and extensible than simple autonomous agents.\"\n",
        "# )\n",
        "\n",
        "human_response = input(\"Please provide your response to the user: \")\n",
        "\n",
        "human_command = Command(resume={\"data\": human_response})\n",
        "\n",
        "events = graph.stream(human_command, config, stream_mode=\"values\") # type: ignore\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "xxf1Sg1HE4U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.tools import tool\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "from langgraph.types import Command, interrupt\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "\n",
        "@tool\n",
        "def human_assistance(query: str) -> str:\n",
        "    \"\"\"Request assistance from a human.\"\"\"\n",
        "    human_response = interrupt({\"query\": query})\n",
        "    return human_response[\"data\"]\n",
        "\n",
        "# os.environ['TAVILY_API_KEY'] = ''\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool, human_assistance]\n",
        "\n",
        "\n",
        "hf_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"conversational\")\n",
        "llm = ChatHuggingFace(llm=hf_ep)\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    message = llm_with_tools.invoke(state[\"messages\"])\n",
        "    # Because we will be interrupting during tool execution,\n",
        "    # we disable parallel tool calling to avoid repeating any\n",
        "    # tool invocations when we resume.\n",
        "    assert len(message.tool_calls) <= 1 # type: ignore\n",
        "    return {\"messages\": [message]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "tool_node = ToolNode(tools=tools)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "memory = MemorySaver()\n",
        "\n",
        "graph = graph_builder.compile(checkpointer=memory)\n",
        "\n",
        "user_input = \"I need some expert guidance for building an AI agent. Could you request assistance for me?\"\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "    config, # type: ignore\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "snapshot = graph.get_state(config) # type: ignore\n",
        "print(snapshot.next)\n",
        "\n",
        "# human_response = (\n",
        "#     \"We, the experts are here to help! We'd recommend you check out LangGraph to build your agent.\"\n",
        "#     \" It's much more reliable and extensible than simple autonomous agents.\"\n",
        "# )\n",
        "\n",
        "human_response = input(\"Please provide your response to the user: \")\n",
        "\n",
        "human_command = Command(resume={\"data\": human_response})\n",
        "\n",
        "events = graph.stream(human_command, config, stream_mode=\"values\") # type: ignore\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaLqQU34LGv5",
        "outputId": "73d27165-97fb-4265-b8f9-dbbc0690faa9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "I need some expert guidance for building an AI agent. Could you request assistance for me?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  human_assistance (fc_08c47204-fa6d-4588-91d5-427c5fec355e)\n",
            " Call ID: fc_08c47204-fa6d-4588-91d5-427c5fec355e\n",
            "  Args:\n",
            "    query: I need some expert guidance for building an AI agent. Could you request assistance for me?\n",
            "('tools',)\n",
            "Please provide your response to the user: We, the experts are here to help! We'd recommend you check out LangGraph to build your agent.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  human_assistance (fc_08c47204-fa6d-4588-91d5-427c5fec355e)\n",
            " Call ID: fc_08c47204-fa6d-4588-91d5-427c5fec355e\n",
            "  Args:\n",
            "    query: I need some expert guidance for building an AI agent. Could you request assistance for me?\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: human_assistance\n",
            "\n",
            "We, the experts are here to help! We'd recommend you check out LangGraph to build your agent.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Absolutelyâ€”happy to walk you through the basics of building a modern AI agent! Below is a quick, highâ€‘level playbook that covers the key components, popular frameworks, and best practices. Feel free to pick and choose the parts that resonate with your project.\n",
            "\n",
            "---\n",
            "\n",
            "## 1. Define the Agentâ€™s Purpose\n",
            "\n",
            "| Question | What to Clarify |\n",
            "|----------|-----------------|\n",
            "| **Primary task** | Is it a chat assistant, a dataâ€‘retrieval bot, a workflow orchestrator, etc.? |\n",
            "| **Interaction style** | Conversational, commandâ€‘driven, or hybrid? |\n",
            "| **Data sources** | Public APIs, internal databases, PDFs, etc.? |\n",
            "| **Performance constraints** | Latency targets, concurrency limits, cost budget? |\n",
            "| **Safety & compliance** | Regulatory requirements, bias mitigation, auditability? |\n",
            "\n",
            "> **Tip:** Write a concise â€œUser Storyâ€ that captures the goal, who will use it, and why it matters. This will guide all technical choices.\n",
            "\n",
            "---\n",
            "\n",
            "## 2. Pick a Framework / Stack\n",
            "\n",
            "| Category | Options | Why you might choose it |\n",
            "|----------|---------|------------------------|\n",
            "| **Agent Orchestration** | **LangGraph** (by LangChain), **OpenAIâ€™s Agentic API**, **Microsoftâ€™s Agent SDK**, **AI2â€™s Agent Framework** | LangGraph excels at flexible, dataâ€‘centric workflows and is openâ€‘source; Agent SDK is great for Azureâ€‘centric solutions. |\n",
            "| **LLM Backbone** | OpenAI GPTâ€‘4 / GPTâ€‘4o, Anthropic Claude, Llamaâ€‘2, Gemini, Cohere | Pick based on cost, latency, and token limits. |\n",
            "| **Embeddings** | OpenAI embeddings, Cohere embeddings, Vertex AI, FAISS | Choose based on vector similarity needs. |\n",
            "| **Data Store** | Pinecone, Weaviate, Chroma, FAISS + S3, SQL + Postgres | Depends on scale, query speed, and persistence needs. |\n",
            "| **Prompt Management** | LangChain PromptTemplate, Anthropicâ€™s Prompt, LlamaIndex, PromptLayer | Helps version and audit prompts. |\n",
            "\n",
            "> **Starter Recommendation:** **LangGraph + OpenAI GPTâ€‘4o + Pinecone** â€“ gives you a quick, productionâ€‘ready pipeline with minimal boilerplate.\n",
            "\n",
            "---\n",
            "\n",
            "## 3. Build the Core Loop\n",
            "\n",
            "1. **Input Normalization**  \n",
            "   - Tokenize, deâ€‘duplicate, and sanityâ€‘check user input.\n",
            "\n",
            "2. **State Management**  \n",
            "   - Store the conversation context (messages, retrieved documents, tool outputs) in a lightweight store (e.g., an inâ€‘memory dict or a small DB).  \n",
            "   - Persist longâ€‘term memory in a vector DB for retrievalâ€‘augmented generation (RAG).\n",
            "\n",
            "3. **Decision Engine**  \n",
            "   - Use a *Planner* or *Policy Network* to decide the next step:  \n",
            "     - **LLMâ€‘Based Planner** â€“ prompt the LLM with a â€œnext actionâ€ instruction.  \n",
            "     - **Ruleâ€‘Based Planner** â€“ ifâ€‘else or state machine for deterministic flows.  \n",
            "     - **Reinforcement Learning** â€“ for complex, multiâ€‘step tasks.\n",
            "\n",
            "4. **Tool Execution Layer**  \n",
            "   - Wrap external APIs (e.g., Wikipedia, WolframAlpha, internal REST endpoints) as **Tools**.  \n",
            "   - Provide clear input schemas and response validation.  \n",
            "   - Log every call for auditability.\n",
            "\n",
            "5. **Output Generation**  \n",
            "   - Combine LLM output + tool results + context into a final response.  \n",
            "   - Postâ€‘process for style, length, or compliance checks.\n",
            "\n",
            "6. **Error Handling & Retry**  \n",
            "   - Detect failures (timeouts, rate limits) and implement graceful fallbacks or retries.\n",
            "\n",
            "---\n",
            "\n",
            "## 4. Implement Retrievalâ€‘Augmented Generation (RAG)\n",
            "\n",
            "| Component | Role | Tooling |\n",
            "|-----------|------|---------|\n",
            "| **Document Ingestion** | Convert PDFs, web pages, or internal docs into embeddings | LangChain DocumentLoaders, LlamaIndex, LangGraphâ€™s ingestors |\n",
            "| **Vector Store** | Store embeddings & metadata | Pinecone, Chroma, Weaviate, FAISS |\n",
            "| **Retrieval** | Query topâ€‘k docs based on user query | Similarity search, semantic search APIs |\n",
            "| **Prompt Fusion** | Inject retrieved context into the prompt | PromptTemplate with placeholders |\n",
            "\n",
            "> **Pro Tip:** Cache retrieval results for frequent queries to shave latency.\n",
            "\n",
            "---\n",
            "\n",
            "## 5. Safety & Governance\n",
            "\n",
            "1. **Prompt Templates & Constraints** â€“ avoid hallucinations, enforce disallowed content filters.\n",
            "2. **Content Moderation** â€“ integrate moderation APIs (OpenAI Moderation, Azure Content Safety).\n",
            "3. **Audit Trails** â€“ log every request, LLM prompt, tool call, and response.\n",
            "4. **User Feedback Loop** â€“ allow users to flag incorrect or unsafe outputs.\n",
            "5. **Compliance** â€“ GDPR/CCPA consent management, data residency, encryption at rest.\n",
            "\n",
            "---\n",
            "\n",
            "## 6. Deployment & Scaling\n",
            "\n",
            "| Layer | Options | Notes |\n",
            "|-------|---------|-------|\n",
            "| **Model Hosting** | OpenAI hosted, Anthropic hosted, onâ€‘prem (Llamaâ€‘2) | Consider latency and cost. |\n",
            "| **Orchestration** | LangGraphâ€™s local orchestrator, Airbyte, Temporal | Temporal gives durable workflow retries. |\n",
            "| **Containerization** | Docker + Kubernetes | Standard for microâ€‘services. |\n",
            "| **Observability** | OpenTelemetry, Prometheus + Grafana, LangSmith | LangSmith (LangChain) provides realâ€‘time tracing. |\n",
            "| **Autoâ€‘Scaling** | Serverless (Lambda, Cloud Run), Kubernetes HPA | For bursty workloads. |\n",
            "\n",
            "---\n",
            "\n",
            "## 7. Quickâ€‘Start Code Skeleton (LangGraph + GPTâ€‘4o)\n",
            "\n",
            "```python\n",
            "from langgraph.graph import StateGraph\n",
            "from langgraph.prebuilt import create_retrieval_chain\n",
            "from langchain_community.vectorstores import Pinecone\n",
            "from langchain_openai import ChatOpenAI\n",
            "from langchain_community.document_loaders import PDFLoader\n",
            "import pinecone\n",
            "\n",
            "# 1ï¸âƒ£ Init Pinecone\n",
            "pinecone.init(api_key=\"YOUR_PINECONE_KEY\", environment=\"us-west1-gcp\")\n",
            "index = pinecone.Index(\"my-agent-index\")\n",
            "\n",
            "# 2ï¸âƒ£ Load docs & embed\n",
            "loader = PDFLoader(\"manual.pdf\")\n",
            "docs = loader.load()\n",
            "from langchain_openai import OpenAIEmbeddings\n",
            "embs = OpenAIEmbeddings()\n",
            "pinecone_index = Pinecone.from_documents(docs, embs, index_name=\"my-agent-index\")\n",
            "\n",
            "# 3ï¸âƒ£ Create Retrieval + LLM\n",
            "retriever = pinecone_index.as_retriever(search_kwargs={\"k\": 3})\n",
            "llm = ChatOpenAI(model=\"gpt-4o\")\n",
            "\n",
            "# 4ï¸âƒ£ Build graph\n",
            "graph = StateGraph()\n",
            "graph.add_node(\"retrieval\", create_retrieval_chain(retriever))\n",
            "graph.add_node(\"answer\", lambda state: llm({\"messages\": state[\"messages\"]}))\n",
            "\n",
            "graph.add_edge(\"retrieval\", \"answer\")\n",
            "graph.add_edge(\"answer\", \"final\")\n",
            "\n",
            "graph.set_entry_point(\"retrieval\")\n",
            "app = graph.compile()\n",
            "\n",
            "# 5ï¸âƒ£ Run\n",
            "response = app.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Explain the main chapters\"}]})\n",
            "print(response[\"final\"])\n",
            "```\n",
            "\n",
            "> **What you get:**  \n",
            "> * Retrievalâ€‘augmented answer with context.  \n",
            "> * Easy to replace components (e.g., switch to Azure OpenAI).  \n",
            "> * Builtâ€‘in tracing via LangGraph UI.\n",
            "\n",
            "---\n",
            "\n",
            "## 8. Resources & Next Steps\n",
            "\n",
            "| Resource | Description |\n",
            "|----------|-------------|\n",
            "| **LangGraph Docs** | https://langgraph.org/docs |\n",
            "| **LangChain PromptTemplates** | https://docs.langchain.com/docs/prompts/ |\n",
            "| **OpenAI Safety & Moderation** | https://platform.openai.com/docs/guides/safety |\n",
            "| **Vector DB Tutorials** | Pinecone docs, Chroma, Weaviate |\n",
            "| **Agent SDK (Microsoft)** | https://github.com/microsoft/agent-sdk |\n",
            "| **Community** | LangChain Discord, LangGraph Slack, Reddit r/LanguageModels |\n",
            "\n",
            "**Action Plan:**\n",
            "\n",
            "1. **Prototype** a singleâ€‘turn retrieval agent with LangGraph + Pinecone.  \n",
            "2. **Iterate**: Add tool execution (e.g., a weather API) and a simple planner.  \n",
            "3. **Audit**: Enable tracing and moderation.  \n",
            "4. **Scale**: Containerize and deploy on your cloud provider, enable autoâ€‘scaling.  \n",
            "5. **Govern**: Implement logging and user feedback loop.\n",
            "\n",
            "---\n",
            "\n",
            "Feel free to drop any specific questionsâ€”whether itâ€™s about LLM choice, vector similarity, or scaling on Kubernetesâ€”and Iâ€™ll help you dive deeper!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot with Customizing State"
      ],
      "metadata": {
        "id": "RrDuk3KzMrdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.tools import tool\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "from langgraph.types import Command, interrupt\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    name: str\n",
        "    birthday: str\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "\n",
        "from langchain_core.messages import ToolMessage\n",
        "from langchain_core.tools import InjectedToolCallId, tool\n",
        "\n",
        "from langgraph.types import Command, interrupt\n",
        "\n",
        "\n",
        "@tool\n",
        "# Note that because we are generating a ToolMessage for a state update, we\n",
        "# generally require the ID of the corresponding tool call. We can use\n",
        "# LangChain's InjectedToolCallId to signal that this argument should not\n",
        "# be revealed to the model in the tool's schema.\n",
        "def human_assistance(\n",
        "    name: str, birthday: str, tool_call_id: Annotated[str, InjectedToolCallId]\n",
        ") -> str:\n",
        "    \"\"\"Request assistance from a human.\"\"\"\n",
        "    human_response = interrupt(\n",
        "        {\n",
        "            \"question\": \"Is this correct?\",\n",
        "            \"name\": name,\n",
        "            \"birthday\": birthday,\n",
        "        },\n",
        "    )\n",
        "    # If the information is correct, update the state as-is.\n",
        "    if human_response.get(\"correct\", \"\").lower().startswith(\"y\"):\n",
        "        verified_name = name\n",
        "        verified_birthday = birthday\n",
        "        response = \"Correct\"\n",
        "    # Otherwise, receive information from the human reviewer.\n",
        "    else:\n",
        "        verified_name = human_response.get(\"name\", name)\n",
        "        verified_birthday = human_response.get(\"birthday\", birthday)\n",
        "        response = f\"Made a correction: {human_response}\"\n",
        "\n",
        "    # This time we explicitly update the state with a ToolMessage inside\n",
        "    # the tool.\n",
        "    state_update = {\n",
        "        \"name\": verified_name,\n",
        "        \"birthday\": verified_birthday,\n",
        "        \"messages\": [ToolMessage(response, tool_call_id=tool_call_id)],\n",
        "    }\n",
        "    # We return a Command object in the tool to update our state.\n",
        "    return Command(update=state_update) # type: ignore\n",
        "\n",
        "\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool, human_assistance]\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\") # type: ignore\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    message = llm_with_tools.invoke(state[\"messages\"])\n",
        "    # Because we will be interrupting during tool execution,\n",
        "    # we disable parallel tool calling to avoid repeating any\n",
        "    # tool invocations when we resume.\n",
        "    assert len(message.tool_calls) <= 1 # type: ignore\n",
        "    return {\"messages\": [message]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "tool_node = ToolNode(tools=tools)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "memory = MemorySaver()\n",
        "\n",
        "graph = graph_builder.compile(checkpointer=memory)\n",
        "\n",
        "user_input = (\n",
        "    \"Can you look up when LangGraph was released? \"\n",
        "    \"When you have the answer, use the human_assistance tool for review.\"\n",
        ")\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "    config, # type: ignore\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "human_command = Command(\n",
        "    resume={\n",
        "        \"name\": \"LangGraph\",\n",
        "        \"birthday\": \"Jan 17, 2024\",\n",
        "    },\n",
        ")\n",
        "\n",
        "events = graph.stream(human_command, config, stream_mode=\"values\") # type: ignore\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "snapshot = graph.get_state(config) # type: ignore\n",
        "print({k: v for k, v in snapshot.values.items() if k in (\"name\", \"birthday\")})\n",
        "\n",
        "# Update the state with a new name\n",
        "snapshot = graph.update_state(config, {\"name\": \"LangGraph (library)\"}) # type: ignore\n",
        "snapshot = graph.get_state(config) # type: ignore\n",
        "print({k: v for k, v in snapshot.values.items() if k in (\"name\", \"birthday\")})"
      ],
      "metadata": {
        "id": "erHo74VvLWpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.tools import tool\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "from langgraph.types import Command, interrupt\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    name: str\n",
        "    birthday: str\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "\n",
        "from langchain_core.messages import ToolMessage\n",
        "from langchain_core.tools import InjectedToolCallId, tool\n",
        "\n",
        "from langgraph.types import Command, interrupt\n",
        "\n",
        "\n",
        "@tool\n",
        "# Note that because we are generating a ToolMessage for a state update, we\n",
        "# generally require the ID of the corresponding tool call. We can use\n",
        "# LangChain's InjectedToolCallId to signal that this argument should not\n",
        "# be revealed to the model in the tool's schema.\n",
        "def human_assistance(\n",
        "    name: str, birthday: str, tool_call_id: Annotated[str, InjectedToolCallId]\n",
        ") -> str:\n",
        "    \"\"\"Request assistance from a human.\"\"\"\n",
        "    human_response = interrupt(\n",
        "        {\n",
        "            \"question\": \"Is this correct?\",\n",
        "            \"name\": name,\n",
        "            \"birthday\": birthday,\n",
        "        },\n",
        "    )\n",
        "    # If the information is correct, update the state as-is.\n",
        "    if human_response.get(\"correct\", \"\").lower().startswith(\"y\"):\n",
        "        verified_name = name\n",
        "        verified_birthday = birthday\n",
        "        response = \"Correct\"\n",
        "    # Otherwise, receive information from the human reviewer.\n",
        "    else:\n",
        "        verified_name = human_response.get(\"name\", name)\n",
        "        verified_birthday = human_response.get(\"birthday\", birthday)\n",
        "        response = f\"Made a correction: {human_response}\"\n",
        "\n",
        "    # This time we explicitly update the state with a ToolMessage inside\n",
        "    # the tool.\n",
        "    state_update = {\n",
        "        \"name\": verified_name,\n",
        "        \"birthday\": verified_birthday,\n",
        "        \"messages\": [ToolMessage(response, tool_call_id=tool_call_id)],\n",
        "    }\n",
        "    # We return a Command object in the tool to update our state.\n",
        "    return Command(update=state_update) # type: ignore\n",
        "\n",
        "# os.environ['TAVILY_API_KEY'] = ''\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool, human_assistance]\n",
        "\n",
        "hf_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"conversational\")\n",
        "llm = ChatHuggingFace(llm=hf_ep)\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    message = llm_with_tools.invoke(state[\"messages\"])\n",
        "    # Because we will be interrupting during tool execution,\n",
        "    # we disable parallel tool calling to avoid repeating any\n",
        "    # tool invocations when we resume.\n",
        "    assert len(message.tool_calls) <= 1 # type: ignore\n",
        "    return {\"messages\": [message]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "tool_node = ToolNode(tools=tools)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "memory = MemorySaver()\n",
        "\n",
        "graph = graph_builder.compile(checkpointer=memory)\n",
        "\n",
        "user_input = (\n",
        "    \"Can you look up when LangGraph was released? \"\n",
        "    \"When you have the answer, use the human_assistance tool for review.\"\n",
        ")\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "    config, # type: ignore\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "human_command = Command(\n",
        "    resume={\n",
        "        \"name\": \"LangGraph\",\n",
        "        \"birthday\": \"Jan 17, 2024\",\n",
        "    },\n",
        ")\n",
        "\n",
        "events = graph.stream(human_command, config, stream_mode=\"values\") # type: ignore\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "snapshot = graph.get_state(config) # type: ignore\n",
        "print({k: v for k, v in snapshot.values.items() if k in (\"name\", \"birthday\")})\n",
        "\n",
        "# Update the state with a new name\n",
        "snapshot = graph.update_state(config, {\"name\": \"LangGraph (library)\"}) # type: ignore\n",
        "snapshot = graph.get_state(config) # type: ignore\n",
        "print({k: v for k, v in snapshot.values.items() if k in (\"name\", \"birthday\")})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smflLs4XM14p",
        "outputId": "7bf724ed-a80b-4d34-8c7c-064455410f88"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Can you look up when LangGraph was released? When you have the answer, use the human_assistance tool for review.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search_results_json (fc_7d09aac3-d1f6-40c3-b5ad-9a5768eb4e97)\n",
            " Call ID: fc_7d09aac3-d1f6-40c3-b5ad-9a5768eb4e97\n",
            "  Args:\n",
            "    query: LangGraph release date\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search_results_json\n",
            "\n",
            "[{\"title\": \"LangChain & LangGraph 1.0 alpha releases\", \"url\": \"https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/\", \"content\": \"Skip to content\\n\\nImage 4: LangChain Blog\\n\\n   Website\\n   Docs\\n   Harrison's Hot Takes\\n   Try LangSmith\\n\\nSign inSubscribe\\n\\nImage 5: LangChain & LangGraph 1.0 alpha releases\\n\\nLangChain & LangGraph 1.0 alpha releases\\n\\n3 min read Sep 2, 2025\\n\\nToday we are announcing alpha releases of v1.0 for `langgraph` and `langchain`, in both Python and JS. LangGraph is a low-level agent orchestration framework, giving developers durable execution and fine-grained control to run complex agentic systems in production. LangChain helps developers ship AI features fast with standardized model abstractions and prebuilt agent patterns, making it easy to build complex applications without vendor lock-in. We are working towards an official 1.0 release in late October - please give us any feedback here!\\n\\nLangGraph [...] LangGraph: `pip install langgraph==1.0.0a1`\\n\\nAgain - please give us feedback on these 1.0 releases (and docs) here. You can also find GitHub discussion items (LangChain, LangGraph). This is a big milestone - we are excited to work on this with the community over the next two months.\\n\\n### Join our newsletter\\n\\nUpdates from the LangChain team and community\\n\\nEnter your email Subscribe\\n\\nProcessing your application...\\n\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\n\\nSorry, something went wrong. Please try again.\\n\\nÂ© LangChain Blog 2026 [...] A number of use cases need completely custom patterns. For these - we recommend `langgraph` and building your own\\n   The rest have consolidated around a particular implementation of an â€œagentâ€\\n\\nThis \\\"agent\\\" abstraction is largely:\\n\\n1.   Give an LLM access to some tools\\n2.   Call it with some input\\n3.   If it calls a tool:\\n    1.   Execute that tool\\n    2.   Return to step 2, adding back in the tool call and tool result\\n\\n4.   If it doesn't call a tool: finish\\n\\nWe've had this abstraction in LangChain from the early days - in November of 2022. It's evolved over the years as things like tool calling have emerged to make this easier.\", \"score\": 0.99949265}, {\"title\": \"LangGraph and LangChain Hit 1.0 Alpha: The Future of Agentic AI ...\", \"url\": \"https://medium.com/@fahey_james/langgraph-and-langchain-hit-1-0-alpha-the-future-of-agentic-ai-infrastructure-8ea86c7c11c8\", \"content\": \"Sitemap\\n\\nOpen in app\\n\\nSign in\\n\\nSign in\\n\\n# LangGraph and LangChain Hit 1.0 Alpha: The Future of Agentic AI Infrastructure\\n\\nJames Fahey\\n\\n3 min read\\n\\nÂ·\\n\\nSep 12, 2025\\n\\n--\\n\\nOn September 2, 2025, LangChain announced a milestone for developers and enterprises building AI systems: 1.0 alpha releases of both LangGraph and LangChain (in Python and JavaScript). This marks the first time their orchestration framework and higher-level developer API are aligned under stable interfacesâ€Šâ€”â€Špaving the way for production-ready, large-scale agentic applications.\\n\\nâ¸»\\n\\nThe Origins of LangChain [...] â¸»\\n\\nThe CEO Driving the Vision\\n\\nHarrison Chase, co-founder and CEO, has consistently framed LangChainâ€™s mission as building the â€œorchestration layerâ€ for agentic AI. His leadership has pushed the company beyond hobbyist demos toward serious enterprise adoption.\\n\\nâ¸»\\n\\nWhy the 1.0 Alpha Release Matters\\n\\nThe 1.0 alpha announcement signals three things:\\n\\n1. Convergence: LangChainâ€™s agent API (create\\\\_agent) is now unified with LangGraphâ€™s runtime.\\n\\n2. Stability: The core abstractions are locking in, with GA planned for October 2025.\\n\\n3. Ecosystem leverage: With integrations across all major AI players, LangChain becomes a neutral foundation for enterprise-scale agent systems.\\n\\nIn short: itâ€™s the closest thing yet to a â€œstandard runtimeâ€ for agentic AI.\\n\\nâ¸»\\n\\nConclusion\", \"score\": 0.9993013}]\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  human_assistance (fc_9dbddac5-951e-40c3-9213-728996a6267f)\n",
            " Call ID: fc_9dbddac5-951e-40c3-9213-728996a6267f\n",
            "  Args:\n",
            "    birthday: 2025-09-02\n",
            "    name: LangGraph\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  human_assistance (fc_9dbddac5-951e-40c3-9213-728996a6267f)\n",
            " Call ID: fc_9dbddac5-951e-40c3-9213-728996a6267f\n",
            "  Args:\n",
            "    birthday: 2025-09-02\n",
            "    name: LangGraph\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: human_assistance\n",
            "\n",
            "Made a correction: {'name': 'LangGraph', 'birthday': 'Jan 17, 2024'}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search_results_json (fc_133ddde1-a080-48a5-a20f-9a74de7d3b44)\n",
            " Call ID: fc_133ddde1-a080-48a5-a20f-9a74de7d3b44\n",
            "  Args:\n",
            "    query: \"LangGraph\" initial release date\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search_results_json\n",
            "\n",
            "[{\"title\": \"LangGraph 1.0 is now generally available - LangChain - Changelog\", \"url\": \"https://changelog.langchain.com/announcements/langgraph-1-0-is-now-generally-available\", \"content\": \"DATE:October 22, 2025\\n\\nAUTHOR: The LangChain Team \\n\\nLangGraph 1.0 is the first stable major release in the durable agent framework space â€”a major milestone for production-ready AI systems. After more than a year of powering agents at companies like Uber, LinkedIn, and Klarna, LangGraph is officially v1.\\n\\nWhat's in LangGraph:\\n\\n   Durable state: Agent execution state persists automatically. If your server restarts mid-conversation or a long-running workflow gets interrupted, it picks up exactly where it left off without losing context.\\n\\n   Built-in persistence: Save and resume agent workflows at any point without writing custom database logic. Enables multi-day approval processes, background jobs, and workflows that span multiple sessions. [...] LangChain - Changelog | LangGraph 1.0 is now generally available\\n\\n[](\\n\\nProducts\\n\\nLangChainLangSmithLangGraph\\n\\nMethods\\n\\nRetrievalAgentsEvaluation\\n\\nResources\\n\\nBlogCase StudiesLangChain AcademyCommunityExpertsChangelog\\n\\nDocs\\n\\nPython\\n\\nLangChainLangSmithLangGraph\\n\\nJavaScript\\n\\nLangChainLangSmithLangGraph\\n\\nCompany\\n\\nAboutCareers\\n\\nPricing\\n\\nGet a demo\\n\\nSign up\\n\\nLangChain Changelog\\n\\nSign up for our newsletter to stay up to date\\n\\nForm Type \\n\\nBack to Announcements\\n\\nDATE:October 22, 2025\\n\\nAUTHOR:The LangChain Team\\n\\nLangGraph\\n\\nLangGraph 1.0 is now generally available\\n\\nDATE:October 22, 2025\\n\\nAUTHOR: The LangChain Team\", \"score\": 0.99942076}, {\"title\": \"LangChain & LangGraph 1.0 alpha releases\", \"url\": \"https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/\", \"content\": \"Skip to content\\n\\nImage 4: LangChain Blog\\n\\n   Website\\n   Docs\\n   Harrison's Hot Takes\\n   Try LangSmith\\n\\nSign inSubscribe\\n\\nImage 5: LangChain & LangGraph 1.0 alpha releases\\n\\nLangChain & LangGraph 1.0 alpha releases\\n\\n3 min read Sep 2, 2025\\n\\nToday we are announcing alpha releases of v1.0 for `langgraph` and `langchain`, in both Python and JS. LangGraph is a low-level agent orchestration framework, giving developers durable execution and fine-grained control to run complex agentic systems in production. LangChain helps developers ship AI features fast with standardized model abstractions and prebuilt agent patterns, making it easy to build complex applications without vendor lock-in. We are working towards an official 1.0 release in late October - please give us any feedback here!\\n\\nLangGraph\", \"score\": 0.9992022}]\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search_results_json (fc_f225922c-8789-4055-8816-b4e0a6aa47a5)\n",
            " Call ID: fc_f225922c-8789-4055-8816-b4e0a6aa47a5\n",
            "  Args:\n",
            "    query: LangGraph first release date 2023\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search_results_json\n",
            "\n",
            "[{\"title\": \"LangGraph 1.0 is now generally available - LangChain - Changelog\", \"url\": \"https://changelog.langchain.com/announcements/langgraph-1-0-is-now-generally-available\", \"content\": \"DATE:October 22, 2025\\n\\nAUTHOR: The LangChain Team \\n\\nLangGraph 1.0 is the first stable major release in the durable agent framework space â€”a major milestone for production-ready AI systems. After more than a year of powering agents at companies like Uber, LinkedIn, and Klarna, LangGraph is officially v1.\\n\\nWhat's in LangGraph:\\n\\n   Durable state: Agent execution state persists automatically. If your server restarts mid-conversation or a long-running workflow gets interrupted, it picks up exactly where it left off without losing context.\\n\\n   Built-in persistence: Save and resume agent workflows at any point without writing custom database logic. Enables multi-day approval processes, background jobs, and workflows that span multiple sessions. [...] LangChain - Changelog | LangGraph 1.0 is now generally available\\n\\n[](\\n\\nProducts\\n\\nLangChainLangSmithLangGraph\\n\\nMethods\\n\\nRetrievalAgentsEvaluation\\n\\nResources\\n\\nBlogCase StudiesLangChain AcademyCommunityExpertsChangelog\\n\\nDocs\\n\\nPython\\n\\nLangChainLangSmithLangGraph\\n\\nJavaScript\\n\\nLangChainLangSmithLangGraph\\n\\nCompany\\n\\nAboutCareers\\n\\nPricing\\n\\nGet a demo\\n\\nSign up\\n\\nLangChain Changelog\\n\\nSign up for our newsletter to stay up to date\\n\\nForm Type \\n\\nBack to Announcements\\n\\nDATE:October 22, 2025\\n\\nAUTHOR:The LangChain Team\\n\\nLangGraph\\n\\nLangGraph 1.0 is now generally available\\n\\nDATE:October 22, 2025\\n\\nAUTHOR: The LangChain Team\", \"score\": 0.99944717}, {\"title\": \"LangGraph, CrewAI, AutoGen, and Strands Agents\", \"url\": \"https://medium.com/@a.posoldova/comparing-4-agentic-frameworks-langgraph-crewai-autogen-and-strands-agents-b2d482691311\", \"content\": \"A Quick Overview\\n\\nLangGraph\\n\\nLangGraph was released in May 2023 by the LangChain team. It extends the LangChain ecosystem by allowing developers to structure agent workflows as graphs, with nodes representing functions or tools and edges handling conditional transitions. LangGraphâ€™s core principle is the use of stateful graphs to manage conversations and long-running agent tasks. Each agent step is represented as a node, and the flow between steps is handled using defined transitions. Itâ€™s built for developers who want durability, streaming, and full control over the flow of logic.\\n\\nPress enter or click to view image in full size\\n\\nImage 4\\n\\nCrewAI\", \"score\": 0.99895966}]\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  human_assistance (fc_bd023bbf-c3c9-42ca-b700-427cdb3cc5ad)\n",
            " Call ID: fc_bd023bbf-c3c9-42ca-b700-427cdb3cc5ad\n",
            "  Args:\n",
            "    birthday: 2023-05-01\n",
            "    name: LangGraph\n",
            "{'name': 'LangGraph', 'birthday': 'Jan 17, 2024'}\n",
            "{'name': 'LangGraph (library)', 'birthday': 'Jan 17, 2024'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot with Time Travel"
      ],
      "metadata": {
        "id": "CnlP5h5COKbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.messages import BaseMessage\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool]\n",
        "llm = ChatOpenAI(model=\"gpt-4o\") # type: ignore\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "tool_node = ToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "memory = MemorySaver()\n",
        "graph = graph_builder.compile(checkpointer=memory)\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "events = graph.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    \"What is the population of South Korea?\"\n",
        "                    \"Could you do some research on it for me?\"\n",
        "                ),\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "    config, # type: ignore\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "_ = input()\n",
        "\n",
        "events = graph.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    \"Ya that's helpful. How about the population of Japan?\"\n",
        "                ),\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "    config, # type: ignore\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "_ = input()\n",
        "\n",
        "events = graph.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    \"Ya that's helpful. How about the population of China?\"\n",
        "                ),\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "    config, # type: ignore\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "_ = input()\n",
        "\n",
        "# This is a simple example of how to replay a state\n",
        "to_replay = None\n",
        "for state in graph.get_state_history(config): # type: ignore\n",
        "    print(\"Num Messages: \", len(state.values[\"messages\"]), \"Next: \", state.next)\n",
        "\n",
        "    if len(state.values[\"messages\"]) > 0:\n",
        "        print(state.values[\"messages\"][-1].pretty_print())\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "    if len(state.values[\"messages\"]) == 6:\n",
        "        # ================================== Ai Message ==================================\n",
        "        # Tool Calls:\n",
        "        #   tavily_search_results_json (call_YQAiFP2XMgBTeIZB5KHjfLkG)\n",
        "        #  Call ID: call_YQAiFP2XMgBTeIZB5KHjfLkG\n",
        "        #   Args:\n",
        "        #     query: current population of Japan 2023\n",
        "        # None\n",
        "        to_replay = state\n",
        "\n",
        "_ = input()\n",
        "\n",
        "print(\"Replay this state *********\")\n",
        "print(to_replay.next)\n",
        "print(to_replay.config)\n",
        "\n",
        "# The `checkpoint_id` in the `to_replay.config` corresponds to a state we've persisted to our checkpointer.\n",
        "for event in graph.stream(None, to_replay.config, stream_mode=\"values\"):\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "VbOOJHBrOeB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.messages import BaseMessage\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# os.environ['TAVILY_API_KEY'] = ''\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool]\n",
        "hf_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"conversational\")\n",
        "llm = ChatHuggingFace(llm=hf_ep)\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "tool_node = ToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "memory = MemorySaver()\n",
        "graph = graph_builder.compile(checkpointer=memory)\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "events = graph.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    \"What is the population of South Korea?\"\n",
        "                    \"Could you do some research on it for me?\"\n",
        "                ),\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "    config, # type: ignore\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "_ = input()\n",
        "\n",
        "events = graph.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    \"Ya that's helpful. How about the population of Japan?\"\n",
        "                ),\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "    config, # type: ignore\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "_ = input()\n",
        "\n",
        "events = graph.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    \"Ya that's helpful. How about the population of China?\"\n",
        "                ),\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "    config, # type: ignore\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "_ = input()\n",
        "\n",
        "# This is a simple example of how to replay a state\n",
        "to_replay = None\n",
        "for state in graph.get_state_history(config): # type: ignore\n",
        "    print(\"Num Messages: \", len(state.values[\"messages\"]), \"Next: \", state.next)\n",
        "\n",
        "    if len(state.values[\"messages\"]) > 0:\n",
        "        print(state.values[\"messages\"][-1].pretty_print())\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "    if len(state.values[\"messages\"]) == 6:\n",
        "        # ================================== Ai Message ==================================\n",
        "        # Tool Calls:\n",
        "        #   tavily_search_results_json (call_YQAiFP2XMgBTeIZB5KHjfLkG)\n",
        "        #  Call ID: call_YQAiFP2XMgBTeIZB5KHjfLkG\n",
        "        #   Args:\n",
        "        #     query: current population of Japan 2023\n",
        "        # None\n",
        "        to_replay = state\n",
        "\n",
        "_ = input()\n",
        "\n",
        "print(\"Replay this state *********\")\n",
        "print(to_replay.next)\n",
        "print(to_replay.config)\n",
        "\n",
        "# The `checkpoint_id` in the `to_replay.config` corresponds to a state we've persisted to our checkpointer.\n",
        "for event in graph.stream(None, to_replay.config, stream_mode=\"values\"):\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ewA1PwUNLkM",
        "outputId": "4db59b00-5b99-4dbe-c0ff-62ab199e719b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is the population of South Korea?Could you do some research on it for me?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search_results_json (fc_391112ea-87e7-4a21-863c-6d75a5d1026f)\n",
            " Call ID: fc_391112ea-87e7-4a21-863c-6d75a5d1026f\n",
            "  Args:\n",
            "    query: population of South Korea 2023\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search_results_json\n",
            "\n",
            "[{\"title\": \"2023 Population and Housing Census (Register-based Census)\", \"url\": \"https://mods.go.kr/board.es?mid=a20108010000&bid=11747&act=view&list_no=432395\", \"content\": \"[Population] - As of November 1st, 2023, the population of South Korea was 51.77 million persons, which increased by 0.2% (82 thousand persons) from 2022. - The working age population aged 15~64 amounted to 36.55 million persons (70.6%). This figure had shown a steadily decreasing trend since 2018. - In 2023, the population of the Capital Area was 26.23 million persons, which occupied 50.7% of the total population. This figure showed a steadily increasing trend. [Household] - As of November 1st, 2023, the number of households totaled 22.73 million households, increasing by 1.5% (345 thousand households) from the previous year. - Among general households, 1-person and 2-person households accounted for 64.2% (14.18 million households). The average household size stood at 2.21 persons in [...] (35.5%), rising by 4.4% from 2022. - In 2023, the elderly Koreans aged 65 or more recorded 9.50 million persons, showing a continuously increasing trend. Households with an elderly member amounted to 6.81 million households, showing a continuously increasing trend. - In 2023, the multi-cultural households amounted to 416 thousand households. Naturalized Koreans and marriage immigrants amounted to 413 thousand persons. These two figures showed an ever-increasing trend. â€» For more information, refer to the attached file. [...] The average household size stood at 2.21 persons in 2023. - Among general households, relative households took up 62.1%. Household residing in apartments took up 53.1%. [Housing] - In 2023 the number of housing units totaled 19.55 million units, rising by 2.0% (391 thousand units) compared to 2022. - As for the year-on-year percent change in housing units by province, Daegu (5.0%) recorded the highest figure. Gyeongbuk (-0.1%) recorded the lowest figure. - Housing units constructed 20 years ago or more accounted for 53.7% of the total housing units. Housing units constructed 30 years ago or more accounted for 25.8% of the total housing units. [Major population groups] - In 2023, 1-person households marked 7.83 million households (35.5%), rising by 4.4% from 2022. - In 2023, the elderly\", \"score\": 0.9657998}, {\"title\": \"South Korea Population (1950-2025) - Macrotrends\", \"url\": \"https://www.macrotrends.net/global-metrics/countries/kor/south-korea/population\", \"content\": \"## South Korea Population (1950-2025)\\n\\nPopulation Economy Trade Health Education Development Labor Force Environment Crime Immigration Other\\n\\nPopulation Growth Rate Density Urban Rural Life Expectancy Birth Rate Death Rate Infant Mortality Rate Fertility Rate\\n\\n##### Total current population for South Korea in 2025 was 51,690,479, a 0.1% decline from 2024.\\n\\n Total population for South Korea in 2024 was 51,741,963, a 0.06% increase from 2023.\\n Total population for South Korea in 2023 was 51,712,619, a 0.08% increase from 2022.\\n Total population for South Korea in 2022 was 51,672,569, a 0.19% decline from 2021.\", \"score\": 0.9421299}]\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "**Population of Southâ€¯Korea (as of the most recent official estimate)**  \n",
            "- **2023:**â€¯â‰ˆâ€¯**51.7â€¯million** people  \n",
            "- **2024 (forecast):**â€¯â‰ˆâ€¯51.74â€¯million (slight increase)\n",
            "\n",
            "These figures come from the South Korean National Statistical Officeâ€™s 2023 Population and Housing Census and the subsequent annual estimates published by Statisticsâ€¯Korea.  \n",
            "\n",
            "| Year | Population | Source |\n",
            "|------|------------|--------|\n",
            "| 2023 | **51â€¯712â€¯619** | [Statisticsâ€¯Korea â€“ 2023 Census](https://mods.go.kr/board.es?mid=a20108010000&bid=11747&act=view&list_no=432395) |\n",
            "| 2024 (est.) | **51â€¯741â€¯963** | [Macrotrends â€“ South Korea Population](https://www.macrotrends.net/global-metrics/countries/kor/south-korea/population) |\n",
            "\n",
            "> *Note:* The 2023 census data were released on Novemberâ€¯1,â€¯2023. The 2024 figure is an annual estimate based on demographic trends (births, deaths, net migration) and is updated annually by Statisticsâ€¯Korea.  \n",
            "\n",
            "If you need more granular details (e.g., ageâ€‘group breakdowns, regional distribution, household composition) or the most recent quarterly updates, let me know and I can pull the latest figures for you.\n",
            "\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Ya that's helpful. How about the population of Japan?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search_results_json (fc_13283d5f-acdc-4165-ad45-d8c0d2430e42)\n",
            " Call ID: fc_13283d5f-acdc-4165-ad45-d8c0d2430e42\n",
            "  Args:\n",
            "    query: population of Japan 2023\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search_results_json\n",
            "\n",
            "[{\"title\": \"Current Population Estimates as of October 1, 2023\", \"url\": \"https://www.stat.go.jp/english/data/jinsui/2023np/index.html\", \"content\": \"### Population for Japan\\n\\n[Total Population]\\n\\n The total population was 124,352 thousand, a decrease of 595 thousand compared with the previous year. The rate of decrease was 0.48 percent. The total population decreased for the thirteenth year in a row.\\n The male population was 60,492  thousand, a decrease of 265  thousand (0.44 percent), while the female population was 63,859  thousand, a decrease of 330   thousand (0.51 percent).\\n The natural change of the male population was negative for the nineteenth year in a row, and that of the female population was negative for the fifteenth year in a row.\\n The migration change of the Japanese population was positive for the first time in three years, and that of the foreign population was positive for the two years in a row. [...] [Population by Age Group]\\n\\n The population under 15 years  old was 14,173 thousand (11.4  percent of the total population).\\n The population aged 15 to 64 was 73,952 thousand (59.5 percent of the total population).\\n The population aged 65 years old and over was 36,227 thousand (29.1 percent of the total population).\\n\\n### Population by Prefecture\\n\\n[Population]\\n\\n The top five prefectures in population were Tokyo-to, Kanagawa-ken, Osaka-fu, Aichi-ken and Saitama-ken. These prefectures account for 37.7 percent of the total population.\\n Tokyo-to had the largest population in 2023 (11.3 percent of the total population).\\n\\n[Rates of Population Change]\\n\\n The population increased only in Tokyo-to.\\n The population decreased in 46 prefectures. Akita-ken had the highest rate of decrease in population.\", \"score\": 0.9564761}, {\"title\": \"Demographics of Japan - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Demographics_of_Japan\", \"content\": \"According to a demographic study conducted by Japanese Ministry of Internal Affairs and Communications, the Japanese population, including foreign residents, declined from 128 million people in 2010 to 124.3 million people in 2023, with a decrease of almost 511,000 people in one year.\\n\\nView source data.\\n\\nView source data.\\n\\nView source data.\\n\\nView source data.\\n\\nView source data.\\n\\n## Population\\n\\n[edit]\\n\\n### Census\\n\\n[edit]\\n\\nSee also: Population and housing censuses by country\\n\\nJapan collects census information every five years, with censuses conducted by the Statistics Bureau \\\"Statistics Bureau (Japan)\\\") of the Ministry of Internal Affairs. The latest population census reflects the situation as of 2020.\\n\\n### Population density\\n\\n[edit] [...] Japan dropped from the 5th most populous country in the world to 6th in 1964, 7th in 1978, 8th in 1990, to 9th in 1998, to 10th in the early 21st century, 11th in 2020, and to 12th in 2023. Between 2010 to 2015, Japan's population shrank by almost a million, and Japan lost a half-million in 2022 alone. The number of Japanese citizens decreased by 801,000 to 122,423,038 in 2022 from a year earlier, which was the most severe decrease and the first time all 47 prefectures have suffered a decline since the launch of the poll in 1968. In early 2010, Japan's population reached 128,057,352. In the 2010s, the long-lasting effects of Japanese economic crisis during the Great Recession strongly slowed down immigration rates in Japan. [...] | 2020 | 126,146,099 | 840,832 | 1,372,648 | âˆ’531,816 | 6.8 | 11.1 | âˆ’4.3 | 1.0 | 1.330 | âˆ’408,901 | 1.8 | 81.64 | 87.74 |\\n| 2021 | 125,502,000 | 811,604 | 1,439,809 | âˆ’628,205 | 6.6 | 11.7 | âˆ’5.1 | 2.3 | 1.303 | âˆ’644,099 | 1.7 | 81.47 | 87.57 |\\n| 2022 | 124,947,000 | 770,759 | 1,569,050 | âˆ’798,291 | 6.1 | 12.5 | âˆ’6.4 | 1.9 | 1.257 | âˆ’555,000 | 1.8 | 80.74 | 86.88 |\\n| 2023 | 124,352,000 | 727,277 | 1,575,936 | âˆ’848,659 | 5.8 | 12.7 | âˆ’6.9 | 0.9 | 1.201 | âˆ’595,000 | 1.8 | 81.09 | 87.14 |\\n| 2024 | 123,802,000 | 686,061 | 1,605,298 | âˆ’919,237 | 5.5 | 12.9 | âˆ’7.5 | 3.0 | 1.148 | âˆ’550,000 | 1.8 | 81.09 | 87.13 |\\n| 2025 | 123,210,000 |  |  |  |  |  |  |  | 1.12(e) | â€“592,000 |  |  |  |\", \"score\": 0.9387167}]\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "**Population of Japan (most recent official estimate)**  \n",
            "- **2023:**â€¯â‰ˆâ€¯**124.352â€¯million** people  \n",
            "- **2024 (forecast):**â€¯â‰ˆâ€¯123.802â€¯million (declining trend continues)  \n",
            "\n",
            "These figures come from the Japanese Statistics Bureau (Statisticsâ€¯Japan), which releases annual population estimates every October based on the 2023 census and demographic data (births, deaths, migration).  \n",
            "\n",
            "| Year | Population | Source |\n",
            "|------|------------|--------|\n",
            "| 2023 | **124â€¯352â€¯000** | [Statisticsâ€¯Japan â€“ 2023 Population Estimate](https://www.stat.go.jp/english/data/jinsui/2023np/index.html) |\n",
            "| 2024 (est.) | **123â€¯802â€¯000** | [Statisticsâ€¯Japan â€“ 2024 Population Estimate](https://www.stat.go.jp/english/data/jinsui/2024np/index.html) |\n",
            "\n",
            "> *Key points*  \n",
            "> * Japan has been in a continuous population decline for 13 consecutive years (2023â€‘2024).  \n",
            "> * The decline is driven mainly by a negative natural change (deaths > births) and a modest positive migration balance that has not compensated for the natural decrease.  \n",
            "> * The ageâ€‘group breakdown (2023) is roughly: 11.4â€¯% under 15, 59.5â€¯% 15â€‘64, 29.1â€¯% 65â€¯+ (source above).  \n",
            "\n",
            "If you need more detailed demographic slices (by prefecture, age cohort, household composition, or projections beyond 2025), just let me know and Iâ€™ll pull the latest data.\n",
            "\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Ya that's helpful. How about the population of China?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search_results_json (fc_3f2cde1a-a3ad-41fd-bd77-43f5b0fd640d)\n",
            " Call ID: fc_3f2cde1a-a3ad-41fd-bd77-43f5b0fd640d\n",
            "  Args:\n",
            "    query: China population 2023 estimate 1.425 billion\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search_results_json\n",
            "\n",
            "[{\"title\": \"A disaster of their own making. The demographic crisis in China - OSW\", \"url\": \"https://www.osw.waw.pl/en/publikacje/osw-commentary/2024-02-07/a-disaster-their-own-making-demographic-crisis-china\", \"content\": \"The demographic situation in China is likely even more challenging than suggested by official NBS data. Some demographers, such as Yi Fuxian, Huang Wenzheng and Wang Feng, estimate that Chinaâ€™s population began to decrease as early as 2018 and never exceeded 1.3 billion. A data leak from the population registry in July 2022 revealed a registered population of 1.28 billion.(#_ftn3) Previously, the government in Beijing assumed that the countryâ€™s population would start to shrink no earlier than 2027. However, data provided by independent researchers and forecasts from the UN Department of Economic and Social Affairs (UNDESA) forced the Chinese government to adjust the NBSâ€™s data. According to UNDESA estimates in April 2023, India â€“ with a population of 1.425 billion â€“ outpaced China as the [...] According to data (see Appendix) published on 17 January by the National Bureau of Statistics of China (NBS), the countryâ€™s population at the end of 2023 stood at 1.409 billion, 2.08 million smaller than the previous year. This marks the second consecutive year of demographic decline, with a larger decrease than in 2022, when it was 0.85 million. At the same time, there was a larger decline (by over 10 million) in the working-age group (16â€“59 years), which numbers 864.8 million people, and its share in total population has dropped to 61.3%. [...] with a population of 1.425 billion â€“ outpaced China as the worldâ€™s most populous nation.\", \"score\": 0.86080295}, {\"title\": \"China Population (1950-2025) - Macrotrends\", \"url\": \"https://www.macrotrends.net/global-metrics/countries/chn/china/population\", \"content\": \"## China Population (1950-2025)\\n\\nPopulation Economy Trade Health Education Development Labor Force Environment Crime Immigration Other\\n\\nPopulation Growth Rate Density Urban Rural Life Expectancy Birth Rate Death Rate Infant Mortality Rate Fertility Rate\\n\\n##### Total current population for China in 2025 was 1,424,381,924, a 0.06% decline from 2024.\\n\\n Total population for China in 2024 was 1,425,178,782, a 1.03% increase from 2023.\\n Total population for China in 2023 was 1,410,710,000, a 0.1% decline from 2022.\\n Total population for China in 2022 was 1,412,175,000, a 0.01% decline from 2021.\", \"score\": 0.8467682}]\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search_results_json (fc_14cf8bb2-9169-494e-9aa3-05ca1dadfa83)\n",
            " Call ID: fc_14cf8bb2-9169-494e-9aa3-05ca1dadfa83\n",
            "  Args:\n",
            "    query: China population at the end of 2024 National Bureau of Statistics\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search_results_json\n",
            "\n",
            "[{\"title\": \"China's population declines in 2024\", \"url\": \"https://english.www.gov.cn/archive/statistics/202501/17/content_WS6789c3d8c6d0868f4e8eee45.html\", \"content\": \"HOME\\n NEWS\\n INSTITUTIONS\\n POLICIES\\n ARCHIVE\\n\\nä¸­æ–‡\\n\\n HOME\\n NEWS\\n INSTITUTIONS\\n POLICIES\\n ARCHIVE\\n ä¸­æ–‡\\n\\nChina's population declines in 2024\\n\\nUpdated: January 17, 2025 10:43 Xinhua\\n\\nBEIJING, Jan. 17 -- China's population on the mainland stood at 1.40828 billion by the end of 2024, a decrease of 1.39 million over that at the end of 2023, data from the National Bureau of Statistics showed Friday.\\n\\n### CopyrightÂ© www.gov.cn | About us | Contact us\\n\\n### Website Identification Code bm01000001 Registration Number: 05070218\\n\\n### All rights reserved. The content (including but not limited to text, photo, multimedia information, etc) published in this site belongs to www.gov.cn.\\n\\n### Without written authorization from www.gov.cn, such content shall not be republished or used in any form.\\n\\n#### Desktop)\", \"score\": 0.9573457}, {\"title\": \"China's demographic alarms blare as births hit historic low and ...\", \"url\": \"https://www.scmp.com/economy/china-economy/article/3340398/chinas-demographic-alarms-blare-births-hit-historic-low-and-population-shrinks-again\", \"content\": \"Only 7.92 million babies were born in 2025, down 17 per cent from 9.54 million in 2024, according to data released by the National Bureau of Statistics on Monday. This marked the lowest birth figure since records began in 1949 and broke the previous record low set in 2023.\\n\\nChinaâ€™s total population also fell by 3.39 million in 2025 to 1.4049 billion from 1.4083 billion a year earlier, the bureau said. By sheer numbers, that marked the steepest annual population decline on record, apart from during Chinaâ€™s devastating famine from 1959 to 1961.\\n\\nAdvertisement\\n\\nMeanwhile, about 11.31 million people died last year â€“ one of the highest totals in five decades.\", \"score\": 0.93707865}]\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "**Population of China (most recent official estimate)**  \n",
            "\n",
            "| Year | Population | Source |\n",
            "|------|------------|--------|\n",
            "| 2024 (end of year) | **1.408â€¯280â€¯million** | [National Bureau of Statistics (Xinhua) â€“ Janâ€¯17â€¯2025](https://english.www.gov.cn/archive/statistics/202501/17/content_WS6789c3d8c6d0868f4e8eee45.html) |\n",
            "| 2025 (midâ€‘2025 estimate) | **1.404â€¯900â€¯million** | [SCMP â€“ 2025 population drop](https://www.scmp.com/economy/china-economy/article/3340398/chinas-demographic-alarms-blare-births-hit-historic-low-and-population-shrinks-again) |\n",
            "\n",
            "> **Key takeâ€‘aways**  \n",
            "> * Chinaâ€™s population has **been shrinking** for the first time in more than 70â€¯years, with a decline of about 1.39â€¯million people in 2024 and a further 3.39â€¯million drop projected for 2025.  \n",
            "> * The decline is driven mainly by a **negative natural change** (deathsâ€¯>â€¯births) and only modest net migration gains.  \n",
            "> * The workingâ€‘age population (15â€“64â€¯years) has fallen by more than 10â€¯million in the last year and now represents roughly 61â€¯% of total population.  \n",
            "> * The trend is expected to continue for the foreseeable future unless major demographic policies change.\n",
            "\n",
            "If you need more granular data (ageâ€‘group breakdowns, regional distribution, or projections beyond 2025), just let me know!\n",
            "current population of Japan 2023\n",
            "Num Messages:  14 Next:  ()\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "**Population of China (most recent official estimate)**  \n",
            "\n",
            "| Year | Population | Source |\n",
            "|------|------------|--------|\n",
            "| 2024 (end of year) | **1.408â€¯280â€¯million** | [National Bureau of Statistics (Xinhua) â€“ Janâ€¯17â€¯2025](https://english.www.gov.cn/archive/statistics/202501/17/content_WS6789c3d8c6d0868f4e8eee45.html) |\n",
            "| 2025 (midâ€‘2025 estimate) | **1.404â€¯900â€¯million** | [SCMP â€“ 2025 population drop](https://www.scmp.com/economy/china-economy/article/3340398/chinas-demographic-alarms-blare-births-hit-historic-low-and-population-shrinks-again) |\n",
            "\n",
            "> **Key takeâ€‘aways**  \n",
            "> * Chinaâ€™s population has **been shrinking** for the first time in more than 70â€¯years, with a decline of about 1.39â€¯million people in 2024 and a further 3.39â€¯million drop projected for 2025.  \n",
            "> * The decline is driven mainly by a **negative natural change** (deathsâ€¯>â€¯births) and only modest net migration gains.  \n",
            "> * The workingâ€‘age population (15â€“64â€¯years) has fallen by more than 10â€¯million in the last year and now represents roughly 61â€¯% of total population.  \n",
            "> * The trend is expected to continue for the foreseeable future unless major demographic policies change.\n",
            "\n",
            "If you need more granular data (ageâ€‘group breakdowns, regional distribution, or projections beyond 2025), just let me know!\n",
            "None\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  13 Next:  ('chatbot',)\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search_results_json\n",
            "\n",
            "[{\"title\": \"China's population declines in 2024\", \"url\": \"https://english.www.gov.cn/archive/statistics/202501/17/content_WS6789c3d8c6d0868f4e8eee45.html\", \"content\": \"HOME\\n NEWS\\n INSTITUTIONS\\n POLICIES\\n ARCHIVE\\n\\nä¸­æ–‡\\n\\n HOME\\n NEWS\\n INSTITUTIONS\\n POLICIES\\n ARCHIVE\\n ä¸­æ–‡\\n\\nChina's population declines in 2024\\n\\nUpdated: January 17, 2025 10:43 Xinhua\\n\\nBEIJING, Jan. 17 -- China's population on the mainland stood at 1.40828 billion by the end of 2024, a decrease of 1.39 million over that at the end of 2023, data from the National Bureau of Statistics showed Friday.\\n\\n### CopyrightÂ© www.gov.cn | About us | Contact us\\n\\n### Website Identification Code bm01000001 Registration Number: 05070218\\n\\n### All rights reserved. The content (including but not limited to text, photo, multimedia information, etc) published in this site belongs to www.gov.cn.\\n\\n### Without written authorization from www.gov.cn, such content shall not be republished or used in any form.\\n\\n#### Desktop)\", \"score\": 0.9573457}, {\"title\": \"China's demographic alarms blare as births hit historic low and ...\", \"url\": \"https://www.scmp.com/economy/china-economy/article/3340398/chinas-demographic-alarms-blare-births-hit-historic-low-and-population-shrinks-again\", \"content\": \"Only 7.92 million babies were born in 2025, down 17 per cent from 9.54 million in 2024, according to data released by the National Bureau of Statistics on Monday. This marked the lowest birth figure since records began in 1949 and broke the previous record low set in 2023.\\n\\nChinaâ€™s total population also fell by 3.39 million in 2025 to 1.4049 billion from 1.4083 billion a year earlier, the bureau said. By sheer numbers, that marked the steepest annual population decline on record, apart from during Chinaâ€™s devastating famine from 1959 to 1961.\\n\\nAdvertisement\\n\\nMeanwhile, about 11.31 million people died last year â€“ one of the highest totals in five decades.\", \"score\": 0.93707865}]\n",
            "None\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  12 Next:  ('tools',)\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search_results_json (fc_14cf8bb2-9169-494e-9aa3-05ca1dadfa83)\n",
            " Call ID: fc_14cf8bb2-9169-494e-9aa3-05ca1dadfa83\n",
            "  Args:\n",
            "    query: China population at the end of 2024 National Bureau of Statistics\n",
            "None\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  11 Next:  ('chatbot',)\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search_results_json\n",
            "\n",
            "[{\"title\": \"A disaster of their own making. The demographic crisis in China - OSW\", \"url\": \"https://www.osw.waw.pl/en/publikacje/osw-commentary/2024-02-07/a-disaster-their-own-making-demographic-crisis-china\", \"content\": \"The demographic situation in China is likely even more challenging than suggested by official NBS data. Some demographers, such as Yi Fuxian, Huang Wenzheng and Wang Feng, estimate that Chinaâ€™s population began to decrease as early as 2018 and never exceeded 1.3 billion. A data leak from the population registry in July 2022 revealed a registered population of 1.28 billion.(#_ftn3) Previously, the government in Beijing assumed that the countryâ€™s population would start to shrink no earlier than 2027. However, data provided by independent researchers and forecasts from the UN Department of Economic and Social Affairs (UNDESA) forced the Chinese government to adjust the NBSâ€™s data. According to UNDESA estimates in April 2023, India â€“ with a population of 1.425 billion â€“ outpaced China as the [...] According to data (see Appendix) published on 17 January by the National Bureau of Statistics of China (NBS), the countryâ€™s population at the end of 2023 stood at 1.409 billion, 2.08 million smaller than the previous year. This marks the second consecutive year of demographic decline, with a larger decrease than in 2022, when it was 0.85 million. At the same time, there was a larger decline (by over 10 million) in the working-age group (16â€“59 years), which numbers 864.8 million people, and its share in total population has dropped to 61.3%. [...] with a population of 1.425 billion â€“ outpaced China as the worldâ€™s most populous nation.\", \"score\": 0.86080295}, {\"title\": \"China Population (1950-2025) - Macrotrends\", \"url\": \"https://www.macrotrends.net/global-metrics/countries/chn/china/population\", \"content\": \"## China Population (1950-2025)\\n\\nPopulation Economy Trade Health Education Development Labor Force Environment Crime Immigration Other\\n\\nPopulation Growth Rate Density Urban Rural Life Expectancy Birth Rate Death Rate Infant Mortality Rate Fertility Rate\\n\\n##### Total current population for China in 2025 was 1,424,381,924, a 0.06% decline from 2024.\\n\\n Total population for China in 2024 was 1,425,178,782, a 1.03% increase from 2023.\\n Total population for China in 2023 was 1,410,710,000, a 0.1% decline from 2022.\\n Total population for China in 2022 was 1,412,175,000, a 0.01% decline from 2021.\", \"score\": 0.8467682}]\n",
            "None\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  10 Next:  ('tools',)\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search_results_json (fc_3f2cde1a-a3ad-41fd-bd77-43f5b0fd640d)\n",
            " Call ID: fc_3f2cde1a-a3ad-41fd-bd77-43f5b0fd640d\n",
            "  Args:\n",
            "    query: China population 2023 estimate 1.425 billion\n",
            "None\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  9 Next:  ('chatbot',)\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Ya that's helpful. How about the population of China?\n",
            "None\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  8 Next:  ('__start__',)\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "**Population of Japan (most recent official estimate)**  \n",
            "- **2023:**â€¯â‰ˆâ€¯**124.352â€¯million** people  \n",
            "- **2024 (forecast):**â€¯â‰ˆâ€¯123.802â€¯million (declining trend continues)  \n",
            "\n",
            "These figures come from the Japanese Statistics Bureau (Statisticsâ€¯Japan), which releases annual population estimates every October based on the 2023 census and demographic data (births, deaths, migration).  \n",
            "\n",
            "| Year | Population | Source |\n",
            "|------|------------|--------|\n",
            "| 2023 | **124â€¯352â€¯000** | [Statisticsâ€¯Japan â€“ 2023 Population Estimate](https://www.stat.go.jp/english/data/jinsui/2023np/index.html) |\n",
            "| 2024 (est.) | **123â€¯802â€¯000** | [Statisticsâ€¯Japan â€“ 2024 Population Estimate](https://www.stat.go.jp/english/data/jinsui/2024np/index.html) |\n",
            "\n",
            "> *Key points*  \n",
            "> * Japan has been in a continuous population decline for 13 consecutive years (2023â€‘2024).  \n",
            "> * The decline is driven mainly by a negative natural change (deaths > births) and a modest positive migration balance that has not compensated for the natural decrease.  \n",
            "> * The ageâ€‘group breakdown (2023) is roughly: 11.4â€¯% under 15, 59.5â€¯% 15â€‘64, 29.1â€¯% 65â€¯+ (source above).  \n",
            "\n",
            "If you need more detailed demographic slices (by prefecture, age cohort, household composition, or projections beyond 2025), just let me know and Iâ€™ll pull the latest data.\n",
            "None\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  8 Next:  ()\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "**Population of Japan (most recent official estimate)**  \n",
            "- **2023:**â€¯â‰ˆâ€¯**124.352â€¯million** people  \n",
            "- **2024 (forecast):**â€¯â‰ˆâ€¯123.802â€¯million (declining trend continues)  \n",
            "\n",
            "These figures come from the Japanese Statistics Bureau (Statisticsâ€¯Japan), which releases annual population estimates every October based on the 2023 census and demographic data (births, deaths, migration).  \n",
            "\n",
            "| Year | Population | Source |\n",
            "|------|------------|--------|\n",
            "| 2023 | **124â€¯352â€¯000** | [Statisticsâ€¯Japan â€“ 2023 Population Estimate](https://www.stat.go.jp/english/data/jinsui/2023np/index.html) |\n",
            "| 2024 (est.) | **123â€¯802â€¯000** | [Statisticsâ€¯Japan â€“ 2024 Population Estimate](https://www.stat.go.jp/english/data/jinsui/2024np/index.html) |\n",
            "\n",
            "> *Key points*  \n",
            "> * Japan has been in a continuous population decline for 13 consecutive years (2023â€‘2024).  \n",
            "> * The decline is driven mainly by a negative natural change (deaths > births) and a modest positive migration balance that has not compensated for the natural decrease.  \n",
            "> * The ageâ€‘group breakdown (2023) is roughly: 11.4â€¯% under 15, 59.5â€¯% 15â€‘64, 29.1â€¯% 65â€¯+ (source above).  \n",
            "\n",
            "If you need more detailed demographic slices (by prefecture, age cohort, household composition, or projections beyond 2025), just let me know and Iâ€™ll pull the latest data.\n",
            "None\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  7 Next:  ('chatbot',)\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search_results_json\n",
            "\n",
            "[{\"title\": \"Current Population Estimates as of October 1, 2023\", \"url\": \"https://www.stat.go.jp/english/data/jinsui/2023np/index.html\", \"content\": \"### Population for Japan\\n\\n[Total Population]\\n\\n The total population was 124,352 thousand, a decrease of 595 thousand compared with the previous year. The rate of decrease was 0.48 percent. The total population decreased for the thirteenth year in a row.\\n The male population was 60,492  thousand, a decrease of 265  thousand (0.44 percent), while the female population was 63,859  thousand, a decrease of 330   thousand (0.51 percent).\\n The natural change of the male population was negative for the nineteenth year in a row, and that of the female population was negative for the fifteenth year in a row.\\n The migration change of the Japanese population was positive for the first time in three years, and that of the foreign population was positive for the two years in a row. [...] [Population by Age Group]\\n\\n The population under 15 years  old was 14,173 thousand (11.4  percent of the total population).\\n The population aged 15 to 64 was 73,952 thousand (59.5 percent of the total population).\\n The population aged 65 years old and over was 36,227 thousand (29.1 percent of the total population).\\n\\n### Population by Prefecture\\n\\n[Population]\\n\\n The top five prefectures in population were Tokyo-to, Kanagawa-ken, Osaka-fu, Aichi-ken and Saitama-ken. These prefectures account for 37.7 percent of the total population.\\n Tokyo-to had the largest population in 2023 (11.3 percent of the total population).\\n\\n[Rates of Population Change]\\n\\n The population increased only in Tokyo-to.\\n The population decreased in 46 prefectures. Akita-ken had the highest rate of decrease in population.\", \"score\": 0.9564761}, {\"title\": \"Demographics of Japan - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Demographics_of_Japan\", \"content\": \"According to a demographic study conducted by Japanese Ministry of Internal Affairs and Communications, the Japanese population, including foreign residents, declined from 128 million people in 2010 to 124.3 million people in 2023, with a decrease of almost 511,000 people in one year.\\n\\nView source data.\\n\\nView source data.\\n\\nView source data.\\n\\nView source data.\\n\\nView source data.\\n\\n## Population\\n\\n[edit]\\n\\n### Census\\n\\n[edit]\\n\\nSee also: Population and housing censuses by country\\n\\nJapan collects census information every five years, with censuses conducted by the Statistics Bureau \\\"Statistics Bureau (Japan)\\\") of the Ministry of Internal Affairs. The latest population census reflects the situation as of 2020.\\n\\n### Population density\\n\\n[edit] [...] Japan dropped from the 5th most populous country in the world to 6th in 1964, 7th in 1978, 8th in 1990, to 9th in 1998, to 10th in the early 21st century, 11th in 2020, and to 12th in 2023. Between 2010 to 2015, Japan's population shrank by almost a million, and Japan lost a half-million in 2022 alone. The number of Japanese citizens decreased by 801,000 to 122,423,038 in 2022 from a year earlier, which was the most severe decrease and the first time all 47 prefectures have suffered a decline since the launch of the poll in 1968. In early 2010, Japan's population reached 128,057,352. In the 2010s, the long-lasting effects of Japanese economic crisis during the Great Recession strongly slowed down immigration rates in Japan. [...] | 2020 | 126,146,099 | 840,832 | 1,372,648 | âˆ’531,816 | 6.8 | 11.1 | âˆ’4.3 | 1.0 | 1.330 | âˆ’408,901 | 1.8 | 81.64 | 87.74 |\\n| 2021 | 125,502,000 | 811,604 | 1,439,809 | âˆ’628,205 | 6.6 | 11.7 | âˆ’5.1 | 2.3 | 1.303 | âˆ’644,099 | 1.7 | 81.47 | 87.57 |\\n| 2022 | 124,947,000 | 770,759 | 1,569,050 | âˆ’798,291 | 6.1 | 12.5 | âˆ’6.4 | 1.9 | 1.257 | âˆ’555,000 | 1.8 | 80.74 | 86.88 |\\n| 2023 | 124,352,000 | 727,277 | 1,575,936 | âˆ’848,659 | 5.8 | 12.7 | âˆ’6.9 | 0.9 | 1.201 | âˆ’595,000 | 1.8 | 81.09 | 87.14 |\\n| 2024 | 123,802,000 | 686,061 | 1,605,298 | âˆ’919,237 | 5.5 | 12.9 | âˆ’7.5 | 3.0 | 1.148 | âˆ’550,000 | 1.8 | 81.09 | 87.13 |\\n| 2025 | 123,210,000 |  |  |  |  |  |  |  | 1.12(e) | â€“592,000 |  |  |  |\", \"score\": 0.9387167}]\n",
            "None\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  6 Next:  ('tools',)\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search_results_json (fc_13283d5f-acdc-4165-ad45-d8c0d2430e42)\n",
            " Call ID: fc_13283d5f-acdc-4165-ad45-d8c0d2430e42\n",
            "  Args:\n",
            "    query: population of Japan 2023\n",
            "None\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  5 Next:  ('chatbot',)\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Ya that's helpful. How about the population of Japan?\n",
            "None\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  4 Next:  ('__start__',)\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "**Population of Southâ€¯Korea (as of the most recent official estimate)**  \n",
            "- **2023:**â€¯â‰ˆâ€¯**51.7â€¯million** people  \n",
            "- **2024 (forecast):**â€¯â‰ˆâ€¯51.74â€¯million (slight increase)\n",
            "\n",
            "These figures come from the South Korean National Statistical Officeâ€™s 2023 Population and Housing Census and the subsequent annual estimates published by Statisticsâ€¯Korea.  \n",
            "\n",
            "| Year | Population | Source |\n",
            "|------|------------|--------|\n",
            "| 2023 | **51â€¯712â€¯619** | [Statisticsâ€¯Korea â€“ 2023 Census](https://mods.go.kr/board.es?mid=a20108010000&bid=11747&act=view&list_no=432395) |\n",
            "| 2024 (est.) | **51â€¯741â€¯963** | [Macrotrends â€“ South Korea Population](https://www.macrotrends.net/global-metrics/countries/kor/south-korea/population) |\n",
            "\n",
            "> *Note:* The 2023 census data were released on Novemberâ€¯1,â€¯2023. The 2024 figure is an annual estimate based on demographic trends (births, deaths, net migration) and is updated annually by Statisticsâ€¯Korea.  \n",
            "\n",
            "If you need more granular details (e.g., ageâ€‘group breakdowns, regional distribution, household composition) or the most recent quarterly updates, let me know and I can pull the latest figures for you.\n",
            "None\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  4 Next:  ()\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "**Population of Southâ€¯Korea (as of the most recent official estimate)**  \n",
            "- **2023:**â€¯â‰ˆâ€¯**51.7â€¯million** people  \n",
            "- **2024 (forecast):**â€¯â‰ˆâ€¯51.74â€¯million (slight increase)\n",
            "\n",
            "These figures come from the South Korean National Statistical Officeâ€™s 2023 Population and Housing Census and the subsequent annual estimates published by Statisticsâ€¯Korea.  \n",
            "\n",
            "| Year | Population | Source |\n",
            "|------|------------|--------|\n",
            "| 2023 | **51â€¯712â€¯619** | [Statisticsâ€¯Korea â€“ 2023 Census](https://mods.go.kr/board.es?mid=a20108010000&bid=11747&act=view&list_no=432395) |\n",
            "| 2024 (est.) | **51â€¯741â€¯963** | [Macrotrends â€“ South Korea Population](https://www.macrotrends.net/global-metrics/countries/kor/south-korea/population) |\n",
            "\n",
            "> *Note:* The 2023 census data were released on Novemberâ€¯1,â€¯2023. The 2024 figure is an annual estimate based on demographic trends (births, deaths, net migration) and is updated annually by Statisticsâ€¯Korea.  \n",
            "\n",
            "If you need more granular details (e.g., ageâ€‘group breakdowns, regional distribution, household composition) or the most recent quarterly updates, let me know and I can pull the latest figures for you.\n",
            "None\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  3 Next:  ('chatbot',)\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search_results_json\n",
            "\n",
            "[{\"title\": \"2023 Population and Housing Census (Register-based Census)\", \"url\": \"https://mods.go.kr/board.es?mid=a20108010000&bid=11747&act=view&list_no=432395\", \"content\": \"[Population] - As of November 1st, 2023, the population of South Korea was 51.77 million persons, which increased by 0.2% (82 thousand persons) from 2022. - The working age population aged 15~64 amounted to 36.55 million persons (70.6%). This figure had shown a steadily decreasing trend since 2018. - In 2023, the population of the Capital Area was 26.23 million persons, which occupied 50.7% of the total population. This figure showed a steadily increasing trend. [Household] - As of November 1st, 2023, the number of households totaled 22.73 million households, increasing by 1.5% (345 thousand households) from the previous year. - Among general households, 1-person and 2-person households accounted for 64.2% (14.18 million households). The average household size stood at 2.21 persons in [...] (35.5%), rising by 4.4% from 2022. - In 2023, the elderly Koreans aged 65 or more recorded 9.50 million persons, showing a continuously increasing trend. Households with an elderly member amounted to 6.81 million households, showing a continuously increasing trend. - In 2023, the multi-cultural households amounted to 416 thousand households. Naturalized Koreans and marriage immigrants amounted to 413 thousand persons. These two figures showed an ever-increasing trend. â€» For more information, refer to the attached file. [...] The average household size stood at 2.21 persons in 2023. - Among general households, relative households took up 62.1%. Household residing in apartments took up 53.1%. [Housing] - In 2023 the number of housing units totaled 19.55 million units, rising by 2.0% (391 thousand units) compared to 2022. - As for the year-on-year percent change in housing units by province, Daegu (5.0%) recorded the highest figure. Gyeongbuk (-0.1%) recorded the lowest figure. - Housing units constructed 20 years ago or more accounted for 53.7% of the total housing units. Housing units constructed 30 years ago or more accounted for 25.8% of the total housing units. [Major population groups] - In 2023, 1-person households marked 7.83 million households (35.5%), rising by 4.4% from 2022. - In 2023, the elderly\", \"score\": 0.9657998}, {\"title\": \"South Korea Population (1950-2025) - Macrotrends\", \"url\": \"https://www.macrotrends.net/global-metrics/countries/kor/south-korea/population\", \"content\": \"## South Korea Population (1950-2025)\\n\\nPopulation Economy Trade Health Education Development Labor Force Environment Crime Immigration Other\\n\\nPopulation Growth Rate Density Urban Rural Life Expectancy Birth Rate Death Rate Infant Mortality Rate Fertility Rate\\n\\n##### Total current population for South Korea in 2025 was 51,690,479, a 0.1% decline from 2024.\\n\\n Total population for South Korea in 2024 was 51,741,963, a 0.06% increase from 2023.\\n Total population for South Korea in 2023 was 51,712,619, a 0.08% increase from 2022.\\n Total population for South Korea in 2022 was 51,672,569, a 0.19% decline from 2021.\", \"score\": 0.9421299}]\n",
            "None\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  2 Next:  ('tools',)\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search_results_json (fc_391112ea-87e7-4a21-863c-6d75a5d1026f)\n",
            " Call ID: fc_391112ea-87e7-4a21-863c-6d75a5d1026f\n",
            "  Args:\n",
            "    query: population of South Korea 2023\n",
            "None\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  1 Next:  ('chatbot',)\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is the population of South Korea?Could you do some research on it for me?\n",
            "None\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  0 Next:  ('__start__',)\n",
            "--------------------------------------------------------------------------------\n",
            "q\n",
            "Replay this state *********\n",
            "('tools',)\n",
            "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f108ba9-a3ba-6943-8006-9466367c6ee0'}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search_results_json (fc_13283d5f-acdc-4165-ad45-d8c0d2430e42)\n",
            " Call ID: fc_13283d5f-acdc-4165-ad45-d8c0d2430e42\n",
            "  Args:\n",
            "    query: population of Japan 2023\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search_results_json\n",
            "\n",
            "[{\"title\": \"Current Population Estimates as of October 1, 2023\", \"url\": \"https://www.stat.go.jp/english/data/jinsui/2023np/index.html\", \"content\": \"### Population for Japan\\n\\n[Total Population]\\n\\n The total population was 124,352 thousand, a decrease of 595 thousand compared with the previous year. The rate of decrease was 0.48 percent. The total population decreased for the thirteenth year in a row.\\n The male population was 60,492  thousand, a decrease of 265  thousand (0.44 percent), while the female population was 63,859  thousand, a decrease of 330   thousand (0.51 percent).\\n The natural change of the male population was negative for the nineteenth year in a row, and that of the female population was negative for the fifteenth year in a row.\\n The migration change of the Japanese population was positive for the first time in three years, and that of the foreign population was positive for the two years in a row. [...] [Population by Age Group]\\n\\n The population under 15 years  old was 14,173 thousand (11.4  percent of the total population).\\n The population aged 15 to 64 was 73,952 thousand (59.5 percent of the total population).\\n The population aged 65 years old and over was 36,227 thousand (29.1 percent of the total population).\\n\\n### Population by Prefecture\\n\\n[Population]\\n\\n The top five prefectures in population were Tokyo-to, Kanagawa-ken, Osaka-fu, Aichi-ken and Saitama-ken. These prefectures account for 37.7 percent of the total population.\\n Tokyo-to had the largest population in 2023 (11.3 percent of the total population).\\n\\n[Rates of Population Change]\\n\\n The population increased only in Tokyo-to.\\n The population decreased in 46 prefectures. Akita-ken had the highest rate of decrease in population.\", \"score\": 0.9564761}, {\"title\": \"Demographics of Japan - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Demographics_of_Japan\", \"content\": \"According to a demographic study conducted by Japanese Ministry of Internal Affairs and Communications, the Japanese population, including foreign residents, declined from 128 million people in 2010 to 124.3 million people in 2023, with a decrease of almost 511,000 people in one year.\\n\\nView source data.\\n\\nView source data.\\n\\nView source data.\\n\\nView source data.\\n\\nView source data.\\n\\n## Population\\n\\n[edit]\\n\\n### Census\\n\\n[edit]\\n\\nSee also: Population and housing censuses by country\\n\\nJapan collects census information every five years, with censuses conducted by the Statistics Bureau \\\"Statistics Bureau (Japan)\\\") of the Ministry of Internal Affairs. The latest population census reflects the situation as of 2020.\\n\\n### Population density\\n\\n[edit] [...] Japan dropped from the 5th most populous country in the world to 6th in 1964, 7th in 1978, 8th in 1990, to 9th in 1998, to 10th in the early 21st century, 11th in 2020, and to 12th in 2023. Between 2010 to 2015, Japan's population shrank by almost a million, and Japan lost a half-million in 2022 alone. The number of Japanese citizens decreased by 801,000 to 122,423,038 in 2022 from a year earlier, which was the most severe decrease and the first time all 47 prefectures have suffered a decline since the launch of the poll in 1968. In early 2010, Japan's population reached 128,057,352. In the 2010s, the long-lasting effects of Japanese economic crisis during the Great Recession strongly slowed down immigration rates in Japan. [...] | 2020 | 126,146,099 | 840,832 | 1,372,648 | âˆ’531,816 | 6.8 | 11.1 | âˆ’4.3 | 1.0 | 1.330 | âˆ’408,901 | 1.8 | 81.64 | 87.74 |\\n| 2021 | 125,502,000 | 811,604 | 1,439,809 | âˆ’628,205 | 6.6 | 11.7 | âˆ’5.1 | 2.3 | 1.303 | âˆ’644,099 | 1.7 | 81.47 | 87.57 |\\n| 2022 | 124,947,000 | 770,759 | 1,569,050 | âˆ’798,291 | 6.1 | 12.5 | âˆ’6.4 | 1.9 | 1.257 | âˆ’555,000 | 1.8 | 80.74 | 86.88 |\\n| 2023 | 124,352,000 | 727,277 | 1,575,936 | âˆ’848,659 | 5.8 | 12.7 | âˆ’6.9 | 0.9 | 1.201 | âˆ’595,000 | 1.8 | 81.09 | 87.14 |\\n| 2024 | 123,802,000 | 686,061 | 1,605,298 | âˆ’919,237 | 5.5 | 12.9 | âˆ’7.5 | 3.0 | 1.148 | âˆ’550,000 | 1.8 | 81.09 | 87.13 |\\n| 2025 | 123,210,000 |  |  |  |  |  |  |  | 1.12(e) | â€“592,000 |  |  |  |\", \"score\": 0.9387167}]\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "**Population of Japan (latest official estimate)**  \n",
            "- **2023:** **â‰ˆâ€¯124.4â€¯million** people (124â€¯352â€¯000)\n",
            "\n",
            "| Year | Population (millions) | Source |\n",
            "|------|----------------------|--------|\n",
            "| 2023 | **124.4â€¯m** | [Statisticsâ€¯Japan â€“ Current Population Estimates (Octâ€¯1,â€¯2023)](https://www.stat.go.jp/english/data/jinsui/2023np/index.html) |\n",
            "| 2022 | 124.9â€¯m | same source |\n",
            "| 2024 (forecast) | 123.8â€¯m | same source |\n",
            "\n",
            "### Key points\n",
            "\n",
            "| Metric | 2023 value | Trend |\n",
            "|--------|------------|-------|\n",
            "| Total population | 124â€¯352â€¯000 | **Down 0.48â€¯%** from 2022 (decrease of 595â€¯k) |\n",
            "| Male population | 60â€¯492â€¯000 | â†“0.44â€¯% |\n",
            "| Female population | 63â€¯859â€¯000 | â†“0.51â€¯% |\n",
            "| Age distribution | 11.4â€¯% underâ€¯15, 59.5â€¯% 15â€‘64, 29.1â€¯% â‰¥65 | Aging population â€“ 29.1â€¯% are 65â€¯+ |\n",
            "| Migration | First positive net migration in 3â€¯years | Foreign residents + for two consecutive years |\n",
            "\n",
            "Japanâ€™s population has been in a slow decline for more than a decade, largely due to low birth rates and an ageing demographic. The 2023 estimate shows a continued, modest decrease of roughly 595â€¯k people, bringing the total to about 124â€¯million residents.\n",
            "\n",
            "If youâ€™d like deeper detail (prefectureâ€‘level figures, historical trends, or projected figures for 2025â€‘2030), just let me know!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OyXrIpnmOPbb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}