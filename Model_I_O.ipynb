{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1vCs8ECJ1sK",
        "outputId": "cd471efe-3601-4dd2-abcd-6d2620658996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m458.9/458.9 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U \"langchain==0.3.*\" \"langchain-core==0.3.*\" \"langchain-community==0.3.*\" \"langgraph==0.3.*\" \"langchain-huggingface\" \"huggingface_hub\" \"sentence-transformers\" wikipedia -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM / Chat Model"
      ],
      "metadata": {
        "id": "tmpiq0oyKE3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMiG02MoKCVc",
        "outputId": "5d9fa214-e34e-400a-bd74-9951a508f445"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def load_api_keys(filepath=\"api_key.txt\"):\n",
        "    with open(filepath, \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line and \"=\" in line:\n",
        "                key, value = line.split(\"=\", 1)\n",
        "                os.environ[key.strip()] = value.strip()\n",
        "\n",
        "path = '/content/drive/MyDrive/LangGraph/'\n",
        "\n",
        "# API í‚¤ ë¡œë“œ ë° í™˜ê²½ë³€ìˆ˜ ì„¤ì •\n",
        "load_api_keys(path + 'api_key.txt')"
      ],
      "metadata": {
        "id": "_KdSmwwqKjls"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.schema import SystemMessage, HumanMessage"
      ],
      "metadata": {
        "id": "RQcvpiXGKVAt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HuggingFaceì—ì„œ í•´ë‹¹ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì—”ë“œí¬ì¸íŠ¸ ì§€ì •\n",
        "llm_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"conversational\")\n",
        "\n",
        "# HuggingFaceì—ì„œ ê°€ì ¸ì˜¨ ëª¨ë¸ì„ ê·¸ëŒ€ë¡œ ì“°ì§€ ì•Šê³ ,\n",
        "# LangChainì—ì„œ ì‰½ê²Œ ì“°ë„ë¡ ê°ì‹¸ëŠ”(wrapper) ë‹¨ê³„\n",
        "llm = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "text = \"What would be a good compnay name for a company that makes colorful socks?\"\n",
        "messages = [SystemMessage(content = \"You are the teenager\"),\n",
        "            HumanMessage(content=text)]\n",
        "print(llm.invoke(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLNQWMZoKd8S",
        "outputId": "9448f916-d3b2-42c5-e8fc-14a7c982593d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Here are a handful of name ideas that capture the playful, vibrant spirit of a colorfulâ€‘sock business. Each one has a short explanation to help you see how it might fit your brand identity and market positioning.\\n\\n| # | Name | Why it works |\\n|---|------|--------------|\\n| 1 | **SockSplash** | Conveys a burst of color, like a splash of paint, and is easy to remember. |\\n| 2 | **KnitKaleidoscope** | Marries the craft (â€œknitâ€) with the visual appeal of a kaleidoscopeâ€”perfect for a wide palette. |\\n| 3 | **HueStep** | â€œHueâ€ for color, â€œStepâ€ for walkingâ€”implies every step is a statement. |\\n| 4 | **VividToes** | Focuses on the toe area, the most visible part of socks, and highlights vivid color. |\\n| 5 | **ThreadTastic** | A playful take on â€œfantasticâ€ that emphasizes the thread (material) and fun. |\\n| 6 | **Spectrum Socks** | Directly references a range of colors; good for a line that showcases gradients or multiâ€‘tone designs. |\\n| 7 | **PizzazzPeds** | â€œPeddâ€ is a playful take on â€œpedsâ€ (feet) with a punch of style. |\\n| 8 | **Socks & Spright** | â€œSprightâ€ evokes a lively, spirited feelâ€”good for a brand that wants to stand out. |\\n| 9 | **KaleidoKicks** | A nod to the colorful patterns (kaleidoscope) and the â€œkickâ€ (the action of wearing). |\\n|10 | **LivelyLoops** | Emphasizes the looping knit process and the liveliness of the colors. |\\n\\n### Tips for Picking the Right Name\\n1. **Check Domain Availability** â€“ A clean, memorable web domain (e.g., *sockSplash.com*) is a huge plus.  \\n2. **Trademark Search** â€“ Make sure the name isnâ€™t already in use in the apparel or sock sector.  \\n3. **Cultural Resonance** â€“ If you plan to sell internationally, test how the name sounds or translates in key markets.  \\n4. **Scalability** â€“ Think beyond socks: if you ever expand to shoes, scarves, or other apparel, will the name still fit?  \\n5. **Visual Identity** â€“ Consider how the name will look on packaging, logos, and social mediaâ€”short, snappy, and easy to stylize.  \\n\\nFeel free to mix and match elements (e.g., *HueLoop Socks*, *KnitKaleido*). The right name should feel alive, colorful, and instantly suggest comfort and fun. Good luck with your brand launch!' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 622, 'prompt_tokens': 87, 'total_tokens': 709}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_756f495284', 'finish_reason': 'stop', 'logprobs': None} id='run--019c142f-9dd1-7200-99e9-f695ee50fe81-0' usage_metadata={'input_tokens': 87, 'output_tokens': 622, 'total_tokens': 709}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "\n",
        "# 1. ì—”ë“œí¬ì¸íŠ¸ ì„¤ì • (ChatOpenAIì˜ ì—”ì§„ ì—­í• )\n",
        "llm_ep = HuggingFaceEndpoint(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\", # ë„êµ¬ ì‚¬ìš©ì´ë‚˜ ëŒ€í™”ì— ìµœì í™”ëœ ëª¨ë¸ ê¶Œì¥\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512\n",
        ")\n",
        "\n",
        "# 2. ChatOpenAI ì—­í• ì„ í•˜ëŠ” ë˜í¼ ìƒì„±\n",
        "chat_model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# 3. ë©”ì‹œì§€ ê¸°ë°˜ í˜¸ì¶œ\n",
        "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a creative teenager.\"),\n",
        "    HumanMessage(content=text)\n",
        "]\n",
        "\n",
        "# í˜¸ì¶œ ë° ì¶œë ¥\n",
        "response = chat_model.invoke(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPFR52PlLRV1",
        "outputId": "262f36a9-1652-4dcc-8270-91f2e1bee7aa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SockPairz - this name emphasizes the idea of having pairs of colorful socks and playfully highlights the concept of pairing up socks. Other options could be:\n",
            "1. VibrantVibe Socks\n",
            "2. ChromaSox\n",
            "3. BrightBundles\n",
            "4. RadiantRivets (rivets are often used to hold sock seams together, adding a fun twist to the name\n",
            "5. FunkyFeet Fashion\n",
            "6. RainbowRhythms\n",
            "7. HueHaven\n",
            "8. ColorCrew Socks\n",
            "9. SockSpectrum\n",
            "10. BoldBundles Brands\n",
            "11. HappyFeet Hues\n",
            "12. Kaleidosock\n",
            "13. ColorsConnect\n",
            "14. ChromaKicks\n",
            "15. SoulfulSocks (the use of \"soul\" here could symbolize the unique personality and style that comes when wearing colorful socks)\n",
            "16. RazzleRainbow Ridges (this name sparks the image of colorful stripes or patterns)\n",
            "17. SpectrumSockery\n",
            "18. VibrantVision Socks\n",
            "19. HueHEights (probably avail socks in different heights like no-show, ankle, quarter, mid-calf, knee-high, etc.)\n",
            "20. FatfootFabrics (playfully emphasizes the thickness of the sock material)\n",
            "21. HipHues Socks (teens are known for being fashionable, so the \"hip\" in the name aligns well)\n",
            "22. FunFeet Fashions\n",
            "23. Colorista Socks\n",
            "24. FunFlair Fibers\n",
            "25. RainbowRiot\n",
            "26. ColourCreations Socks\n",
            "27. Hockstix\n",
            "28. Sockistry (the ending \"-istry\" is reminiscent of creativity and design)\n",
            "29. WowSox\n",
            "30. ColorCaddies.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Template"
      ],
      "metadata": {
        "id": "HQKE68HUNZ5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
        "prompt_output = prompt.format(product=\"colorful socks\")\n",
        "print(prompt_output)\n",
        "# What is a good name for a company that makes colorful socks?\n",
        "\n",
        "from langchain.prompts.chat import ChatPromptTemplate\n",
        "\n",
        "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "human_template = \"{text}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template), # role\n",
        "    (\"human\", human_template), # content\n",
        "])\n",
        "\n",
        "chat_prompt_output = chat_prompt.format_messages(\n",
        "                        input_language=\"English\",\n",
        "                        output_language=\"French\",\n",
        "                        text=\"I love programming.\")\n",
        "print(chat_prompt_output)\n",
        "# [SystemMessage(content='You are a helpful assistant that translates English to French.'),\n",
        "#   HumanMessage(content='I love programming.')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnU1PC7jMHQ8",
        "outputId": "38bc1b5d-733b-432d-9150-5709dbd9b8a8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is a good name for a company that makes colorful socks?\n",
            "[SystemMessage(content='You are a helpful assistant that translates English to French.', additional_kwargs={}, response_metadata={}), HumanMessage(content='I love programming.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "llm_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"text-generation\")\n",
        "llm = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "llm.invoke(chat_prompt_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DDKFSBaPcDf",
        "outputId": "ba7dd1dd-dc85-402c-b047-2723d62f4ee7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 95, 'prompt_tokens': 89, 'total_tokens': 184}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_334cc21c60', 'finish_reason': 'stop', 'logprobs': None}, id='run--019c1446-4821-7410-a2ab-b1c62a706cc2-0', usage_metadata={'input_tokens': 89, 'output_tokens': 95, 'total_tokens': 184})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering"
      ],
      "metadata": {
        "id": "gW0lbOLqVCQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "examples = [\n",
        "  {\n",
        "    \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
        "    \"answer\":\n",
        "\"\"\"\n",
        "Are follow up questions needed here: Yes.\n",
        "Follow up: How old was Muhammad Ali when he died?\n",
        "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
        "Follow up: How old was Alan Turing when he died?\n",
        "Intermediate answer: Alan Turing was 41 years old when he died.\n",
        "So the final answer is: Muhammad Ali\n",
        "\"\"\"\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"When was the founder of craigslist born?\",\n",
        "    \"answer\":\n",
        "\"\"\"\n",
        "Are follow up questions needed here: Yes.\n",
        "Follow up: Who was the founder of craigslist?\n",
        "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
        "Follow up: When was Craig Newmark born?\n",
        "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
        "So the final answer is: December 6, 1952\n",
        "\"\"\"\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"Who was the maternal grandfather of George Washington?\",\n",
        "    \"answer\":\n",
        "\"\"\"\n",
        "Are follow up questions needed here: Yes.\n",
        "Follow up: Who was the mother of George Washington?\n",
        "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
        "Follow up: Who was the father of Mary Ball Washington?\n",
        "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
        "So the final answer is: Joseph Ball\n",
        "\"\"\"\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",\n",
        "    \"answer\":\n",
        "\"\"\"\n",
        "Are follow up questions needed here: Yes.\n",
        "Follow up: Who is the director of Jaws?\n",
        "Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
        "Follow up: Where is Steven Spielberg from?\n",
        "Intermediate Answer: The United States.\n",
        "Follow up: Who is the director of Casino Royale?\n",
        "Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
        "Follow up: Where is Martin Campbell from?\n",
        "Intermediate Answer: New Zealand.\n",
        "So the final answer is: No\n",
        "\"\"\"\n",
        "  }\n",
        "]\n",
        "\n",
        "example_prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\")\n",
        "\n",
        "print(example_prompt.format(**examples[0]))\n",
        "\n",
        "prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    suffix=\"Question: {input}\",\n",
        "    input_variables=[\"input\"]\n",
        ")\n",
        "\n",
        "print(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "llm_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"text-generation\")\n",
        "llm = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "llm.invoke(chat_prompt_output)\n",
        "print(llm.invoke(prompt.format(input=\"Who was the father of Mary Ball Washington?\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s12OJTstP13a",
        "outputId": "04f1ac68-f01e-415b-9568-781605fbc87c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
            "\n",
            "Are follow up questions needed here: Yes.\n",
            "Follow up: How old was Muhammad Ali when he died?\n",
            "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
            "Follow up: How old was Alan Turing when he died?\n",
            "Intermediate answer: Alan Turing was 41 years old when he died.\n",
            "So the final answer is: Muhammad Ali\n",
            "\n",
            "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
            "\n",
            "Are follow up questions needed here: Yes.\n",
            "Follow up: How old was Muhammad Ali when he died?\n",
            "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
            "Follow up: How old was Alan Turing when he died?\n",
            "Intermediate answer: Alan Turing was 41 years old when he died.\n",
            "So the final answer is: Muhammad Ali\n",
            "\n",
            "\n",
            "Question: When was the founder of craigslist born?\n",
            "\n",
            "Are follow up questions needed here: Yes.\n",
            "Follow up: Who was the founder of craigslist?\n",
            "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
            "Follow up: When was Craig Newmark born?\n",
            "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
            "So the final answer is: December 6, 1952\n",
            "\n",
            "\n",
            "Question: Who was the maternal grandfather of George Washington?\n",
            "\n",
            "Are follow up questions needed here: Yes.\n",
            "Follow up: Who was the mother of George Washington?\n",
            "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
            "Follow up: Who was the father of Mary Ball Washington?\n",
            "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
            "So the final answer is: Joseph Ball\n",
            "\n",
            "\n",
            "Question: Are both the directors of Jaws and Casino Royale from the same country?\n",
            "\n",
            "Are follow up questions needed here: Yes.\n",
            "Follow up: Who is the director of Jaws?\n",
            "Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
            "Follow up: Where is Steven Spielberg from?\n",
            "Intermediate Answer: The United States.\n",
            "Follow up: Who is the director of Casino Royale?\n",
            "Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
            "Follow up: Where is Martin Campbell from?\n",
            "Intermediate Answer: New Zealand.\n",
            "So the final answer is: No\n",
            "\n",
            "\n",
            "Question: Who was the father of Mary Ball Washington?\n",
            "content='Joseph Ball.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 126, 'prompt_tokens': 437, 'total_tokens': 563}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_3023a70d60', 'finish_reason': 'stop', 'logprobs': None} id='run--019c1454-dfd0-7352-94c2-cb97f01a9df9-0' usage_metadata={'input_tokens': 437, 'output_tokens': 126, 'total_tokens': 563}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output Parser1"
      ],
      "metadata": {
        "id": "QI8LQFaFWpZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import BaseOutputParser\n",
        "\n",
        "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
        "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
        "\n",
        "\n",
        "    def parse(self, text: str):\n",
        "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
        "        return text.strip().split(\", \")\n",
        "\n",
        "# When LLM returns the response in text format\n",
        "print(CommaSeparatedListOutputParser().parse(\"hi, bye\"))\n",
        "# >> ['hi', 'bye']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0cj4PCpVOBF",
        "outputId": "c9f7879b-7e29-44f2-f27f-b0f17af51fd4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hi', 'bye']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from typing import List\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import BaseOutputParser\n",
        "\n",
        "class CommaSeparatedListOutputParser(BaseOutputParser[List[str]]):\n",
        "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
        "    def parse(self, text: str) -> List[str]:\n",
        "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
        "        return text.strip().split(\", \")\n",
        "\n",
        "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
        "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
        "ONLY return a comma separated list, and nothing more.\"\"\"\n",
        "human_template = \"{text}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template),\n",
        "    (\"human\", human_template),\n",
        "])\n",
        "\n",
        "llm_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"text-generation\")\n",
        "llm = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# LangChain Expression Language(LCEL)\n",
        "chain = chat_prompt | llm | CommaSeparatedListOutputParser()\n",
        "print(chain.invoke({\"text\": \"colors\"}))\n",
        "# ['red', 'blue', 'green', 'yellow', 'purple']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kreIM8LUblfp",
        "outputId": "bed6a3d8-3b8e-4355-88a7-37546beb69ed"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['red', 'blue', 'green', 'yellow', 'purple']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output Parser2"
      ],
      "metadata": {
        "id": "NDR8quF_cKmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## list_parser"
      ],
      "metadata": {
        "id": "YXYrEa24mLxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "# --- 1. ì¶œë ¥ íŒŒì„œ ì„¤ì • ---\n",
        "# ëª¨ë¸ì—ê²Œ \"ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì¤˜\"ë¼ê³  ëª…ë ¹í•˜ê³ , ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°”ê¿€ ì¤€ë¹„ë¥¼ í•©ë‹ˆë‹¤.\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "# íŒŒì„œê°€ ëª¨ë¸ì—ê²Œ ì „ë‹¬í•  'ë‹µë³€ í˜•ì‹ ì§€ì¹¨'ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "# (ì˜ˆ: \"Your response should be a list of comma separated values...\")\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\"\"\"\n",
        "Your response should be a list of comma separated values, eg: `foo, bar, baz`\n",
        "\"\"\"\n",
        "\n",
        "# --- 2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„± ---\n",
        "# {subject}: ì£¼ì œê°€ ë“¤ì–´ê°ˆ ìë¦¬\n",
        "# {format_instructions}: ìœ„ì—ì„œ ë§Œë“  ë‹µë³€ í˜•ì‹ ì§€ì¹¨ì´ ë“¤ì–´ê°ˆ ìë¦¬\n",
        "# check what to expect to LLM\n",
        "prompt = PromptTemplate(\n",
        "    template=\"List five {subject}.\\n{format_instructions}\",\n",
        "    input_variables=[\"subject\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# --- 3. ëª¨ë¸ ì„¤ì • ---\n",
        "llm_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"text-generation\")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# --- 4. ì‹¤í–‰ ë° ê²°ê³¼ ì²˜ë¦¬ ---\n",
        "# ì£¼ì œë¥¼ \"ice cream flavors\"ë¡œ ì„¤ì •í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "_input = prompt.format(subject=\"ice cream flavors\")\n",
        "output = model.invoke(_input)\n",
        "\n",
        "print(output_parser.parse(output.content))\n",
        "# ëª¨ë¸ ë²„ì „ì´ ë°”ë€ í›„ë¡œëŠ”\n",
        "# ['1. Chocolate\\n2. Vanilla\\n3. Strawberry\\n4. Mint chocolate chip\\n5. Cookies and cream']\n",
        "# ìœ¼ë¡œ ì•„ì›ƒí’‹ì´ ë©ë‹ˆë‹¤."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLYk3DP3cEIB",
        "outputId": "89506538-3af1-41f8-ab37-ac9286cc3f23"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Vanilla', 'Chocolate', 'Strawberry', 'Mint Chocolate Chip', 'Cookies and Cream']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## enum_parser"
      ],
      "metadata": {
        "id": "VCK_XoJLmONH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers.enum import EnumOutputParser\n",
        "from enum import Enum\n",
        "\n",
        "# 1. í—ˆìš© ê°€ëŠ¥í•œ ê°’ë“¤ì˜ ì§‘í•©(Enum)ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "# ëª¨ë¸ì€ ë°˜ë“œì‹œ ì´ ì„¸ ê°€ì§€ ìƒ‰ìƒ ì¤‘ í•˜ë‚˜ë§Œ ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "class Colors(Enum):\n",
        "    RED = \"red\"\n",
        "    GREEN = \"green\"\n",
        "    BLUE = \"blue\"\n",
        "\n",
        "# 2. Enum íŒŒì„œë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. ê¸°ì¤€ì´ ë  Enum í´ë˜ìŠ¤ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
        "parser = EnumOutputParser(enum=Colors)\n",
        "\n",
        "# --- ì •ìƒ ì‘ë™ ì¼€ì´ìŠ¤ ---\n",
        "\n",
        "# ë¬¸ìì—´ \"red\"ë¥¼ ì…ë ¥ë°›ì•„ Colors.REDë¼ëŠ” Enum ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "# ê²°ê³¼: <Colors.RED: 'red'>\n",
        "# # Colors.RED\n",
        "parser.parse(\"red\")\n",
        "\n",
        "# ì•ë’¤ ê³µë°±ì´ ìˆì–´ë„ íŒŒì„œê°€ ë‚´ë¶€ì ìœ¼ë¡œ .strip()ì„ í˜¸ì¶œí•´ ê¹”ë”í•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "# ê²°ê³¼: <Colors.GREEN: 'green'>\n",
        "# # Can handle spaces\n",
        "# parser.parse(\" green\")\n",
        "\n",
        "# ì¤„ë°”ê¿ˆ(\\n) ê°™ì€ íŠ¹ìˆ˜ ë¬¸ìê°€ í¬í•¨ë˜ì–´ë„ ë¬¸ì œì—†ì´ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "# ê²°ê³¼: <Colors.BLUE: 'blue'>\n",
        "# # And new lines\n",
        "# parser.parse(\"blue\\n\")\n",
        "\n",
        "# --- ì—ëŸ¬ ë°œìƒ ì¼€ì´ìŠ¤ ---\n",
        "\n",
        "# Enum(Colors)ì— ì •ì˜ë˜ì§€ ì•Šì€ \"yellow\"ë¥¼ ì…ë ¥í•˜ë©´ ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤.\n",
        "# # And raises errors when appropriate\n",
        "# parser.parse(\"yellow\")\n",
        "\n",
        "# \"\"\"\n",
        "# Traceback (most recent call last):\n",
        "# File \"/Users/seungjoonlee/git/learn-langchain/venv/lib/python3.10/site-packages/langchain/output_parsers/enum.py\", line 27, in parse\n",
        "# return self.enum(response.strip())\n",
        "# File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/enum.py\", line 385, in __call__\n",
        "# return cls.__new__(cls, value)\n",
        "# File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/enum.py\", line 710, in __new__\n",
        "# raise ve_exc\n",
        "# ValueError: 'yellow' is not a valid Colors\n",
        "\n",
        "# During handling of the above exception, another exception occurred:\n",
        "\n",
        "# Traceback (most recent call last):\n",
        "# File \"/Users/seungjoonlee/git/learn-langchain/output_parsers/enum_parser.py\", line 20, in <module>\n",
        "# parser.parse(\"yellow\")\n",
        "# File \"/Users/seungjoonlee/git/learn-langchain/venv/lib/python3.10/site-packages/langchain/output_parsers/enum.py\", line 29, in parse\n",
        "# raise OutputParserException(\n",
        "# langchain.schema.output_parser.OutputParserException: Response 'yellow' is not one of the expected values: ['red', 'green', 'blue']\n",
        "# \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kM7QFel1f_lp",
        "outputId": "dc722d9c-5fae-48c3-a075-44aa9c005ab8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Colors.BLUE: 'blue'>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## datetime_parser"
      ],
      "metadata": {
        "id": "IfYyf8T7ngv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ’¡ ì™œ ì´ ì½”ë“œê°€ ìœ ìš©í• ê¹Œìš”?\n",
        "\n",
        "ë°ì´í„° ê·œê²©í™”: LLMì€ ë³´í†µ \"ë¹„íŠ¸ì½”ì¸ì€ 2009ë…„ 1ì›”ì— ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤\"ë¼ê³  ë¬¸ì¥ìœ¼ë¡œ ë‹µí•˜ì§€ë§Œ, ì´ íŒŒì„œë¥¼ ì“°ë©´ 2009-01-03T18:15:05.000000Z ê°™ì€ ì •í•´ì§„ í˜•ì‹ë§Œ ë‚¨ê¹ë‹ˆë‹¤.\n",
        "\n",
        "í”„ë¡œê·¸ë˜ë° í™œìš©: íŒŒì‹±ëœ ê²°ê³¼ê°€ datetime ê°ì²´ì´ê¸° ë•Œë¬¸ì—, \"ì§€ê¸ˆìœ¼ë¡œë¶€í„° ë©°ì¹  ì „ì¸ê°€?\" ê°™ì€ ë‚ ì§œ ì—°ì‚°ì„ ë°”ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "GqTDSdbZzNn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from langchain.chains import LLMChain\n",
        "# from langchain_openai import OpenAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.output_parsers import DatetimeOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# 1. ë‚ ì§œ/ì‹œê°„ ì¶œë ¥ íŒŒì„œ ì„¤ì •\n",
        "# ëª¨ë¸ì˜ ë‹µë³€ì„ íŒŒì´ì¬ datetime ê°ì²´ë¡œ ë³€í™˜í•  ì¤€ë¹„ë¥¼ í•©ë‹ˆë‹¤.\n",
        "output_parser = DatetimeOutputParser()\n",
        "# print(output_parser.get_format_instructions())\n",
        "\"\"\"\n",
        "Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\n",
        "\n",
        "Examples: 0845-05-01T23:51:26.179657Z, 0953-10-03T20:38:54.550930Z, 0188-02-08T08:37:30.449473Z\n",
        "\n",
        "Return ONLY this string, no other words!\n",
        "\"\"\"\n",
        "\n",
        "# 2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±\n",
        "# {question}: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ë“¤ì–´ê°ˆ ìë¦¬\n",
        "# {format_instructions}: ìœ„ì—ì„œ ìƒì„±í•œ ë‚ ì§œ í˜•ì‹ ê·œì¹™ì´ ì‚½ì…ë  ìë¦¬\n",
        "template = \"\"\"Answer the users question:\n",
        "\n",
        "{question}\n",
        "\n",
        "{format_instructions}\"\"\"\n",
        "prompt = PromptTemplate.from_template(\n",
        "    template,\n",
        "    # partial_variablesë¥¼ í†µí•´ ì§€ì‹œì‚¬í•­ì„ í…œí”Œë¦¿ì— ë¯¸ë¦¬ ê³ ì •ì‹œì¼œë‘¡ë‹ˆë‹¤.\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "# 3. ëª¨ë¸ ì„¤ì •\n",
        "llm_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"text-generation\")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# 4. ì²´ì¸(Chain) ìƒì„± ë° ì‹¤í–‰\n",
        "# í”„ë¡¬í”„íŠ¸ì™€ ëª¨ë¸ì„ í•˜ë‚˜ë¡œ ë¬¶ì–´ ì‹¤í–‰ ê³¼ì •ì„ ìë™í™”í•©ë‹ˆë‹¤.\n",
        "chain = LLMChain(prompt=prompt, llm=model)\n",
        "output_dict = chain.invoke(\"around when was bitcoin founded?\")\n",
        "\n",
        "# 5. ê²°ê³¼ íŒŒì‹± (ë°ì´í„° ë³€í™˜)\n",
        "# [ì¤‘ìš”] ëª¨ë¸ì´ ë‚´ë±‰ì€ í…ìŠ¤íŠ¸(output_dict['text'])ë¥¼ íŒŒì´ì¬ datetime ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "# ë‚´ë¶€ì ìœ¼ë¡œ ìŠ¤íŠ¸ë¦½(strip) ì²˜ë¦¬ë¥¼ í•˜ì—¬ ì•ë’¤ ê³µë°±ì„ ì œê±°í•˜ê³  í˜•ì‹ì„ ë§ì¶¥ë‹ˆë‹¤.\n",
        "print(output_parser.parse(output_dict['text']))\n",
        "\"\"\"\n",
        "2009-01-03 18:15:05\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "ip5IZ7_jngNQ",
        "outputId": "1ec6e0a6-beeb-4860-d520-34f169896662"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2221207516.py:30: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(prompt=prompt, llm=model)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2009-01-03 00:00:00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n2009-01-03 18:15:05\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pydantic_parser_actor"
      ],
      "metadata": {
        "id": "BYQffK-xmFZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ’¡ ì´ ì½”ë“œì˜ í•µì‹¬ ì˜ë„\n",
        "\n",
        "ë°ì´í„° ê·œê²©í™”: LLMì€ ë³´í†µ \"í†° í–‰í¬ìŠ¤ì˜ ì˜í™”ëŠ” í¬ë ˆìŠ¤íŠ¸ ê²€í”„ ë“±ì´ ìˆìŠµë‹ˆë‹¤\"ë¼ê³  ë¬¸ì¥ìœ¼ë¡œ ë‹µí•˜ì§€ë§Œ, ì´ ì½”ë“œë¥¼ í†µê³¼í•˜ë©´ {\"name\": \"Tom Hanks\", \"film_names\": [...]} ë¼ëŠ” ì •ì œëœ ë°ì´í„°ë§Œ ë‚¨ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì•ˆì •ì„±: ë§Œì•½ LLMì´ film_namesì— ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ìˆ«ìë¥¼ ë„£ê±°ë‚˜ í•„ë“œëª…ì„ í‹€ë¦¬ë©´, Pydanticì´ ì´ë¥¼ ê°ì§€í•˜ì—¬ ì—ëŸ¬ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤. ì¦‰, í”„ë¡œê·¸ë¨ì´ ì˜ˆìƒì¹˜ ëª»í•œ ë°ì´í„° ë•Œë¬¸ì— í„°ì§€ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.\n",
        "\n",
        "ê°ì²´ ì§€í–¥ í™œìš©: íŒŒì‹± ê²°ê³¼ë¬¼(m)ì´ ê°ì²´ì´ë¯€ë¡œ m.name, m.film_names ì²˜ëŸ¼ ì (.) í‘œê¸°ë²•ì„ í†µí•´ ì•„ì£¼ ì‰½ê²Œ ë°ì´í„°ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "HMEeQJ9ozH9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from typing import List\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "# from langchain.pydantic_v1 import BaseModel, Field\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# model_name = \"gpt-3.5-turbo-instruct\"\n",
        "temperature = 0.0\n",
        "llm_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"text-generation\")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# --- 2. ë°ì´í„° êµ¬ì¡° ì •ì˜ (Pydantic ëª¨ë¸) ---\n",
        "# LLMì´ ë‚´ë†“ì•„ì•¼ í•  ê²°ê³¼ê°’ì˜ 'ê·œê²©'ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "# Here's another example, but with a compound typed field.\n",
        "class Actor(BaseModel):\n",
        "    name: str = Field(description=\"name of an actor\")\n",
        "    film_names: List[str] = Field(description=\"list of names of films they starred in\")\n",
        "\n",
        "\n",
        "# --- 3. ì¶œë ¥ íŒŒì„œ ë° í”„ë¡¬í”„íŠ¸ ì„¤ì • ---\n",
        "# Actor í´ë˜ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ JSON ì‘ë‹µì„ íŒŒì‹±í•  íŒŒì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "parser = PydanticOutputParser(pydantic_object=Actor)\n",
        "\n",
        "print(parser.get_format_instructions())\n",
        "\"\"\"\n",
        "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
        "\n",
        "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
        "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
        "\n",
        "Here is the output schema:\n",
        "```\n",
        "{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"name of an actor\", \"type\": \"string\"}, \"film_names\": {\"title\": \"Film Names\", \"description\": \"list of names of films they starred in\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"name\", \"film_names\"]}\n",
        "```\n",
        "\"\"\"\n",
        "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
        "# {format_instructions} ìë¦¬ì—ëŠ” íŒŒì„œê°€ ìƒì„±í•œ JSON ìŠ¤í‚¤ë§ˆ ê·œì¹™ì´ ìë™ìœ¼ë¡œ ì‚½ì…ë©ë‹ˆë‹¤.\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "# --- 4. ì‹¤í–‰ ë° ê²°ê³¼ ì²˜ë¦¬ ---\n",
        "# ì§ˆë¬¸ ë‚´ìš©ì„ ë‹´ì•„ ìµœì¢… í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "actor_query = \"Generate the filmography for a random actor.\"\n",
        "_input = prompt.format_prompt(query=actor_query)\n",
        "\n",
        "output = model.invoke(_input.to_string())\n",
        "\"\"\"\n",
        "Output:\n",
        "{\"name\": \"Tom Hanks\", \"film_names\": [\"Forrest Gump\", \"Saving Private Ryan\", \"Cast Away\", \"The Green Mile\", \"Apollo 13\", \"Toy Story\", \"Toy Story 2\", \"Toy Story 3\", \"The Da Vinci Code\", \"Catch Me If You Can\"]}\n",
        "name='Tom Hanks' film_names=['Forrest Gump', 'Saving Private Ryan', 'Cast Away', 'The Green Mile', 'Apollo 13', 'Toy Story', 'Toy Story 2', 'Toy Story 3', 'The Da Vinci Code', 'Catch Me If You Can']\n",
        "\"\"\"\n",
        "m = parser.parse(output.content)\n",
        "print(m)\n",
        "\"\"\"\n",
        "name='Tom Hanks' film_names=['Forrest Gump', 'Saving Private Ryan', 'The Green Mile', 'Cast Away', 'Toy Story']\n",
        "\"\"\"\n",
        "print(m.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAvIH5apidVB",
        "outputId": "ee392d99-830a-4a88-f326-3b432bc98331"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"name\": {\"description\": \"name of an actor\", \"title\": \"Name\", \"type\": \"string\"}, \"film_names\": {\"description\": \"list of names of films they starred in\", \"items\": {\"type\": \"string\"}, \"title\": \"Film Names\", \"type\": \"array\"}}, \"required\": [\"name\", \"film_names\"]}\n",
            "```\n",
            "name='Tom Hanks' film_names=['Forrest Gump', 'Saving Private Ryan', 'Cast Away', 'Apollo 13', 'The Green Mile', 'Captain Phillips']\n",
            "Tom Hanks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Actor Name: {m.name}\")\n",
        "print(f\"Films: {m.film_names}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NgnvllykF_B",
        "outputId": "caeb0a97-b9b1-4643-9475-6d01147f76cb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actor Name: Tom Hanks\n",
            "Films: ['Forrest Gump', 'Saving Private Ryan', 'Cast Away', 'Apollo 13', 'The Green Mile', 'Captain Phillips']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "# ìˆ˜ì • ì „: from langchain.pydantic_v1 import BaseModel, Field\n",
        "# ìˆ˜ì • í›„: ìµœì‹  pydanticì—ì„œ ì§ì ‘ ì„í¬íŠ¸\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# 1. Pydantic v2 ë°©ì‹ì˜ í´ë˜ìŠ¤ ì •ì˜\n",
        "class Actor(BaseModel):\n",
        "    name: str = Field(description=\"name of an actor\")\n",
        "    film_names: List[str] = Field(description=\"list of names of films they starred in\")\n",
        "\n",
        "# 2. íŒŒì„œ ì •ì˜\n",
        "parser = PydanticOutputParser(pydantic_object=Actor)\n",
        "\n",
        "# 3. ëª¨ë¸ ì„¤ì •\n",
        "llm_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"text-generation\")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# 4. í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "# 5. ì‹¤í–‰ ë° íŒŒì‹± (ì´ì „ ë‹µë³€ì²˜ëŸ¼ .contentë¥¼ ë¶™ì—¬ì•¼ í•¨)\n",
        "_input = prompt.format_prompt(query=\"Generate the filmography for a random actor.\")\n",
        "output = model.invoke(_input.to_string())\n",
        "\n",
        "# AIMessage ê°ì²´ì˜ contentë§Œ íŒŒì„œì— ì „ë‹¬\n",
        "m = parser.parse(output.content)\n",
        "\n",
        "print(f\"Actor Name: {m.name}\")\n",
        "print(f\"Films: {m.film_names}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQbvdJMtiqdW",
        "outputId": "af90e6ee-b550-4226-c2fd-c75c8937a29d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actor Name: Tom Hanks\n",
            "Films: ['Forrest Gump', 'Saving Private Ryan', 'Cast Away', 'Toy Story', 'The Terminal', 'The Green Mile', 'Apollo 13']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pydantic_parser_joke"
      ],
      "metadata": {
        "id": "EOoqdP-Ql-Tz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ’¡ ì´ ì½”ë“œì˜ í•µì‹¬ í¬ì¸íŠ¸\n",
        "\n",
        "ì§€ëŠ¥í˜• ë°ì´í„° ì¶”ì¶œ: LLMì—ê²Œ ë‹¨ìˆœíˆ ë†ë‹´ì„ í•´ë‹¬ë¼ê³  í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼, setupê³¼ punchlineì´ë¼ëŠ” ëª…í™•í•œ í•„ë“œì— ë§ì¶°ì„œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ë„ë¡ ê°•ì œí•©ë‹ˆë‹¤.\n",
        "\n",
        "ìë™ ê²€ì¦ (Validation): @field_validatorë¥¼ í†µí•´ LLMì´ ì¤€ ë‹µë³€ì´ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ì¡°ê±´(ì˜ˆ: ì§ˆë¬¸ì€ ?ë¡œ ëë‚˜ì•¼ í•¨)ì„ ë§Œì¡±í•˜ëŠ”ì§€ ì¦‰ì‹œ ì²´í¬í•©ë‹ˆë‹¤. ë§Œì•½ ì¡°ê±´ì„ ì–´ê¸°ë©´ íŒŒì´ì¬ ë ˆë²¨ì—ì„œ ì—ëŸ¬ë¥¼ ë˜ì ¸ ì˜ëª»ëœ ë°ì´í„°ê°€ ì‹œìŠ¤í…œì— ë“¤ì–´ì˜¤ëŠ” ê²ƒì„ ë§‰ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì½”ë“œ ì•ˆì •ì„±: m.setup ì²˜ëŸ¼ ê°ì²´ ì§€í–¥ ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ì— ì ‘ê·¼í•  ìˆ˜ ìˆì–´, ì˜¤íƒ€ë¡œ ì¸í•œ ë²„ê·¸ë¥¼ ì¤„ì´ê³  ìë™ ì™„ì„± ê¸°ëŠ¥ì„ í™œìš©í•˜ê¸° ì¢‹ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "7n4aXlrJzDu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from typing import List\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "# from langchain.pydantic_v1 import BaseModel, Field, validator\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "\n",
        "temperature = 0.0\n",
        "llm_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"text-generation\")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# --- 2. ë°ì´í„° êµ¬ì¡° ë° ê²€ì¦ ë¡œì§ ì •ì˜ ---\n",
        "# Define your desired data structure.\n",
        "class Joke(BaseModel):\n",
        "  # ë†ë‹´ì˜ ë„ì…ë¶€ (ì§ˆë¬¸ í˜•íƒœ)\n",
        "    setup: str = Field(description=\"question to set up a joke\")\n",
        "    # ë†ë‹´ì˜ í•µì‹¬ ë‹µë³€\n",
        "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
        "\n",
        "    # [ê²€ì¦ ë¡œì§] Pydanticì˜ field_validatorë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬\n",
        "    # You can add custom validation logic easily with Pydantic.\n",
        "    @field_validator(\"setup\")\n",
        "    def question_ends_with_question_mark(cls, field):\n",
        "      # ì§ˆë¬¸ì˜ ë§ˆì§€ë§‰ ê¸€ìê°€ ë¬¼ìŒí‘œ(?)ê°€ ì•„ë‹ˆë©´ ì—ëŸ¬ ë°œìƒ\n",
        "        if field[-1] != \"?\":\n",
        "            raise ValueError(\"Badly formed question!\")\n",
        "        return field\n",
        "\n",
        "\n",
        "\n",
        "# --- 3. ì¶œë ¥ íŒŒì„œ ë° í”„ë¡¬í”„íŠ¸ ì„¤ì • ---\n",
        "# Joke í´ë˜ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µì„ JSONìœ¼ë¡œ ë³€í™˜í•´ì¤„ íŒŒì„œ ìƒì„±\n",
        "# Set up a parser + inject instructions into the prompt template.\n",
        "parser = PydanticOutputParser(pydantic_object=Joke)\n",
        "\n",
        "# print(parser.get_format_instructions())\n",
        "\"\"\"\n",
        "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
        "\n",
        "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
        "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
        "\n",
        "Here is the output schema:\n",
        "```\n",
        "{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "# And a query intented to prompt a language model to populate the data structure.\n",
        "joke_query = \"Tell me a joke.\"\n",
        "_input = prompt.format_prompt(query=joke_query)\n",
        "\n",
        "output = model.invoke(_input.to_string())\n",
        "print(output.content)\n",
        "output = '{\"setup\": \"Why did the chicken cross the road?\", \"punchline\": \"To get to the other side!\"}'\n",
        "\"\"\"\n",
        "{\"setup\": \"Why did the chicken cross the road?\", \"punchline\": \"To get to the other side!\"}\n",
        "\"\"\"\n",
        "m = parser.parse(output)\n",
        "print(m)\n",
        "\"\"\"\n",
        "setup='Why did the chicken cross the road?' punchline='To get to the other side!\n",
        "\"\"\"\n",
        "print(m.setup)\n",
        "\"\"\"\n",
        "Why did the chicken cross the road?\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "5rJRE7pTjJ5o",
        "outputId": "bdbc5384-383a-4716-d61f-ff17e5748ca8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"setup\":\"Why don't scientists trust atoms?\",\"punchline\":\"Because they make up everything!\"}\n",
            "setup='Why did the chicken cross the road?' punchline='To get to the other side!'\n",
            "Why did the chicken cross the road?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWhy did the chicken cross the road?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## auto_fixing_parser"
      ],
      "metadata": {
        "id": "eikNa6CWl5v2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ’¡ ì´ ì½”ë“œì˜ ì‘ë™ ì›ë¦¬ (Workflow)\n",
        "\n",
        "ì‹¤íŒ¨ ê°ì§€: PydanticOutputParserê°€ ì…ë ¥ê°’ misformattedë¥¼ ì½ìœ¼ë ¤ë‹¤ JSONDecodeErrorë¥¼ ëƒ…ë‹ˆë‹¤.\n",
        "\n",
        "ë„ì›€ ìš”ì²­: OutputFixingParserê°€ ê²°í•©ëœ LLMì—ê²Œ \"ì›ë˜ ì˜ë„í•œ í¬ë§· ì§€ì¹¨\", \"ì—ëŸ¬ê°€ ë‚œ í…ìŠ¤íŠ¸\", **\"ë°œìƒí•œ ì—ëŸ¬ ë©”ì‹œì§€\"**ë¥¼ í•œêº¼ë²ˆì— ì „ë‹¬í•©ë‹ˆë‹¤.\n",
        "\n",
        "ìê°€ ìˆ˜ì •: LLMì€ ì—ëŸ¬ ë©”ì‹œì§€(Expecting property name enclosed in double quotes)ë¥¼ ë³´ê³  ì‘ì€ë”°ì˜´í‘œë¥¼ í°ë”°ì˜´í‘œë¡œ ë°”ê¾¼ ì˜¬ë°”ë¥¸ JSON ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "ìµœì¢… ë°˜í™˜: ìˆ˜ì •ëœ ë¬¸ìì—´ì„ ë‹¤ì‹œ íŒŒì´ì¬ ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "MAIhe1ZFy_ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "# from langchain.pydantic_v1 import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "# 1. ë°ì´í„° ê·œê²© ì •ì˜ (ë°°ìš°ì™€ ì˜í™” ëª©ë¡)\n",
        "class Actor(BaseModel):\n",
        "    name: str = Field(description=\"name of an actor\")\n",
        "    film_names: List[str] = Field(description=\"list of names of films they starred in\")\n",
        "\n",
        "actor_query = \"Generate the filmography for a random actor.\"\n",
        "\n",
        "# 2. ê¸°ë³¸ Pydantic íŒŒì„œ ì„¤ì •\n",
        "parser = PydanticOutputParser(pydantic_object=Actor)\n",
        "\n",
        "# 3. ì˜ëª»ëœ í˜•ì‹ì˜ ì‘ë‹µ ì˜ˆì‹œ (JSON í‘œì¤€ì¸ \" ëŒ€ì‹  ' ì‚¬ìš©)\n",
        "# íŒŒì´ì¬ì˜ json.loads()ëŠ” ì‘ì€ë”°ì˜´í‘œ(')ë¡œ ê°ì‹¸ì§„ í‚¤ ê°’ì„ ì¸ì‹í•˜ì§€ ëª»í•´ ì—ëŸ¬ê°€ ë‚©ë‹ˆë‹¤.\n",
        "# assuming we got the misformmated response i.e single quote instead of double quote\n",
        "misformatted = \"{'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}\"\n",
        "\n",
        "# 4. ì—ëŸ¬ ë°œìƒ ë° ìë™ ìˆ˜ì • í”„ë¡œì„¸ìŠ¤\n",
        "try:\n",
        "  # ì¼ë°˜ íŒŒì„œë¡œ íŒŒì‹± ì‹œë„ -> ì—¬ê¸°ì„œ JSONDecodeError ë°œìƒ\n",
        "    parser.parse(misformatted)\n",
        "    \"\"\"\n",
        "    Traceback (most recent call last):\n",
        "    File \"/Users/seungjoonlee/git/learn-langchain/venv/lib/python3.10/site-packages/langchain/output_parsers/pydantic.py\", line 27, in parse\n",
        "        json_object = json.loads(json_str, strict=False)\n",
        "    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py\", line 359, in loads\n",
        "        return cls(**kw).decode(s)\n",
        "    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py\", line 337, in decode\n",
        "        obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
        "    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
        "        obj, end = self.scan_once(s, idx)\n",
        "    json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
        "\n",
        "    During handling of the above exception, another exception occurred:\n",
        "\n",
        "    Traceback (most recent call last):\n",
        "    File \"/Users/seungjoonlee/git/learn-langchain/output_parsers/auto_fixing_parser.py\", line 15, in <module>\n",
        "        print(parser.parse(misformatted))\n",
        "    File \"/Users/seungjoonlee/git/learn-langchain/venv/lib/python3.10/site-packages/langchain/output_parsers/pydantic.py\", line 33, in parse\n",
        "        raise OutputParserException(msg, llm_output=text)\n",
        "    langchain.schema.output_parser.OutputParserException: Failed to parse Actor from completion {'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}. Got: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
        "    \"\"\"\n",
        "except Exception as e:\n",
        "    print(\"Got an error!!!!! trying with OutputFixingParser...\")\n",
        "    from langchain.output_parsers import OutputFixingParser\n",
        "\n",
        "    llm_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"text-generation\")\n",
        "    model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "    # [í•µì‹¬] OutputFixingParser ì •ì˜\n",
        "    # ê¸°ì¡´ íŒŒì„œ(parser)ì™€ ìˆ˜ì • ì‘ì—…ì„ ìˆ˜í–‰í•  ëª¨ë¸(model)ì„ ê²°í•©í•©ë‹ˆë‹¤.\n",
        "    # ì´ íŒŒì„œëŠ” ë‚´ë¶€ì ìœ¼ë¡œ \"ë‹¤ìŒ ì—ëŸ¬ë¥¼ ë³´ê³  í˜•ì‹ì„ ìˆ˜ì •í•´ì¤˜\"ë¼ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ LLMì—ê²Œ ë³´ëƒ…ë‹ˆë‹¤.\n",
        "    new_parser = OutputFixingParser.from_llm(parser=parser, llm=model)\n",
        "    print(new_parser.parse(misformatted))\n",
        "    \"\"\"\n",
        "    name='Tom Hanks' film_names=['Forrest Gump']\n",
        "    \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCOiZL_Hkjak",
        "outputId": "24e65ee2-b60f-4004-c4e5-85928c03b84f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got an error!!!!! trying with OutputFixingParser...\n",
            "name='Tom Hanks' film_names=['Forrest Gump']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## structured_output_parser"
      ],
      "metadata": {
        "id": "8-ttG1DVml6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ’¡ ì´ ì½”ë“œì˜ í•µì‹¬ ì˜ë„\n",
        "\n",
        "ë‹¤ì¤‘ ë°ì´í„° ì¶”ì¶œ: ë‹¨ìˆœíˆ ë‹µë³€ë§Œ ë°›ëŠ” ê²Œ ì•„ë‹ˆë¼, ê·¸ ë‹µë³€ì˜ **ê·¼ê±°ê°€ ë˜ëŠ” ì¶œì²˜(source)**ê¹Œì§€ í•¨ê»˜ ë¬¶ì–´ì„œ ë°›ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "í¬ë§· ì•ˆì •ì„±: format_instructionsë¥¼ í†µí•´ LLMì—ê²Œ json ë§ˆí¬ë‹¤ìš´ íƒœê·¸ë¥¼ ë¶™ì—¬ì„œ ë‹µí•˜ë¼ê³  ê°€ì´ë“œí•˜ë©°, íŒŒì„œëŠ” ê·¸ íƒœê·¸ ì•ˆì˜ ë‚´ìš©ë§Œ ì™ ê³¨ë¼ëƒ…ë‹ˆë‹¤.\n",
        "\n",
        "ìœ ì—°ì„±: PydanticOutputParserì²˜ëŸ¼ í´ë˜ìŠ¤ë¥¼ ì •ì˜í•  í•„ìš” ì—†ì´, ResponseSchema ë¦¬ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ë¹ ë¥´ê²Œ ë°ì´í„° êµ¬ì¡°ë¥¼ ì„¤ê³„í•  ìˆ˜ ìˆì–´ í¸ë¦¬í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "EDKzy_Fry3SV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "# from langchain.llms import OpenAI\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "response_schemas = [\n",
        "    ResponseSchema(name=\"answer\", description=\"answer to the user's question\"),\n",
        "    ResponseSchema(name=\"source\", description=\"source used to answer the user's question, should be a website.\")\n",
        "]\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "# The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
        "\"\"\"\n",
        "```json\n",
        "{\n",
        "    \"answer\": string  // answer to the user's question\n",
        "    \"source\": string  // source used to answer the user's question, should be a website.\n",
        "}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"answer the users question as best as possible.\\n{format_instructions}\\n{question}\",\n",
        "    input_variables=[\"question\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# model = OpenAI(temperature=0)\n",
        "llm_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"text-generation\")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "_input = prompt.format_prompt(question=\"what's the capital of france?\")\n",
        "# output = model(_input.to_string())\n",
        "output = model.invoke(_input.to_messages())\n",
        "\"\"\"\n",
        "```json\n",
        "{\n",
        "    \"answer\": \"Paris\",\n",
        "    \"source\": \"https://www.worldatlas.com/articles/what-is-the-capital-of-france.html\"\n",
        "}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "# chat_model = ChatOpenAI(temperature=0)\n",
        "llm_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"text-generation\")\n",
        "chat_model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        HumanMessagePromptTemplate.from_template(\"answer the users question as best as possible.\\n{format_instructions}\\n{question}\")\n",
        "    ],\n",
        "    input_variables=[\"question\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "_input = prompt.format_prompt(question=\"what's the capital of france?\")\n",
        "output = chat_model(_input.to_messages())\n",
        "output_parser.parse(output.content)\n",
        "# {'answer': 'The capital of France is Paris.', 'source': 'https://en.wikipedia.org/wiki/Paris'}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEo_MBvQlemP",
        "outputId": "ecead1a3-58d6-4379-e055-0e3b82cf99bd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'Paris', 'source': 'https://www.britannica.com/place/France'}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Serialization"
      ],
      "metadata": {
        "id": "LXF8oOtSzSMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## json"
      ],
      "metadata": {
        "id": "x3prc5LPzxpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All prompts are loaded through the `load_prompt` function.\n",
        "from langchain.prompts import load_prompt, PromptTemplate\n",
        "\n",
        "\n",
        "template = \"Tell me a {adjective} joke about {content}.\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"adjective\", \"content\"])\n",
        "# input_variables=['adjective', 'content'] template='Tell me a {adjective} joke about {content}.'\n",
        "\n",
        "# save the prompt in JSON\n",
        "prompt.save(\"simple_prompt.json\")\n",
        "\n",
        "# load the prompt from JSON file\n",
        "prompt = load_prompt(\"simple_prompt.json\")\n",
        "print(prompt.format(adjective=\"funny\", content=\"chickens\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYg4WurinE3B",
        "outputId": "4ebc9b55-a2be-406d-89d4-3cf475c2418c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell me a funny joke about chickens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## yaml"
      ],
      "metadata": {
        "id": "Za0B9X7mzyvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All prompts are loaded through the `load_prompt` function.\n",
        "from langchain.prompts import load_prompt, PromptTemplate\n",
        "\n",
        "\n",
        "template = \"Tell me a {adjective} joke about {content}.\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"adjective\", \"content\"])\n",
        "# input_variables=['adjective', 'content'] template='Tell me a {adjective} joke about {content}.'\n",
        "\n",
        "# save the prompt in YAML\n",
        "prompt.save(\"simple_prompt.yaml\")\n",
        "\n",
        "# load the prompt from YAML file\n",
        "prompt = load_prompt(\"simple_prompt.yaml\")\n",
        "print(prompt.format(adjective=\"funny\", content=\"chickens\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcmKGBoVzxTu",
        "outputId": "2b44ac63-d825-44a7-abd7-603292f54674"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell me a funny joke about chickens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mxoTmhqzz43j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}