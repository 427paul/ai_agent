{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMT5om0EbEKuJxcCb3HNZlt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/427paul/ai_agent/blob/main/ai_agent_03_LangChain_Expression_Language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyFfU5k10RDm"
      },
      "outputs": [],
      "source": [
        "!pip install -U \"langchain==0.3.*\" \"langchain-core==0.3.*\" \"langchain-community==0.3.*\" \"langgraph==0.3.*\" \"langchain-huggingface\" \"huggingface_hub\" \"sentence-transformers\" wikipedia -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4Z68J7Xf0a3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def load_api_keys(filepath=\"api_key.txt\"):\n",
        "    with open(filepath, \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line and \"=\" in line:\n",
        "                key, value = line.split(\"=\", 1)\n",
        "                os.environ[key.strip()] = value.strip()\n",
        "\n",
        "path = '/content/drive/MyDrive/LangGraph/'\n",
        "\n",
        "# API 키 로드 및 환경변수 설정\n",
        "load_api_keys(path + 'api_key.txt')"
      ],
      "metadata": {
        "id": "15yLmyOq0cAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prompt llm output parser"
      ],
      "metadata": {
        "id": "DMFPg6yA002E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
        "prompt_value = prompt.invoke({\"topic\": \"ice cream\"})\n",
        "\"\"\"\n",
        "messages=[HumanMessage(content='tell me a short joke about ice cream')]\n",
        "\"\"\"\n",
        "print(prompt_value.to_string())\n",
        "\"\"\"\n",
        "Human: tell me a short joke about ice cream\n",
        "\"\"\"\n",
        "\n",
        "model = ChatOpenAI()\n",
        "message = model.invoke(prompt_value)\n",
        "\"\"\"\n",
        "content='Why did the ice cream go to therapy? \\n\\nBecause it had too many scoops of emotions!'\n",
        "\"\"\"\n",
        "\n",
        "# from langchain.llms import OpenAI\n",
        "# llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
        "# llm_message = llm.invoke(prompt_value)\n",
        "\"\"\"\n",
        "Why did the ice cream go to therapy?\n",
        "\n",
        "Because it was having a meltdown!\n",
        "\"\"\"\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "print(output_parser.invoke(message))\n",
        "\"\"\"\n",
        "Why did the ice cream go to therapy?\n",
        "\n",
        "Because it had a meltdown!\n",
        "\"\"\"\n",
        "\n",
        "# similar to unix pipe operator\n",
        "chain = prompt | model | output_parser\n",
        "\n",
        "result = chain.invoke({\"topic\": \"ice cream\"})\n",
        "print(result)\n",
        "\"\"\"\n",
        "Why did the ice cream go to therapy?\n",
        "\n",
        "Because it had too many sprinkles of anxiety!\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "rM6ogqEM0ij_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# 1. 프롬프트 템플릿 정의\n",
        "# {topic} 자리에 사용자가 원하는 주제를 넣을 수 있는 대화형 프롬프트입니다.\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
        "\n",
        "# 프롬프트 단독 실행 테스트 (입력값을 넣어 메시지 객체로 변환)\n",
        "prompt_value = prompt.invoke({\"topic\": \"ice cream\"})\n",
        "\"\"\"\n",
        "messages=[HumanMessage(content='tell me a short joke about ice cream')]\n",
        "\"\"\"\n",
        "print(prompt_value.to_string())\n",
        "\"\"\"\n",
        "Human: tell me a short joke about ice cream\n",
        "\"\"\"\n",
        "\n",
        "# 2. 모델 설정 (HuggingFace 엔드포인트 사용)\n",
        "# gpt-oss-20b 모델을 호출하는 엔드포인트를 생성합니다.\n",
        "llm_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"text-generation\")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# 모델 단독 실행 테스트 (프롬프트 결과값을 모델에 전달)\n",
        "# 결과는 AIMessage 객체 형태로 반환됩니다. (내용은 content 속성에 담김)\n",
        "message = model.invoke(prompt_value)\n",
        "\"\"\"\n",
        "content='Why did the ice cream go to therapy? \\n\\nBecause it had too many scoops of emotions!'\n",
        "\"\"\"\n",
        "\n",
        "# from langchain.llms import OpenAI\n",
        "# llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
        "# llm_message = llm.invoke(prompt_value)\n",
        "\"\"\"\n",
        "Why did the ice cream go to therapy?\n",
        "\n",
        "Because it was having a meltdown!\n",
        "\"\"\"\n",
        "\n",
        "# 3. 출력 파서 설정\n",
        "# 모델의 응답(객체)에서 텍스트(string)만 깔끔하게 추출해주는 파서입니다.\n",
        "output_parser = StrOutputParser()\n",
        "# 파서 단독 실행 테스트\n",
        "print(output_parser.invoke(message))\n",
        "\"\"\"\n",
        "Why did the ice cream go to therapy?\n",
        "\n",
        "Because it had a meltdown!\n",
        "\"\"\"\n",
        "\n",
        "# --- 4. LCEL(LangChain Expression Language) 체인 생성 ---\n",
        "# | 연산자를 사용하여 각 단계를 하나로 묶습니다. (Unix 파이프와 동일한 원리)\n",
        "# 데이터 흐름: 입력(dict) -> 프롬프트 -> 모델 -> 출력 파서 -> 최종 문자열\n",
        "# similar to unix pipe operator\n",
        "chain = prompt | model | output_parser\n",
        "\n",
        "# 5. 체인 실행 (전체 과정을 한 번에 수행)\n",
        "result = chain.invoke({\"topic\": \"ice cream\"})\n",
        "print(result)\n",
        "\"\"\"\n",
        "Why did the ice cream go to therapy?\n",
        "\n",
        "Because it had too many sprinkles of anxiety!\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "-c5ogi2i2BPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prompt llm function call"
      ],
      "metadata": {
        "id": "CMbGuJY7-tqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.output_parsers.openai_functions import (\n",
        "    JsonOutputFunctionsParser,\n",
        "    JsonKeyOutputFunctionsParser)\n",
        "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
        "model = ChatOpenAI()\n",
        "\n",
        "# function call\n",
        "functions = [\n",
        "    {\n",
        "        \"name\": \"joke\",\n",
        "        \"description\": \"A joke\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"setup\": {\"type\": \"string\", \"description\": \"The setup for the joke\"},\n",
        "                \"punchline\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The punchline for the joke\",\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"setup\", \"punchline\"],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "chain = prompt | model.bind(function_call={\"name\": \"joke\"}, functions=functions)\n",
        "# print(chain.invoke({\"topic\": \"ice cream\"}, config={}))\n",
        "\"\"\"\n",
        "content='' additional_kwargs={'function_call': {'arguments': '{\\n  \"setup\": \"Why did the ice cream go to therapy?\",\\n  \"punchline\": \"Because it was feeling a little melty!\"\\n}', 'name': 'joke'}}\n",
        "\"\"\"\n",
        "\n",
        "# chain = prompt | model.bind(function_call={\"name\": \"joke\"}, functions=functions) | JsonOutputFunctionsParser()\n",
        "# print(chain.invoke({\"topic\": \"ice cream\"}))\n",
        "\"\"\"\n",
        "{'setup': 'Why did the ice cream go to therapy?', 'punchline': 'Because it had too many sprinkles of anxiety!'}\n",
        "\"\"\"\n",
        "\n",
        "# chain = prompt | model.bind(function_call={\"name\": \"joke\"}, functions=functions) | JsonKeyOutputFunctionsParser(key_name=\"setup\")\n",
        "# print(chain.invoke({\"topic\": \"ice cream\"}))\n",
        "\"\"\"\n",
        "Why did the ice cream go to therapy?\n",
        "\"\"\"\n",
        "\n",
        "map_ = RunnableParallel(topic=RunnablePassthrough())\n",
        "chain = (\n",
        "    map_\n",
        "    | prompt\n",
        "    | model.bind(function_call={\"name\": \"joke\"}, functions=functions)\n",
        "    | JsonKeyOutputFunctionsParser(key_name=\"setup\")\n",
        ")\n",
        "print(chain.invoke(\"ice cream\"))\n",
        "\"\"\"\n",
        "Why did the ice cream go to therapy?\n",
        "\"\"\"\n",
        "\n",
        "chain = (\n",
        "    {\"topic\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model.bind(function_call={\"name\": \"joke\"}, functions=functions)\n",
        "    | JsonKeyOutputFunctionsParser(key_name=\"setup\")\n",
        ")\n",
        "print(chain.invoke(\"ice cream\"))\n",
        "\"\"\"\n",
        "Why did the ice cream break up with the cone?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "iFeLt5Ku2by7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-core"
      ],
      "metadata": {
        "id": "W8_yisuuADZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# 1. HF_TOKEN 설정 (선택 사항이지만 권장)\n",
        "# Colab 왼쪽 열쇠 아이콘(Secrets)에서 HF_TOKEN을 추가하거나 직접 입력하세요.\n",
        "# os.environ[\"HF_TOKEN\"] = \"your_huggingface_token_here\"\n",
        "\n",
        "# 2. 모델 설정 (대화형 태스크에 최적화된 Llama-3.2 사용)\n",
        "repo_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    task=\"text-generation\", # 내부 엔진용\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "# Chat 모델로 래핑 (conversational 태스크 대응)\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "# 3. JSON 형식을 강력하게 요구하는 프롬프트\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are a joke generator.\n",
        "    Respond ONLY with a JSON object containing 'setup' and 'punchline' keys.\n",
        "    Do not include any other text or explanation.\n",
        "\n",
        "    Topic: {topic}\"\"\"\n",
        ")\n",
        "\n",
        "# 4. 체인 구성\n",
        "chain = (\n",
        "    {\"topic\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | JsonOutputParser()\n",
        ")\n",
        "\n",
        "# 5. 실행\n",
        "try:\n",
        "    print(\"--- 결과 확인 ---\")\n",
        "    result = chain.invoke(\"ice cream\")\n",
        "    print(f\"성공! 결과: {result}\")\n",
        "    print(f\"Setup: {result.get('setup')}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"에러 발생: {e}\")\n",
        "    # 실패 시 모델이 보낸 생문자열 확인 (디버깅용)\n",
        "    debug_chain = {\"topic\": RunnablePassthrough()} | prompt | model\n",
        "    debug_res = debug_chain.invoke(\"ice cream\")\n",
        "    print(f\"모델 원본 응답: {debug_res.content}\")"
      ],
      "metadata": {
        "id": "sIOgD3aH-zv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval-augmented generation(RAG)"
      ],
      "metadata": {
        "id": "EaDmzBx3EJh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from operator import itemgetter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
        "from langchain_community.vectorstores.faiss import FAISS\n",
        "\n",
        "# It requires `pip install langchain openai faiss-cpu tiktoken`\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_texts(\n",
        "    [\"harrison worked at kensho\"], embedding=embedding_model\n",
        ")\n",
        "# save\n",
        "vectorstore.save_local(\"faiss_index\")\n",
        "\n",
        "vectorstore_new = FAISS.load_local(\"faiss_index\", embedding_model)\n",
        "\n",
        "retriever = vectorstore_new.as_retriever()\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(chain.invoke(\"where did harrison work?\"))\n",
        "\"\"\"\n",
        "Harrison worked at Kensho.\n",
        "\"\"\"\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer in the following language: {language}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "        \"language\": itemgetter(\"language\"),\n",
        "    }\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(chain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"}))\n",
        "\"\"\"\n",
        "Harrison ha lavorato a Kensho.\n",
        "\"\"\"\n",
        "\n",
        "# # itemgetter example\n",
        "# from operator import itemgetter\n",
        "# # Suppose we have a dictionary\n",
        "# person = {'name': 'Alice', 'age': 30, 'job': 'Engineer'}\n",
        "# # We can use itemgetter to create a function that fetches the 'name' from a dictionary\n",
        "# get_name = itemgetter('name')\n",
        "# # Now, when we use this function with our dictionary\n",
        "# name = get_name(person)\n",
        "# print(name)  # Output: Alice"
      ],
      "metadata": {
        "id": "MPiwOSX9-8lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "id": "ZyBUszOVElNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from operator import itemgetter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# 1. 임베딩 모델 설정 (로컬 HuggingFace 모델)\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# 2. 벡터스토어 생성 및 로드 (FAISS)\n",
        "# 주의: allow_dangerous_deserialization=True는 로컬에서 직접 만든 인덱스를 로드할 때 필요합니다.\n",
        "vectorstore = FAISS.from_texts(\n",
        "    [\"harrison worked at kensho\"], embedding=embeddings\n",
        ")\n",
        "vectorstore.save_local(\"faiss_index\")\n",
        "\n",
        "vectorstore_new = FAISS.load_local(\n",
        "    \"faiss_index\", embeddings, allow_dangerous_deserialization=True\n",
        ")\n",
        "retriever = vectorstore_new.as_retriever()\n",
        "\n",
        "# 3. LLM 설정 (Hugging Face 오픈소스 모델)\n",
        "# llm_ep = HuggingFaceEndpoint(\n",
        "#     repo_id=\"meta-llama/Llama-3.2-3B-Instruct\", # 또는 \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "#     task=\"text-generation\",\n",
        "#     max_new_tokens=512,\n",
        "#     temperature=0.1\n",
        "# )\n",
        "llm_ep = HuggingFaceEndpoint(repo_id=\"openai/gpt-oss-20b\", task=\"text-generation\")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# --- 시나리오 1: 단순 질문 전달 ---\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"--- Scenario 1 Result ---\")\n",
        "print(chain.invoke(\"where did harrison work?\"))\n",
        "\n",
        "\n",
        "# --- 시나리오 2: itemgetter를 이용한 다중 파라미터 전달 ---\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer in the following language: {language}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# [핵심] 입력 데이터 딕셔너리에서 원하는 키값만 뽑아서 각 컴포넌트에 전달합니다.\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | retriever, # 질문만 뽑아서 리트리버에 전달\n",
        "        \"question\": itemgetter(\"question\"),           # 질문만 뽑아서 프롬프트에 전달\n",
        "        \"language\": itemgetter(\"language\"),           # 언어 설정만 뽑아서 프롬프트에 전달\n",
        "    }\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"\\n--- Scenario 2 Result (Italian) ---\")\n",
        "print(chain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"}))"
      ],
      "metadata": {
        "id": "BholklytEN89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runnable Protocol"
      ],
      "metadata": {
        "id": "vK1kWVpJFvrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Runnable Lambda"
      ],
      "metadata": {
        "id": "anUPJhMeKSc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from operator import itemgetter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "\n",
        "def length_function(text):\n",
        "    return len(text)\n",
        "\n",
        "def _multiple_length_function(text1, text2):\n",
        "    return len(text1) * len(text2)\n",
        "\n",
        "def multiple_length_function(_dict):\n",
        "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
        "model = ChatOpenAI()\n",
        "chain = (\n",
        "    {\n",
        "        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),\n",
        "        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}\n",
        "        | RunnableLambda(multiple_length_function),\n",
        "    }\n",
        "    | prompt\n",
        "    | model\n",
        ")\n",
        "print(chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"}))\n",
        "\"\"\"\n",
        "content='3 + 9 = 12'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cYOjZ47UEUui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from operator import itemgetter\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 1. 사용자 정의 함수 정의\n",
        "# 각 함수는 입력값을 받아 특정 연산을 수행합니다.\n",
        "def length_function(text):\n",
        "    return len(text)\n",
        "\n",
        "def _multiple_length_function(text1, text2):\n",
        "    return len(text1) * len(text2)\n",
        "\n",
        "def multiple_length_function(_dict):\n",
        "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
        "\n",
        "# 2. 모델 설정 (Hugging Face 오픈소스 모델)\n",
        "# Llama-3 또는 Qwen2.5 같은 고성능 모델을 사용하면 계산 결과를 더 잘 설명합니다.\n",
        "llm_ep = HuggingFaceEndpoint(\n",
        "    repo_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.1\n",
        ")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# 3. 프롬프트 정의\n",
        "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
        "\n",
        "# 4. 체인 구성 (LCEL)\n",
        "# RunnableLambda를 사용해 일반 파이썬 함수를 체인의 구성 요소로 변환합니다.\n",
        "chain = (\n",
        "    {\n",
        "        # \"foo\"의 길이를 계산하여 'a'에 할당\n",
        "        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),\n",
        "\n",
        "        # \"foo\"와 \"bar\"의 길이를 곱하여 'b'에 할당\n",
        "        \"b\": (\n",
        "            {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}\n",
        "            | RunnableLambda(multiple_length_function)\n",
        "        ),\n",
        "    }\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser() # 깔끔한 텍스트 출력을 위해 추가\n",
        ")\n",
        "\n",
        "# 5. 실행\n",
        "# foo(3글자), bar(3글자) -> a=3, b=3*3=9 -> \"what is 3 + 9\"\n",
        "print(\"--- 연산 결과 ---\")\n",
        "print(chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"}))"
      ],
      "metadata": {
        "id": "Sc7tkMUvKfZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Runnable Branch"
      ],
      "metadata": {
        "id": "tTPyKTc8KVhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"Given the user question below, classify it as either being about `Strawberry`, `Banana`, or `Other`.\n",
        "\n",
        "Do not respond with more than one word.\n",
        "\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\n",
        "Classification:\"\"\"\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(chain.invoke({\"question\": \"What is the fruit that has red color?\"}))\n",
        "\"\"\"\n",
        "Strawberry\n",
        "\"\"\"\n",
        "\n",
        "strawberry_chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"You are an expert about strawberry. \\\n",
        "Always answer questions starting with \"As a Strawberry expert ... \". \\\n",
        "Respond to the following question:\n",
        "\n",
        "# Question: {question}\n",
        "# Answer:\"\"\"\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        ")\n",
        "\n",
        "banana_chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"You are an expert about banana. \\\n",
        "Always answer questions starting with \"As a Banana expert ... \". \\\n",
        "Respond to the following question:\n",
        "\n",
        "# Question: {question}\n",
        "# Answer:\"\"\"\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        ")\n",
        "\n",
        "general_chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"Respond to the following question:\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        ")\n",
        "\n",
        "\n",
        "from langchain.schema.runnable import RunnableBranch\n",
        "\n",
        "# the first element is a condition (a lambda function) and\n",
        "# the second element is the chain to execute if the condition is true\n",
        "branch = RunnableBranch(\n",
        "    (lambda x: \"strawberry\" in x[\"topic\"].lower(), strawberry_chain), # type: ignore\n",
        "    (lambda x: \"banana\" in x[\"topic\"].lower(), banana_chain), # type: ignore\n",
        "    general_chain,\n",
        ")\n",
        "\n",
        "# chain is invoked to classify the question, and its output is stored under the key topic.\n",
        "# The original question is passed through unchanged under the key question.\n",
        "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | branch\n",
        "\n",
        "print(full_chain.invoke({\"question\": \"What is the fruit that has red color?\"}))\n",
        "print(full_chain.invoke({\"question\": \"What is the fruit that has yellow color?\"}))\n",
        "print(full_chain.invoke({\"question\": \"What is the fruit that has green color?\"}))\n",
        "\"\"\"\n",
        "content='As a Strawberry expert, I can tell you that strawberries are the fruit that has a vibrant red color.'\n",
        "content='As a Banana expert, the fruit that has a yellow color is the banana.'\n",
        "content='The fruit that has a green color is typically an apple or a lime.'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UK6NVQORKZPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_core.runnables import RunnableBranch\n",
        "\n",
        "# 1. LLM 설정 (Hugging Face 오픈소스 모델)\n",
        "# 분류 및 전문 답변 능력이 좋은 모델을 사용합니다.\n",
        "llm_ep = HuggingFaceEndpoint(\n",
        "    repo_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.1\n",
        ")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# 2. 질문 분류 체인 (Classifier)\n",
        "# 입력된 질문을 Strawberry, Banana, Other 중 하나로 분류합니다.\n",
        "classification_chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"Given the user question below, classify it as either being about `Strawberry`, `Banana`, or `Other`.\n",
        "Do not respond with more than one word.\n",
        "\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\n",
        "Classification:\"\"\"\n",
        "    )\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 3. 각 주제별 전문 체인 (Sub-chains)\n",
        "strawberry_chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"You are an expert about strawberry. Always answer questions starting with \"As a Strawberry expert ... \".\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "    )\n",
        "    | model\n",
        ")\n",
        "\n",
        "banana_chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"You are an expert about banana. Always answer questions starting with \"As a Banana expert ... \".\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "    )\n",
        "    | model\n",
        ")\n",
        "\n",
        "general_chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"Respond to the following question:\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "    )\n",
        "    | model\n",
        ")\n",
        "\n",
        "# 4. RunnableBranch 설정 (분기 로직)\n",
        "# \"topic\" 결과값에 따라 어느 체인으로 갈지 결정합니다.\n",
        "branch = RunnableBranch(\n",
        "    (lambda x: \"strawberry\" in x[\"topic\"].lower(), strawberry_chain),\n",
        "    (lambda x: \"banana\" in x[\"topic\"].lower(), banana_chain),\n",
        "    general_chain,\n",
        ")\n",
        "\n",
        "# 5. 전체 체인 구성 (Full Chain)\n",
        "# 먼저 분류(topic)를 수행하고, 원본 질문(question)과 함께 branch로 넘깁니다.\n",
        "full_chain = {\"topic\": classification_chain, \"question\": lambda x: x[\"question\"]} | branch\n",
        "\n",
        "# 6. 실행 및 테스트\n",
        "print(\"--- 결과 확인 ---\")\n",
        "res1 = full_chain.invoke({\"question\": \"What is the fruit that has red color?\"})\n",
        "print(f\"Red: {res1.content}\")\n",
        "\n",
        "res2 = full_chain.invoke({\"question\": \"What is the fruit that has yellow color?\"})\n",
        "print(f\"Yellow: {res2.content}\")\n",
        "\n",
        "res3 = full_chain.invoke({\"question\": \"What is the fruit that has green color?\"})\n",
        "print(f\"Green: {res3.content}\")"
      ],
      "metadata": {
        "id": "dAr3CMXpLByU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Runnable Parallel"
      ],
      "metadata": {
        "id": "5I0qMT7dKZwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from operator import itemgetter\n",
        "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "runnable = RunnableParallel(\n",
        "    passed=RunnablePassthrough()\n",
        ")\n",
        "\n",
        "print(runnable.invoke({\"num\": 1}))\n",
        "\"\"\"\n",
        "{'passed': {'num': 1}}\n",
        "\"\"\"\n",
        "\n",
        "runnable = RunnableParallel(\n",
        "    passed=RunnablePassthrough(),\n",
        "    extra=RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),\n",
        "    modified=lambda x: x[\"num\"] + 1,\n",
        ")\n",
        "\n",
        "print(runnable.invoke({\"num\": 1}))\n",
        "\"\"\"\n",
        "{'passed': {'num': 1}, 'extra': {'num': 1, 'mult': 3}, 'modified': 2}\n",
        "\"\"\"\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnableParallel\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n",
        "poem_chain = (\n",
        "    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model\n",
        ")\n",
        "\n",
        "# easy to execute multiple Runnables in parallel\n",
        "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
        "\n",
        "print(map_chain.invoke({\"topic\": \"bear\"}))\n",
        "\"\"\"\n",
        "{\n",
        "    'joke': AIMessage(content=\"Why did the bear break up with his girlfriend? \\n\\nBecause he couldn't bear the relationship any longer!\"),\n",
        "    'poem': AIMessage(content='In the dark woods, a bear roams free,\\nA majestic creature, wild and full of mystery.')\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LEcp_RWoKeTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# --- 1. 데이터 가공 기본 (RunnablePassthrough & assign) ---\n",
        "# 이 부분은 모델과 상관없이 LangChain의 핵심 로직이므로 동일하게 작동합니다.\n",
        "\n",
        "# 입력받은 값을 그대로 'passed' 키에 담습니다.\n",
        "runnable_basic = RunnableParallel(\n",
        "    passed=RunnablePassthrough()\n",
        ")\n",
        "print(\"--- Basic Invoke ---\")\n",
        "print(runnable_basic.invoke({\"num\": 1})) # {'passed': {'num': 1}}\n",
        "\n",
        "# assign을 사용해 기존 데이터를 유지하면서 새로운 값을 추가합니다.\n",
        "runnable_complex = RunnableParallel(\n",
        "    passed=RunnablePassthrough(),\n",
        "    extra=RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),\n",
        "    modified=lambda x: x[\"num\"] + 1,\n",
        ")\n",
        "print(\"\\n--- Complex Invoke ---\")\n",
        "print(runnable_complex.invoke({\"num\": 1}))\n",
        "# {'passed': {'num': 1}, 'extra': {'num': 1, 'mult': 3}, 'modified': 2}\n",
        "\n",
        "\n",
        "# --- 2. 병렬 체인 실행 (Hugging Face 모델 연동) ---\n",
        "\n",
        "# 모델 설정 (Llama-3.2 사용)\n",
        "llm_ep = HuggingFaceEndpoint(\n",
        "    repo_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.7\n",
        ")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# 농담 체인과 시(Poem) 체인을 각각 정의합니다.\n",
        "joke_chain = (\n",
        "    ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
        "    | model\n",
        "    | StrOutputParser() # 텍스트만 깔끔하게 추출\n",
        ")\n",
        "poem_chain = (\n",
        "    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\")\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# [핵심] RunnableParallel을 사용하여 두 체인을 묶습니다.\n",
        "# invoke 하나로 joke와 poem이 동시에 생성됩니다.\n",
        "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
        "\n",
        "print(\"\\n--- Map Chain Invoke (Parallel) ---\")\n",
        "result = map_chain.invoke({\"topic\": \"bear\"})\n",
        "print(f\"Joke: {result['joke']}\")\n",
        "print(f\"Poem: {result['poem']}\")"
      ],
      "metadata": {
        "id": "0OCWSTxqLI_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple Chains"
      ],
      "metadata": {
        "id": "tbeyoOcmMTro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from operator import itemgetter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "# Example1\n",
        "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
        "prompt2 = ChatPromptTemplate.from_template(\n",
        "    \"what country is the city {city} in? respond in {language}\"\n",
        ")\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "chain1 = prompt1 | model | StrOutputParser()\n",
        "\n",
        "chain2 = (\n",
        "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
        "    | prompt2\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(chain2.invoke({\"person\": \"obama\", \"language\": \"english\"}))\n",
        "\"\"\"\n",
        "Barack Obama, the 44th President of the United States, was born in Honolulu, Hawaii, which is located in the United States of America.\n",
        "\"\"\"\n",
        "\n",
        "# Example2\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# Generates a prompt asking for a color based on a given attribute.\n",
        "prompt1 = ChatPromptTemplate.from_template(\n",
        "    \"generate a {attribute} color. Return the name of the color and nothing else:\"\n",
        ")\n",
        "\n",
        "# Asks for a fruit of a specified color.\n",
        "prompt2 = ChatPromptTemplate.from_template(\n",
        "    \"what is a fruit of color: {color}. Return the name of the fruit and nothing else:\"\n",
        ")\n",
        "\n",
        "# Requests the name of a country with a flag containing a certain color.\n",
        "prompt3 = ChatPromptTemplate.from_template(\n",
        "    \"what is a country with a flag that has the color: {color}. Return the name of the country and nothing else:\"\n",
        ")\n",
        "\n",
        "# Forms a prompt asking for the color of a specific fruit and the flag of a specific country\n",
        "prompt4 = ChatPromptTemplate.from_template(\n",
        "    \"What is the color of {fruit} and the flag of {country}?\"\n",
        ")\n",
        "\n",
        "# Extract model message\n",
        "model_parser = model | StrOutputParser()\n",
        "\n",
        "# Generating Color\n",
        "color_generator = (\n",
        "    {\"attribute\": RunnablePassthrough()} | prompt1 | {\"color\": model_parser}\n",
        ")\n",
        "\n",
        "# Takes a color and uses prompt2 to ask for a corresponding fruit\n",
        "color_to_fruit = prompt2 | model_parser\n",
        "\n",
        "# uses prompt3 to find a country with a flag containing that color\n",
        "color_to_country = prompt3 | model_parser\n",
        "\n",
        "\n",
        "question_generator = (\n",
        "    color_generator | {\"fruit\": color_to_fruit, \"country\": color_to_country} | prompt4\n",
        ")\n",
        "\n",
        "prompt = question_generator.invoke(\"warm\")\n",
        "print(prompt)\n",
        "\"\"\"\n",
        "messages=[HumanMessage(content='What is the color of Coral. and the flag of Comoros?')]\n",
        "\"\"\"\n",
        "\n",
        "print(model.invoke(prompt))\n",
        "\"\"\"\n",
        "content='The color of a pomegranate is typically a deep red or maroon. The flag of Armenia consists of three horizontal bands of equal width - the top band is red, the middle band is blue, and the bottom band is orange.'\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Example3\n",
        "# Branching and Merging\n",
        "\"\"\"\n",
        "     Input\n",
        "      / \\\n",
        "     /   \\\n",
        " Branch1 Branch2\n",
        "     \\   /\n",
        "      \\ /\n",
        "      Combine\n",
        "\"\"\"\n",
        "planner = (\n",
        "    ChatPromptTemplate.from_template(\"Generate an argument about: {input}\")\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        "    | {\"base_response\": RunnablePassthrough()}\n",
        ")\n",
        "\n",
        "arguments_for = (\n",
        "    ChatPromptTemplate.from_template(\n",
        "        \"List the pros or positive aspects of {base_response}\"\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        ")\n",
        "arguments_against = (\n",
        "    ChatPromptTemplate.from_template(\n",
        "        \"List the cons or negative aspects of {base_response}\"\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_responder = (\n",
        "    ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"ai\", \"{original_response}\"),\n",
        "            (\"human\", \"Pros:\\n{results_1}\\n\\nCons:\\n{results_2}\"),\n",
        "            (\"system\", \"Generate a final response given the critique\"),\n",
        "        ]\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain = (\n",
        "    planner\n",
        "    | {\n",
        "        \"results_1\": arguments_for,\n",
        "        \"results_2\": arguments_against,\n",
        "        \"original_response\": itemgetter(\"base_response\"),\n",
        "    }\n",
        "    | final_responder\n",
        ")\n",
        "\n",
        "print(chain.invoke({\"input\": \"scrum\"}))\n",
        "\"\"\"\n",
        "While Scrum has its pros and cons, it is important to recognize that no project management framework is a one-size-fits-all solution. The cons mentioned should be considered in the context of the specific project and organization.\n",
        "\n",
        "For example, while Scrum may have a lack of predictability, this can be mitigated by implementing effective estimation techniques and regularly reassessing and adjusting plans. Additionally, while Scrum relies on team communication, organizations can invest in improving communication practices and tools to address any gaps or issues.\n",
        "\n",
        "Similarly, while Scrum may have limitations for large projects, organizations can adapt Scrum by implementing scaled agile frameworks like SAFe or LeSS to address complexities and dependencies.\n",
        "\n",
        "Furthermore, while Scrum may prioritize working software over comprehensive documentation, it does not mean that documentation is disregarded entirely. Organizations can establish guidelines and processes to ensure that essential documentation is maintained alongside the iterative development.\n",
        "\n",
        "Regarding role clarity, organizations can establish clear role definitions and ensure that team members understand their responsibilities and accountabilities. This can be achieved through effective communication and regular feedback.\n",
        "\n",
        "Lastly, while Scrum relies on experienced Scrum Masters, organizations can invest in training and development programs to enhance the skills and knowledge of Scrum Masters, ensuring effective facilitation of the Scrum process.\n",
        "\n",
        "In conclusion, while Scrum has its limitations, many of these can be addressed and mitigated through proper implementation, adaptation, and organizational support. It is important to carefully consider the specific project and organizational context to determine if Scrum is the right fit and to make necessary adjustments to maximize its benefits and overcome any potential drawbacks.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "uU9ujTsaMQLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from operator import itemgetter\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# 1. 공통 모델 설정 (Llama-3.2 또는 Qwen 사용 권장)\n",
        "llm_ep = HuggingFaceEndpoint(\n",
        "    repo_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=1024,\n",
        "    temperature=0.7\n",
        ")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "model_parser = model | StrOutputParser()\n",
        "\n",
        "# --- Example 1: Sequential Chaining (순차 체인) ---\n",
        "#\n",
        "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
        "prompt2 = ChatPromptTemplate.from_template(\"what country is the city {city} in? respond in {language}\")\n",
        "\n",
        "chain1 = prompt1 | model_parser\n",
        "chain2 = (\n",
        "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
        "    | prompt2\n",
        "    | model_parser\n",
        ")\n",
        "\n",
        "print(\"--- Example 1 Result ---\")\n",
        "print(chain2.invoke({\"person\": \"obama\", \"language\": \"english\"}))\n",
        "\n",
        "\n",
        "# --- Example 2: Parallel Input (병렬 입력 생성) ---\n",
        "#\n",
        "prompt1 = ChatPromptTemplate.from_template(\"generate a {attribute} color. Return the name of the color and nothing else:\")\n",
        "prompt2 = ChatPromptTemplate.from_template(\"what is a fruit of color: {color}. Return the name of the fruit and nothing else:\")\n",
        "prompt3 = ChatPromptTemplate.from_template(\"what is a country with a flag that has the color: {color}. Return the name of the country and nothing else:\")\n",
        "prompt4 = ChatPromptTemplate.from_template(\"What is the color of {fruit} and the flag of {country}?\")\n",
        "\n",
        "color_generator = ({\"attribute\": RunnablePassthrough()} | prompt1 | {\"color\": model_parser})\n",
        "color_to_fruit = prompt2 | model_parser\n",
        "color_to_country = prompt3 | model_parser\n",
        "\n",
        "question_generator = (\n",
        "    color_generator\n",
        "    | {\"fruit\": color_to_fruit, \"country\": color_to_country}\n",
        "    | prompt4\n",
        ")\n",
        "\n",
        "print(\"\\n--- Example 2 Result ---\")\n",
        "generated_prompt = question_generator.invoke(\"warm\")\n",
        "print(f\"Generated Question: {generated_prompt.messages[0].content}\")\n",
        "print(model_parser.invoke(generated_prompt))\n",
        "\n",
        "\n",
        "# --- Example 3: Branching and Merging (분기 및 병합 - Critique Loop) ---\n",
        "#\n",
        "planner = (\n",
        "    ChatPromptTemplate.from_template(\"Generate a concise argument about: {input}\")\n",
        "    | model_parser\n",
        "    | {\"base_response\": RunnablePassthrough()}\n",
        ")\n",
        "\n",
        "arguments_for = (\n",
        "    ChatPromptTemplate.from_template(\"List the pros or positive aspects of {base_response}\")\n",
        "    | model_parser\n",
        ")\n",
        "arguments_against = (\n",
        "    ChatPromptTemplate.from_template(\"List the cons or negative aspects of {base_response}\")\n",
        "    | model_parser\n",
        ")\n",
        "\n",
        "final_responder = (\n",
        "    ChatPromptTemplate.from_messages([\n",
        "        (\"ai\", \"{original_response}\"),\n",
        "        (\"human\", \"Pros:\\n{results_1}\\n\\nCons:\\n{results_2}\"),\n",
        "        (\"system\", \"Generate a final balanced response given the critique above.\"),\n",
        "    ])\n",
        "    | model_parser\n",
        ")\n",
        "\n",
        "full_critique_chain = (\n",
        "    planner\n",
        "    | {\n",
        "        \"results_1\": arguments_for,\n",
        "        \"results_2\": arguments_against,\n",
        "        \"original_response\": itemgetter(\"base_response\"),\n",
        "    }\n",
        "    | final_responder\n",
        ")\n",
        "\n",
        "print(\"\\n--- Example 3 Result (Scrum Analysis) ---\")\n",
        "print(full_critique_chain.invoke({\"input\": \"scrum\"}))"
      ],
      "metadata": {
        "id": "I3yVVwz_NMqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Querying a SQL DB"
      ],
      "metadata": {
        "id": "he01S3jBQW0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_community.utilities.sql_database import SQLDatabase\n",
        "\n",
        "\n",
        "template = \"\"\"Based on the table schema below, write a SQL query that would answer the user's question:\n",
        "{schema}\n",
        "\n",
        "Question: {question}\n",
        "SQL Query:\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# sqlite3 Chinook.db\n",
        "# .read Chinook_Sqlite.sql\n",
        "# download the sql file from the link below\n",
        "# https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql\n",
        "\n",
        "db = SQLDatabase.from_uri(\"sqlite:///./Chinook.db\")\n",
        "\n",
        "def get_schema(_):\n",
        "    return db.get_table_info()\n",
        "\n",
        "def run_query(query):\n",
        "    return db.run(query)\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "sql_response = (\n",
        "    RunnablePassthrough.assign(schema=get_schema)\n",
        "    | prompt\n",
        "    | model.bind(stop=[\"\\nSQLResult:\"])\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(sql_response.invoke({\"question\": \"How many employees are there?\"}))\n",
        "\"\"\"\n",
        "SELECT COUNT(*) FROM Employee\n",
        "\"\"\"\n",
        "\n",
        "template = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n",
        "{schema}\n",
        "\n",
        "Question: {question}\n",
        "SQL Query: {query}\n",
        "SQL Response: {response}\"\"\"\n",
        "prompt_response = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "full_chain = (\n",
        "    RunnablePassthrough.assign(query=sql_response)\n",
        "    | RunnablePassthrough.assign(\n",
        "        schema=get_schema,\n",
        "        response=lambda x: run_query(x[\"query\"]),\n",
        "    )\n",
        "    | prompt_response\n",
        "    | model\n",
        ")\n",
        "\n",
        "print(full_chain.invoke({\"question\": \"How many employees are there?\"}))\n",
        "\"\"\"\n",
        "content='There are 8 employees.'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9U8SMVDKNV-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_community.utilities.sql_database import SQLDatabase\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# 1. 데이터베이스 연결 (sqlite:///./Chinook.db 파일이 있어야 함)\n",
        "db = SQLDatabase.from_uri(\"sqlite:///./Chinook.db\")\n",
        "\n",
        "def get_schema(_):\n",
        "    return db.get_table_info()\n",
        "\n",
        "def run_query(query):\n",
        "    # 모델이 생성한 쿼리에 간혹 붙는 백틱(```sql) 등을 제거하는 전처리\n",
        "    clean_query = query.replace(\"```sql\", \"\").replace(\"```\", \"\").strip()\n",
        "    return db.run(clean_query)\n",
        "\n",
        "# 2. 모델 설정 (Qwen2.5 또는 Llama-3.2 추천 - 코딩 능력이 좋음)\n",
        "llm_ep = HuggingFaceEndpoint(\n",
        "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.1 # 쿼리 생성은 결정적이어야 하므로 낮은 온도 설정\n",
        ")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# 3. SQL 생성 체인 (첫 번째 단계)\n",
        "# [Image of Text-to-SQL architecture showing natural language query being converted to SQL via LLM and executed on a database]\n",
        "sql_template = \"\"\"Based on the table schema below, write ONLY the SQL query that answers the user's question.\n",
        "Do not write any explanation.\n",
        "\n",
        "{schema}\n",
        "\n",
        "Question: {question}\n",
        "SQL Query:\"\"\"\n",
        "sql_prompt = ChatPromptTemplate.from_template(sql_template)\n",
        "\n",
        "sql_response = (\n",
        "    RunnablePassthrough.assign(schema=get_schema)\n",
        "    | sql_prompt\n",
        "    | model.bind(stop=[\"\\nSQLResult:\", \" ;\", \";\"])\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 4. 자연어 응답 체인 (두 번째 단계)\n",
        "# [Image of SQL-to-Text process showing database results being formatted into a natural language response by an AI model]\n",
        "response_template = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n",
        "{schema}\n",
        "\n",
        "Question: {question}\n",
        "SQL Query: {query}\n",
        "SQL Response: {response}\n",
        "Natural Language Response:\"\"\"\n",
        "response_prompt = ChatPromptTemplate.from_template(response_template)\n",
        "\n",
        "full_chain = (\n",
        "    RunnablePassthrough.assign(query=sql_response)\n",
        "    | RunnablePassthrough.assign(\n",
        "        schema=get_schema,\n",
        "        response=lambda x: run_query(x[\"query\"]),\n",
        "    )\n",
        "    | response_prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 5. 실행 및 확인\n",
        "try:\n",
        "    print(\"--- 1. 생성된 SQL 쿼리 ---\")\n",
        "    query_text = sql_response.invoke({\"question\": \"How many employees are there?\"})\n",
        "    print(query_text)\n",
        "\n",
        "    print(\"\\n--- 2. 최종 자연어 답변 ---\")\n",
        "    final_answer = full_chain.invoke({\"question\": \"How many employees are there?\"})\n",
        "    print(final_answer)\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "vFWavYHaQdT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Generator"
      ],
      "metadata": {
        "id": "4Pe20D7qRXz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note as of 02/27/2024\n",
        "# before you start you need to install the following\n",
        "# pip install langchain==0.1.9 langchain-openai==0.0.8\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        ")\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain_community.utilities import PythonREPL\n",
        "\n",
        "# Introduce PythonREPL\n",
        "python_repl = PythonREPL()\n",
        "print(python_repl.run(\"print(1+1)\"))\n",
        "\"\"\"\n",
        "Python REPL can execute arbitrary code. Use with caution.\n",
        "2\n",
        "\"\"\"\n",
        "\n",
        "template = \"\"\"Write some python code to solve the user's problem.\n",
        "\n",
        "Return only python code in Markdown format, e.g.:\n",
        "\n",
        "```python\n",
        "....\n",
        "```\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", template), (\"human\", \"{input}\")])\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "def _sanitize_output(text: str):\n",
        "    _, after = text.split(\"```python\")\n",
        "    return after.split(\"```\")[0]\n",
        "\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "print(chain.invoke({\"input\": \"Write the function to sort the list. Then call the function by pasing [1,4,2]\"}))\n",
        "\"\"\"\n",
        "```python\n",
        "def sort_list(lst):\n",
        "    return sorted(lst)\n",
        "\n",
        "my_list = [1, 4, 2]\n",
        "sorted_list = sort_list(my_list)\n",
        "print(sorted_list)\n",
        "```\n",
        "\"\"\"\n",
        "repl_chain = chain | _sanitize_output | PythonREPL().run\n",
        "\n",
        "print(repl_chain.invoke({\"input\": \"Write the function to sort the list. Then call the function by pasing [1,4,2]\"}))\n",
        "\"\"\"\n",
        "Python REPL can execute arbitrary code. Use with caution.\n",
        "[1, 2, 4]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Lqt3WbQmQi_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-experimental"
      ],
      "metadata": {
        "id": "uo-nk1uSSYhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "# [중요] 임포트 경로를 experimental로 변경합니다.\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "\n",
        "# 1. 모델 설정 (Qwen2.5-7B 추천)\n",
        "llm_ep = HuggingFaceEndpoint(\n",
        "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=1024,\n",
        "    temperature=0.1\n",
        ")\n",
        "model = ChatHuggingFace(llm=llm_ep)\n",
        "\n",
        "# 2. 프롬프트 설정\n",
        "template = \"\"\"Write some python code to solve the user's problem.\n",
        "\n",
        "Return only python code in Markdown format, e.g.:\n",
        "\n",
        "```python\n",
        "....\n",
        "```\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", template), (\"human\", \"{input}\")])\n",
        "\n",
        "# 3. 출력 정제 함수\n",
        "def _sanitize_output(text: str):\n",
        "    if \"```python\" in text:\n",
        "        _, after = text.split(\"```python\")\n",
        "        return after.split(\"```\")[0].strip()\n",
        "    return text.strip()\n",
        "\n",
        "# 4. 체인 구성\n",
        "code_gen_chain = prompt | model | StrOutputParser()\n",
        "\n",
        "# PythonREPL 객체 생성 (실행 시 주의 문구가 출력됩니다)\n",
        "repl = PythonREPL()\n",
        "repl_chain = code_gen_chain | _sanitize_output | repl.run\n",
        "\n",
        "# 5. 실행 테스트\n",
        "input_query = \"Write a function to sort a list. Then call the function by passing [1, 4, 2] and print the result.\"\n",
        "\n",
        "print(\"--- 1. 생성된 코드 확인 ---\")\n",
        "generated_code = code_gen_chain.invoke({\"input\": input_query})\n",
        "print(generated_code)\n",
        "\n",
        "print(\"\\n--- 2. 실행 결과 확인 ---\")\n",
        "#\n",
        "execution_result = repl_chain.invoke({\"input\": input_query})\n",
        "print(execution_result)"
      ],
      "metadata": {
        "id": "ys10eaeaSJwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t_JNDfmESKCm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}